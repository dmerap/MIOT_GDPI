{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58c08b4-d5a2-4a53-97ce-b6233bd7d18c",
   "metadata": {},
   "source": [
    "![MIoT_GDPI](img/MIOT_GDPI_header.png)\n",
    "\n",
    "# Unidad 05 - Introducción a las Redes Neuronales Recurrentes (RNN)\n",
    "Típicamente, las plantas industriales cuentan con líneas de producción secuenciales en las que los procesos son altamente no lineales y tienen grandes dependencias temporales. Como consecuencia, los modelos predictivos que no tienen en cuenta el tiempo, suelen ser poco apropiados. En esta práctica introduciremos las Redes Neuronales Recurrentes (RNN) simples como sistemas capaces de modelar procesos en los que se tiene en cuenta el histórico de las observaciones para poder predecir con precisión. \n",
    "\n",
    "\n",
    "\n",
    "La mayor parte del contenido de este Notebook se dedica a explicar la adaptación de los datos y el uso del API Keras para generar y utilizar RNNs. Es crucial que dediquéis tiempo a leer y comprender el material, en lugar de simplemente ejecutar el código. Os invitamos a experimentar modificando y variando el código proporcionado para que podáis explorar las distintas opciones y profundizar en cada uno de los conceptos mostrados.\n",
    "\n",
    "\n",
    "\n",
    "**Importante**: El Notebook contiene varios ejercicios sencillos que deberéis desarrollar durante la clase y enviarlos por el aula virtual del curso a través de la tarea correspondiente.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Referencias útiles para la práctica\n",
    "\n",
    "1. [Documentación oficial](https://www.tensorflow.org/learn?hl=es-419) de Tensorflow\n",
    "2. [Guías](https://keras.io/guides/) de Keras\n",
    "3. [Vídeo](https://www.youtube.com/watch?v=AsNTP8Kwu80 ) intuitivo y ameno sobre las RNNs\n",
    "4. A. Bosch Rué, J. Casas-Roma, T. Lozano Bagén (2019): [Deep learning : principios y fundamentos](https://elibro-net.ezbusc.usc.gal/es/ereader/busc/126167/).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Redes Neuronales Recurrentes\n",
    "En el ámbito de la optimización y el análisis predictivo dentro de las líneas de producción industrial, nos encontramos a menudo con procesos que no son estáticos, sino que se desarrollan a lo largo del tiempo, donde cada paso o estación influye en los siguientes. La naturaleza secuencial de estas operaciones es fundamental: el estado actual de un producto o del proceso mismo depende intrínsecamente de su historia previa. Si intentamos aplicar modelos de aprendizaje automático que tratan cada punto de datos o cada instante como una observación independiente, sin tener en cuenta el orden o la dependencia temporal (como haríamos con modelos tradicionales que asumen independencia y distribución idéntica), estaremos ignorando información crucial. Estos modelos no pueden \"recordar\" lo que ocurrió antes, lo que los hace inadecuados para capturar patrones que se despliegan a lo largo de la secuencia de producción. Es aquí donde las Redes Neuronales Recurrentes (RNNs) cobran relevancia.\n",
    "\n",
    "Su arquitectura inherente, diseñada para procesar secuencias, les permite mantener un estado interno que actúa como una memoria, incorporando información de pasos temporales anteriores para influir en el procesamiento del paso actual. Esta capacidad de aprender y utilizar el contexto histórico las convierte en la herramienta perfecta para modelar las complejas dinámicas temporales de una línea de producción secuencial, permitiéndonos abordar tareas como la detección temprana de fallos que se desarrollan con el tiempo, la predicción de desvíos en variables de calidad teniendo en cuenta las observaciones históricas  o la optimización de parámetros en tiempo real considerando el impacto de ajustes pasados.\n",
    "\n",
    "\n",
    "### 1.1 Concepto de recurrencia\n",
    "\n",
    "Hasta ahora, en todas las ANN que habéis visto la información fluye siempre en una única dirección: desde las neuronas de entrada a las neuronas de salida. Son las ANN conocidas como *feedforward*, pero, en realidad, nada impide que haya conexiones desde una capa posterior a una anterior. Empleando esta particularidad podemos hacer que cuando la ANN procese una observación concreta, utilice también información que generó la red en un paso anterior. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd30d01-825c-45f8-a659-bc65125d644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importaciones generales\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as err:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as err:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except ImportError as err:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError as err:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "# Asegurarnos de usar Keras 3 con backend TensorFlow\n",
    "# Es necesario hacerlo antes de cargar Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Importaciones de Keras y TensorFlow\n",
    "try:\n",
    "    import keras\n",
    "except ImportError as err:\n",
    "    !pip install keras\n",
    "    import keras\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf \n",
    "except ImportError as err:\n",
    "    !pip install tensorflow\n",
    "    import tensorflow as tf \n",
    "\n",
    "\n",
    "\n",
    "# Importaciones de Scikit-learn\n",
    "#Solo para facilitarnos el uso de algunas operaciones típicas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# Configuraciones para visualización. No es necesario\n",
    "plt.style.use('seaborn-v0_8-whitegrid') # Estilo de gráficos\n",
    "sns.set_palette('viridis') # Paleta de colores\n",
    "\n",
    "\n",
    "# Verificación de versiones\n",
    "#Existen diferentes versiones y compatibilidades. Es importantes saber lo que estamos ejecutando\n",
    "#Para estos ejemplos queremos ejecutar el API de Keras3 (standalone) y tensorflo3>2.16\n",
    "print(f\"Versión de Keras: {keras.__version__}\")\n",
    "print(f\"Backend de Keras: {keras.backend.backend()}\")\n",
    "print(f\"Versión de TensorFlow: {tf.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "# Verificar si TensorFlow puede usar GPU en nuestro sistema (opcional, pero bueno saberlo)\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"GPU disponible: {gpu_devices}\")\n",
    "else:\n",
    "    print(\"GPU no encontrada, se usará CPU.\")\n",
    "\n",
    "\n",
    "#Establecemos una semilla para todos los procesos en los que lo necesitemos\n",
    "SEED=1234\n",
    "\n",
    "print(f\"la semilla que emplearemos para todos los procesos pseudoaleatorios es:{SEED}\")\n",
    "\n",
    "# Establece la semilla para los números aleatorios con  keras.utils.set_random_seed. Esto establecerá:\n",
    "# 1) `numpy` seed\n",
    "# 2) backend random seed\n",
    "# 3) `python` random seed\n",
    "#Permitir la reproducibilidad es clave para poder comparar\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ebecf-b625-45d1-a004-c0fb7efff0a3",
   "metadata": {},
   "source": [
    "## 2. Generación de un *dataset* sintético\n",
    "\n",
    "Vamos a crear un dataset sintético  que imite una fábrica con una línea de producción  secuencial dividida en **5 secciones**. \n",
    "\n",
    "1.  **Sensores en cada sección:**\n",
    "    * En cada sección, una máquina realiza una operación (ej: apretar un tornillo, calentar una pieza, etc.) y un sensor mide un parámetro clave de esa operación (ej: las revoluciones por minuto de un motor, la temperatura aplicada, etc.). Tendremos 5 series de datos, una por cada sensor de sección.\n",
    "    * Cada sección puede influir en la siguiente; por ejemplo, si una máquina en la sección 2 trabajó muy rápido, la sección 3 podría tener que ajustarse ligeramente.\n",
    "\n",
    "2.  **Variable de Calidad Final:**\n",
    "    * Al final de toda la línea, se inspecciona el producto y se mide una **variable de calidad** (ej: la resistencia de una soldadura, la humedad interna, el perfil de densidad, etc).\n",
    "    * Esta calidad final **NO depende solo de la última operación**. Más bien, es el resultado de una *combinación* de cómo se aplicaron los procesos previos en las diferentes secciones cuando ese producto pasó por ellas.\n",
    "        * Por ejemplo, la calidad podría depender mucho de lo que hizo la máquina de la sección 1 cuando el producto estuvo allí (hace 5 \"pasos de tiempo\"), un poco de la sección 2 (hace 3 \"pasos de tiempo\"), y así sucesivamente. Algunas secciones podrían no influir nada en esta variable de calidad particular.\n",
    "    * Esta relación entre las operaciones pasadas y la calidad final no es simple (es no-lineal) y tiene algo de \"ruido\" o variabilidad, como en la vida real.\n",
    "\n",
    "3.  **El Reto para la Red Neuronal:**\n",
    "    * Nuestro DataFrame mostrará, para cada producto, las lecturas actuales de los 5 sensores y su calidad final.\n",
    "    * La Red Neuronal Recurrente (RNN) recibirá secuencias de las lecturas de los sensores de varios productos consecutivos. Su tarea será **aprender y descubrir por sí misma cómo esas lecturas pasadas (y de qué secciones específicas y con qué \"retraso\") afectan a la calidad del siguiente producto**, sin que le digamos explícitamente \"fíjate en la sección 1 de hace 5 pasos\".\n",
    "\n",
    "Este dataset nos permitirá simular un problema realista donde es necesario analizar secuencias temporales para predecir un resultado futuro que tiene dependencias complejas con el pasado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cf7ce-d929-4653-97fd-cdb8fefc9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generación de Datos Sintéticos (Línea de Producción Secuencial)\n",
    "\n",
    "N_SAMPLES_FINAL = 20000 # Número de muestras finales en nuestro dataset\n",
    "N_SECTIONS = 5 #secciones de nuestra línea de producción\n",
    "SEQUENCE_LENGTH = 5 # Usaremos secuencias de 5 timesteps para predecir el siguiente.\n",
    "\n",
    "# Parámetros para la generación de datos de sensores por sección\n",
    "# Lags (en pasos de tiempp para la contribución de cada sección a la calidad final.\n",
    "# L_s = k significa que el valor del sensor de la sección 's' hace k observaciones afectó la calidad del producto actual.\n",
    "# Ejemplo: L_s = 1 significa que P_{s, t-1} afecta a Q_t.\n",
    "section_lags = np.array([5, 4, 3, 1, 1]) # Lags para S1, S2, S3, S4, S5 respectivamente\n",
    "\n",
    "L_max = np.max(section_lags) if len(section_lags) > 0 else 0\n",
    "\n",
    "# Necesitamos generar más datos al principio para acomodar los retardos máximos.\n",
    "N_RAW_SAMPLES = N_SAMPLES_FINAL + L_max\n",
    "\n",
    "# Inicializar arrays para los valores de los sensores\n",
    "P_raw = np.zeros((N_SECTIONS, N_RAW_SAMPLES))\n",
    "time_raw = np.arange(N_RAW_SAMPLES)#pasos de tiempo\n",
    "\n",
    "# Parámetros base para cada sección (amplitud, frecuencia, offset, ruido)\n",
    "base_params_amplitude = [10, 8, 12, 7, 10]\n",
    "base_params_period = [50, 180, 10, 70, 45]\n",
    "base_params_offset = [100, 80, 120, 90, 110]\n",
    "base_params_noise_std = [2, 1.5, 2, 1, 1.8]\n",
    "\n",
    "\n",
    "\n",
    "# Sección 1 (S1) - Comportamiento base\n",
    "P_raw[0, :] = 100 + 10 * np.sin(time_raw / 50) + 5 * np.cos(time_raw / 20) + np.random.randn(N_RAW_SAMPLES) * 2\n",
    "P_raw[0, :] = (base_params_amplitude[0] * np.sin(time_raw / base_params_period[0])) +  base_params_offset[0] + \\\n",
    "                5 * np.cos(time_raw / 20) + np.random.randn(N_RAW_SAMPLES) * base_params_noise_std[0]\n",
    "\n",
    "# Secciones 2 a 5 (S2-S5) - Influenciadas por la sección anterior\n",
    "# Coeficientes de influencia de la sección anterior (pequeños)\n",
    "alpha_influence = [0.15, -0.2, 0, 0.05] # Para S2 por S1, S3 por S2, etc.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for s in range(1, N_SECTIONS): # Para S2, S3, S4, S5\n",
    "    influence_term = alpha_influence[s-1] * P_raw[s-1, :]\n",
    "    base_behavior = (base_params_amplitude[s-1] * np.sin(time_raw / base_params_period[s-1]) +\n",
    "                     base_params_offset[s-1] +\n",
    "                     np.random.randn(N_RAW_SAMPLES) * base_params_noise_std[s-1])\n",
    "    P_raw[s, :] = influence_term + base_behavior\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generación de la Variable de Calidad Final (Q_t)\n",
    "# Q_t = f(P_{1,t-L1}, P_{2,t-L2}, P_{3,t-L3}, P_{4,t-L4}, P_{5,t-L5}) + global_noise_t\n",
    "# Los pesos (w_s) indican la contribución de cada sección. Algunos pueden ser cero.\n",
    "quality_weights = np.array([0.8, 0.2, -0.5, 0.1, 0]) \n",
    "\n",
    "\n",
    "\n",
    "Q_raw = np.zeros(N_RAW_SAMPLES)\n",
    "base_quality_offset = 60  #valor base de la variable de calidad al que se le añade el efecto de las otras secciones\n",
    "#quality_noise_std = 1.5\n",
    "\n",
    "\n",
    "\n",
    "#Generamos una variable de calidad con dependencias temporales\n",
    "#pero con un comportamiento lineal. Las redes RNN tienen sus limitaciones\n",
    "#generar un \n",
    "for t in range(L_max, N_RAW_SAMPLES):   \n",
    "    Q_raw[t] = base_quality_offset\n",
    "    for i in range(N_SECTIONS):\n",
    "        Q_raw[t] += quality_weights[i] * P_raw[0, t - section_lags[i]] #podríamos crear un comportamiento lineal\n",
    "    \n",
    "    #np.random.randn(N_RAW_SAMPLES) + quality_noise_std #Podemos complicarlo añadiendo ruido\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#Generamos un dataframe por compdidad\n",
    "\n",
    "df_cols = {}\n",
    "for s in range(N_SECTIONS):\n",
    "    df_cols[f'Seccion_{s+1}'] = P_raw[s, L_max:]\n",
    "df_cols['Calidad'] = Q_raw[L_max:]\n",
    "\n",
    "data_df = pd.DataFrame(df_cols)\n",
    "\n",
    "print(\"Primeras filas del dataset generado:\")\n",
    "print(data_df.head())\n",
    "print(f\"Estructura del dataset: {data_df.shape}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782ee56-d4b3-4bf8-9028-7e4ca2a97cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. Exploración y Visualización de Datos\n",
    "# Visualizar las series temporales generadas para entender sus patrones.\n",
    "\n",
    "fig, axs = plt.subplots(N_SECTIONS + 1, 1, figsize=(14, 18), sharex=True)\n",
    "features_to_plot = [f'Seccion_{s+1}' for s in range(N_SECTIONS)] + ['Calidad']\n",
    "\n",
    "for i, feature_name in enumerate(features_to_plot):\n",
    "    axs[i].plot(data_df[feature_name], label=feature_name)\n",
    "    axs[i].set_ylabel(feature_name)\n",
    "    axs[i].legend(loc=\"upper right\")\n",
    "axs[-1].set_xlabel(\"Tiempo\")\n",
    "fig.suptitle(\"Series Temporales de Parámetros de Sección y Calidad del Producto\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2ebc6-14c1-467e-aa36-5d037b33b6a4",
   "metadata": {},
   "source": [
    "## 3. Preparación de los datos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ee358-4b08-4e1d-bba4-6b2e99c18958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Características de entrada (X): S1_Param, S2_Param, S3_Param, S4_Param, S5_Param en el instante t.\n",
    "# Variable objetivo (Y): Calidad en el instante t\n",
    "def separar_inputs_outputs(dataset):\n",
    "    \"\"\"Separa un DataFrame de Pandas en características (inputs) y la columna objetivo 'Calidad'.\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): El DataFrame de Pandas que se va a procesar.\n",
    "            Se espera que este DataFrame contenga las características de entrada\n",
    "            y, potencialmente, la columna objetivo 'Calidad'.\n",
    "\n",
    "    Returns:\n",
    "          Una tupla con dos elementos:\n",
    "            1. inputs (pd.DataFrame | None): Un DataFrame con las características\n",
    "               (todas las columnas del dataset original excepto 'Calidad').\n",
    "               Retorna `None` si la columna 'Calidad' no se encuentra en el\n",
    "               `dataset`.\n",
    "            2. outputs (pd.Series | None): Una Serie de Pandas con la columna\n",
    "               'Calidad'. Retorna `None` si la columna 'Calidad' no se\n",
    "               encuentra en el `dataset`.\n",
    "    \"\"\"\n",
    "    if 'Calidad' in dataset.columns: \n",
    "        return dataset.drop('Calidad', axis=1), dataset['Calidad']\n",
    "    else: return None, None\n",
    "\n",
    "def separar_conjuntos(dataset, perc_entrenamiento=0.7):\n",
    "    \"\"\"Divide un conjunto de datos secuencial en entrenamiento, validación y prueba.\n",
    "    - El conjunto de entrenamiento obtiene `perc_entrenamiento` del total de muestras.\n",
    "    - El porcentaje restante, `(1 - perc_entrenamiento)`, se divide equitativamente\n",
    "      entre los conjuntos de validación y prueba.\n",
    "    Args:\n",
    "        dataset: El conjunto de datos completo a dividir.\n",
    "        perc_entrenamiento (float, optional): Proporción del dataset a asignar al\n",
    "            conjunto de entrenamiento. \n",
    "\n",
    "    Returns:\n",
    "        Tuple[Sequence, Sequence, Sequence]: Una tupla con tres subconjuntos de datos:\n",
    "            1. El subconjunto de entrenamiento.\n",
    "            2. El subconjunto de validación.\n",
    "            3. El subconjunto de prueba.\n",
    "    \"\"\"\n",
    "    perc_entrenamiento=perc_entrenamiento if 0<perc_entrenamiento<1 else 0.7\n",
    "    \n",
    "    num_train_samples = int(perc_entrenamiento * len(dataset))\n",
    "    num_val_samples =int((1-perc_entrenamiento)/2 * len(dataset))\n",
    "    num_test_samples = len(dataset) - num_train_samples - num_val_samples\n",
    "    return dataset[:num_train_samples], \\\n",
    "            dataset[num_train_samples : num_train_samples + num_val_samples],\\\n",
    "            dataset[num_train_samples + num_val_samples:]\n",
    "    \n",
    "    \n",
    "\n",
    "train_set, val_set, test_set=separar_conjuntos(data_df)\n",
    "\n",
    "\n",
    "train_features, train_targets=separar_inputs_outputs(train_set)\n",
    "val_features, val_targets=separar_inputs_outputs(val_set)\n",
    "test_features, test_targets=separar_inputs_outputs(test_set)\n",
    "\n",
    "print(f\"Estructura de entrenamiento (features): {train_features.shape}\")\n",
    "print(f\"Estructura de entrenamiento (outputs): {train_targets.shape}\")\n",
    "\n",
    "\n",
    "#feature_cols = [f'Seccion_{s+1}' for s in range(N_SECTIONS)]\n",
    "#target_col = 'Calidad'\n",
    "\n",
    "\n",
    "#rnn_features = data_df[feature_cols].values\n",
    "#rnn_targets = data_df[target_col].values\n",
    "\n",
    "#print(f\"Forma de rnn_features: {rnn_features.shape}\")\n",
    "#print(f\"Forma de rnn_targets: {rnn_targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb0350-3fd2-4d65-9be1-9198d45c0e87",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Estandarización de los datos con Keras\n",
    "\n",
    "\n",
    "En lugar de estandarizar los datos a través de una operación de `scikit-learn` (sistema que ya conocéis), lo haremos desde el propio Keras generando una [capa de preprocesado](https://keras.io/api/layers/preprocessing_layers/). Esto tiene como ventaja poder incluirlo en el propio modelo y que este sea autosuficiente. Debéis de recordar que el entrenamiento de los modelos es solo la primera fase, luego es necesario desplegarlos en producción y trabajar con ellos. Poder guardar con el modelo toda la información necesaria para poder emplearlo (ej. capa de estandarización), simplifica su empleo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180a8ac-63d9-4fb7-bb36-cedd97bceeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarización de las características de entrada\n",
    "\n",
    "normalization_layer = keras.layers.Normalization(axis=-1)\n",
    "normalization_layer.adapt(train_features.values) # ajustar SOLO con datos de entrenamiento (las 5 características)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566a7fa-0c03-4439-bd38-607dccd644b8",
   "metadata": {},
   "source": [
    "## 3.2 Preparación de las secuencias de datos\n",
    "Para poder  entrenar modelos densos recurrentes es necesario preparar las secuencias que vamos a emplear para entrenarlos. Keras espera un tensor de la forma `(batch_size, timesteps, features)`:\n",
    "* `batch_size` (tamaño del lote): Es el número de secuencias que se procesan simultáneamente en una pasada de entrenamiento.\n",
    "* `timesteps` (pasos de yiempo o longitud de la secuencia): Es el número de observaciones consecutivas en cada secuencia individual que se van a emplear para entrenar.\n",
    "* `features` (características): Es el número de variables o características que observas en cada paso de tiempo.\n",
    "\n",
    "Para preparar las secuencias respecto a nuestros *datasets* emplearemos [keras.utils.timeseries_dataset_from_array](https://keras.io/api/data_loading/timeseries/). Su principal objetivo es facilitar la preparación de datos para modelos que aprenden de secuencias, como las Redes Neuronales Recurrentes (RNNs), LSTMs, GRUs o Transformers, cuando se trabaja con datos de series temporales.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d8dcd-e7b5-4d6c-9a57-afe87687e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Creación de Secuencias (Ventanas de Tiempo)**\n",
    "# empleando `tf.keras.utils.timeseries_dataset_from_array`.\n",
    "# el formato generado es (batch_size, timesteps, features)\n",
    "\n",
    "def generacion_secuencias_temporales(features, targets, seq_len, solape=False, shuffle=True):\n",
    "    BATCH_SIZE = 32    \n",
    "    seq_stride=1 if  solape else seq_len\n",
    "\n",
    "    return  keras.utils.timeseries_dataset_from_array(\n",
    "        data=features,\n",
    "        targets=targets,\n",
    "        sequence_length=seq_len,\n",
    "        sequence_stride= seq_stride,#importante decidir si queremos o no secuencias con solape\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=shuffle,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_dataset=generacion_secuencias_temporales(train_features, train_targets, solape=False, seq_len=SEQUENCE_LENGTH)\n",
    "val_dataset=generacion_secuencias_temporales(val_features, val_targets, solape=False, seq_len=SEQUENCE_LENGTH, shuffle=False)\n",
    "test_dataset=generacion_secuencias_temporales(test_features, test_targets, solape=False,  seq_len=SEQUENCE_LENGTH, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nInspección de un batch del dataset de entrenamiento:\")\n",
    "for batch_inputs, batch_targets in train_dataset.take(1):\n",
    "    print(\"Forma de las entradas del lote (batch_inputs):\", batch_inputs.shape) # (BATCH_SIZE, SEQUENCE_LENGTH, N_SECTIONS)\n",
    "    print(\"Forma de las etiquetas del lote (batch_targets):\", batch_targets.shape) # (BATCH_SIZE,)\n",
    "\n",
    "print(f\"Batches de entrenamiento: {len(train_dataset)}\")\n",
    "print(f\"Batches de validación: {len(val_dataset)}\")\n",
    "print(f\"Batches de test: {len(test_dataset)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e1808-d45a-426d-8131-4840eb50fc19",
   "metadata": {},
   "source": [
    "## 4. Definición del modelo \n",
    "\n",
    "Desarrollaremos un modelo simple RNN con una capa de entrada, una oculta RNN y una densa de salida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f013c93-6781-46b9-8d4b-93e3b596b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del Modelo RNN\n",
    "# Modelo secuencial con: Input, Normalization, SimpleRNN, Dense (salida).\n",
    "\n",
    "def get_model(seq_len, num_model_features, preprocesing_layers, name_model, type_rnn=True):\n",
    "    hidden_units=64\n",
    "    #input layer\n",
    "\n",
    "    #input layer\n",
    "    model = keras.Sequential(name=name_model)\n",
    "\n",
    "    shape=(seq_len, num_model_features) if type_rnn else (num_model_features, )\n",
    "    model.add( keras.Input(shape=shape, name=\"input_layer\"))\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    #preprocesing layers\n",
    "    for layer in preprocesing_layers:\n",
    "       model.add(layer)\n",
    "\n",
    "\n",
    "    #Hidden layer\n",
    "    hidden_layer= keras.layers.LSTM(units=hidden_units, activation='relu', name=\"hidden_layer\") if type_rnn \\\n",
    "                                    else keras.layers.Dense(hidden_units, activation='relu', name=\"hidden_layer\")\n",
    "\n",
    "    model.add(hidden_layer)\n",
    "    model.add( keras.layers.Dense(units=1,  activation='linear', name=\"output\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb512a1-1bc2-4615-a116-29a68db2d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model=get_model(SEQUENCE_LENGTH, N_SECTIONS, [normalization_layer], \"rnn_model\", type_rnn=True)\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6ba99-5fb6-4ea6-aa16-23ab9453354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model=get_model(SEQUENCE_LENGTH, N_SECTIONS, [normalization_layer], \"dense_model\", type_rnn=False)\n",
    "dense_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006ff94-0454-421e-ba2e-540147aa5462",
   "metadata": {},
   "source": [
    "### 4. Compilación del modelo\n",
    "\n",
    "\n",
    "Antes de entrenar, necesitamos \"compilar\" el modelo para configurar su proceso de entrenamiento. Esto implica definir:\n",
    "\n",
    "* **Optimizador**: Algoritmo que ajusta los pesos de la red durante el entrenamiento (ej. Adam, SGD, RMSprop). Adam es una opción robusta y popular. Tenéis [aquí](https://keras.io/api/optimizers/) un listado de los disponibles. La configuración de los parámetros del optimizador sería algo para optimizar en la búsqueda hiperparamétrica.\n",
    "\n",
    "* **Función de Pérdida** (*Loss Function*): Mide qué tan bien se desempeña el modelo durante el entrenamiento. Para regresión, mean_squared_error (MSE) es una opción muy típica. Tenéis [otras](https://keras.io/api/losses/) disponibles.\n",
    "\n",
    "\n",
    "* **Métricas**: Funciones para evaluar el rendimiento del modelo (ej. mean_absolute_error - MAE). **Las métricas no se usan para optimizar el modelo, solo para monitorizarlo**. Tenéis [aquí](https://keras.io/api/metrics/) un listado de las disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc0292-f7a4-4f56-83b9-dd62b38e402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del Modelo\n",
    "# Optimizador Adam, pérdida MSE, métrica MAE.\n",
    "\n",
    "rnn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "dense_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8264694e-2378-44e8-b73d-036f4d91bb42",
   "metadata": {},
   "source": [
    "### 5 Entrenamiento del modelo\n",
    "\n",
    "Ya tenemos definido y configurado el modelo para poder entrenarse con nuestro  dataset de entrada. Recordad que el **modelo contiene una capa de normalización**, por lo que **los datos que se le pasen para entrenar NO pueden estar normalizados**(lo hará el propio modelo).\n",
    "\n",
    "\n",
    "El método `fit` es el empleado para entrenar. Tenéis [aquí](https://keras.io/api/models/model_training_apis/#fit-method) disponible todos los parámetros posibles. En este caso emplearemos:\n",
    "\n",
    "* **epochs**: Número de veces que el modelo verá el conjunto de datos completo.\n",
    "* **batch_size**: Número de muestras procesadas antes de actualizar los pesos (entrenamiento con *mini-batches*).\n",
    "* **validation_data**: conjunto de datos sobre los que validaremos el entrenamiento del modelo. Lo podemos emplear para comparar configuraciones o para parar el entrenamiento de forma prematura (*early_stopping*), lo que puede ayudar a detectar el *overfitting*.\n",
    "\n",
    "El método `fit` devuelve un objeto *History* que contiene información sobre el proceso de entrenamiento (pérdida y métricas en cada época) que podremos utilizar posteriormente para graficar el proceso de entreno y poder valorar si tenemos *overfitting* o *underfitting*.\n",
    "\n",
    "En este caso definiremos un *callback* que permita parar el entrenamiento cuando las métricas sobre el conjunto de validación dejen de mejorar y nos quedamos con el mejor modelo visto.\n",
    "\n",
    "**IMPORTANTE**: cada vez que ejecutéis el entrenamiento, el modelo parte de su último estado. Si queréis volver a entrenarlo desde cero, entonces tenéis que volver a crearlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d344b8-132a-4295-a691-b1600f6ec614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del Modelo\n",
    "EPOCHS = 1000 # Aumentamos un poco por la posible mayor complejidad\n",
    "# Considerar EarlyStopping en una implementación real.\n",
    "callback_early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10, # Número de épocas sin mejora antes de detener\n",
    "    restore_best_weights=True # Restaurar los pesos del modelo de la mejor época\n",
    ")\n",
    "\n",
    "rnn_history = rnn_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[callback_early_stopping] # Añadimos el callback\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a183e-d8d6-4a92-9dc8-9de66e7161d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_history = dense_model.fit(\n",
    "    train_features,\n",
    "    train_targets,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(val_features,val_targets),\n",
    "    callbacks=[callback_early_stopping] # Añadimos el callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac55e6f-7e28-4e23-b6db-75984a059272",
   "metadata": {},
   "source": [
    "### 6.  Evaluación del Modelo\n",
    "\n",
    "Una vez entrenado el modelo, tenemos que evaluar su rendimento. Lo haremos desde dos puntos de vista:\n",
    "\n",
    "1. **Visualizando el historial de entrenamiento**: Graficaremos la pérdida y la/s métrica/s disponibles (el MAE en este caso) tanto para el conjunto de entrenamiento como para el de validación a lo largo de las épocas. Esto nos ayuda a diagnosticar problemas como el *overfitting* (cuando la pérdida de validación empieza a aumentar mientras la de entrenamiento sigue bajando) o el *underfitting* (cuando ambas pérdidas son muy altas).\n",
    "2. **Evaluando en el conjunto de Test**: Usaremos el método `evaluate` con los datos de Test, que el modelo nunca ha visto, para generar la  estimación final e imparcial del rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9fe177-490e-43ed-bc3f-d7fc69915d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficas_entrenamiento(hist, model_name=\"\"):\n",
    "    \"\"\"Genera y muestra gráficos de pérdida y MAE del entrenamiento de un modelo Keras.\n",
    "\n",
    "    Esta función toma un objeto `History` de Keras (devuelto por el método `fit()`)\n",
    "    y grafica dos subplots:\n",
    "    1.  La pérdida de entrenamiento ('loss') y la pérdida de validación ('val_loss')\n",
    "        contra el número de épocas.\n",
    "    2.  El MAE de entrenamiento  y el MAE de validación contra el número de épocas.\n",
    "\n",
    "    Args:\n",
    "        history_obj (keras.callbacks.History): El objeto `History` que Keras\n",
    "            retorna después de entrenar un modelo con `model.fit()`.\n",
    "        model_name: nombre del modelo que generó los datos históricos de entrenamiento\n",
    "    \"\"\"      \n",
    "    \n",
    "    hist=pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(hist['epoch'], hist['loss'], label='Pérdida Entrenamiento')\n",
    "    plt.plot(hist['epoch'], hist['val_loss'], label = 'Pérdida Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida (MSE)')\n",
    "    plt.title(f'{model_name} - Pérdida (MSE) durante Entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Asegúrate de usar el nombre correcto de la métrica (puede variar ligeramente)\n",
    "    mae_key = 'mae' if 'mae' in hist.columns else list(hist.columns)[1] # Intenta encontrar la clave MAE\n",
    "    val_mae_key = 'val_' + mae_key\n",
    "    plt.plot(hist['epoch'], hist[mae_key], label='MAE Entrenamiento')\n",
    "    plt.plot(hist['epoch'], hist[val_mae_key], label = 'MAE Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Error Absoluto Medio (MAE)')\n",
    "    plt.title(f'{model_name} - MAE durante Entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Graficar el entrenamiento por épocas. Ayuda a valorar el posible (sobre/infra)entrenamiento\n",
    "\n",
    "graficas_entrenamiento(rnn_history, model_name=rnn_model.name)\n",
    "graficas_entrenamiento(dense_history,  model_name=dense_model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031eeb8f-7d5a-4df6-b47a-bf412f36f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_metrics(model, test_dataset, targets=None):\n",
    "    \n",
    "    print(f\"Metricas del modelo: {model.name}\")\n",
    "    if targets is None: #se pasan los datos como un dataset de tensorflow con inputs y ouputs juntos (secuencias)\n",
    "        test_loss, test_mae = model.evaluate(test_dataset, verbose=0)#para las redes recurrentes\n",
    "    else:\n",
    "        test_loss, test_mae = model.evaluate(test_dataset, targets,verbose=0)#para las redes densas\n",
    "        \n",
    "    print(f\"{model.name} - Pérdida (MSE) en Test: {test_loss:.4f}\")\n",
    "    print(f\"{model.name} - Error Absoluto Medio (MAE) en Test: {test_mae:.4f}\")\n",
    "    \n",
    "def print_seq__pred_vs_real(model, test_dataset, targets=None):\n",
    "    print(f\"Comparativa del modelo: {model.name}\")\n",
    "    predictions = model.predict(test_dataset, verbose=0)\n",
    "\n",
    "    if targets is None: \n",
    "        real_test=[batch_targets_test.numpy() for _, batch_targets_test in test_dataset]#devuelve una lista de lista. Los outputs están agrupados en batches de 32\n",
    "        #print(f\"numero de batches: {len(real_test)} - Elementos en el batch: {len(real_test[0])}\")\n",
    "        actual_real_test = np.concatenate(real_test)#contactenamos todas las listas dentro de la lista (numero de batches) en una sola lista\n",
    "    else:\n",
    "        actual_real_test=targets\n",
    "\n",
    "\n",
    "    print(predictions[:10])    \n",
    "    print(actual_real_test[:10])    \n",
    "    r2 = r2_score(actual_real_test, predictions)\n",
    "    #print(f\"{model.name} - Coeficiente de Determinación (R²) en el conjunto de Test: {r2:.4f}\")\n",
    "    plot_range = min(300, len(actual_targets_test))\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(actual_targets_test[:plot_range], label='Valores Reales ', color='blue', marker='.', linestyle='-')\n",
    "    plt.plot(predictions[:plot_range, 0], label='Predicciones del Modelo', color='red', marker='x', linestyle='--')\n",
    "    plt.title(f'{model.name} - R² en el conjunto de Test: {r2:.4f} - Comparación de Calidad Real vs. Predicha (Primeros {plot_range} productos del Test Set)')\n",
    "    plt.xlabel('Observaciones ordenadas en el tiempo')\n",
    "    plt.ylabel('Variable de Calidad')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "print_test_metrics(rnn_model, test_dataset)\n",
    "print_seq__pred_vs_real(rnn_model, test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "print_test_metrics(dense_model, test_features[::], test_targets[::])\n",
    "print_seq__pred_vs_real(dense_model, test_features[::], test_targets[::])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
