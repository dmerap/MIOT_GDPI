{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58c08b4-d5a2-4a53-97ce-b6233bd7d18c",
   "metadata": {},
   "source": [
    "![MIoT_GDPI](img/MIOT_GDPI_header.png)\n",
    "\n",
    "# Unidad 05 - Introducción a las Redes Neuronales Recurrentes (RNN)\n",
    "Típicamente, las plantas industriales cuentan con **líneas de producción secuenciales en las que los procesos son altamente no lineales y tienen grandes dependencias temporales**. Como consecuencia, los modelos predictivos que no tienen en cuenta el tiempo, suelen ser poco apropiados. En esta práctica introduciremos las **Redes Neuronales Recurrentes** (RNNs) como **sistemas capaces de modelar procesos en los que se tiene en cuenta el histórico de las observaciones** para poder predecir con precisión. \n",
    "\n",
    "\n",
    "\n",
    "La mayor parte del contenido de este Notebook se dedica a explicar la adaptación de los datos y el uso del API Keras para generar y utilizar RNNs. Es crucial que dediquéis tiempo a leer y comprender el material, en lugar de simplemente ejecutar el código. Os invitamos a experimentar modificando y variando el código proporcionado para que podáis explorar las distintas opciones y profundizar en cada uno de los conceptos mostrados. **Este es un Notebook especialmente indicado para probar cosas**.\n",
    "\n",
    "\n",
    "\n",
    "**Importante**: El Notebook contiene un ejercicio sencillo que deberéis desarrollar durante la clase y enviarlo por el aula virtual del curso a través de la tarea correspondiente. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Referencias útiles para la práctica\n",
    "\n",
    "1. [Documentación oficial](https://www.tensorflow.org/learn?hl=es-419) de Tensorflow\n",
    "2. [Guías](https://keras.io/guides/) de Keras\n",
    "3. [Vídeo](https://www.youtube.com/watch?v=AsNTP8Kwu80 ) intuitivo y ameno sobre las RNNs\n",
    "4. A. Bosch Rué, J. Casas-Roma, T. Lozano Bagén (2019): [Deep learning : principios y fundamentos](https://elibro-net.ezbusc.usc.gal/es/ereader/busc/126167/).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Redes Neuronales Recurrentes\n",
    "En el ámbito de la optimización y el análisis predictivo dentro de las líneas de producción industrial, nos encontramos a menudo con **procesos que no son estáticos**, sino que se desarrollan a lo largo del tiempo, donde **cada paso o estación influye en los siguientes**. La naturaleza secuencial de estas operaciones es fundamental: **el estado actual de un producto o del proceso mismo depende intrínsecamente de su historia previa**. Si intentamos aplicar modelos de aprendizaje automático que tratan cada punto de datos o cada instante como una observación independiente, sin tener en cuenta el orden o la dependencia temporal (como haríamos con modelos tradicionales que asumen independencia y distribución idéntica), estaremos ignorando información crucial. **Estos modelos no pueden \"recordar\"** lo que ocurrió antes, lo que los hace inadecuados para capturar patrones que se despliegan a lo largo de la secuencia de producción. Es aquí donde las RNNs cobran relevancia.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"img/observacionLinea.png\" />\n",
    "\n",
    "</div>\n",
    "<p style=\"text-align:center; font-style:italic;\">Una observación de una línea secuencial está capturando datos no relacionados, debido a los desplazamientos temporales.</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**La arquitectura inherente de las RNNs está diseñada para procesar secuencias y les permite mantener un estado interno que actúa como una memoria, incorporando información de pasos temporales anteriores para influir en el procesamiento del paso actual**. Esta capacidad de aprender y utilizar el contexto histórico las convierte en la herramienta perfecta para modelar las complejas dinámicas temporales de una línea de producción secuencial, permitiéndonos abordar tareas como la detección temprana de fallos que se desarrollan con el tiempo, la predicción de desvíos en variables de calidad teniendo en cuenta las observaciones históricas  o la optimización de parámetros en tiempo real considerando el impacto de ajustes pasados.\n",
    "\n",
    "\n",
    "### 1.1 Concepto de recurrencia\n",
    "\n",
    "Hasta ahora, en todas las ANN que habéis visto la información fluye siempre en una única dirección: desde las neuronas de entrada a las neuronas de salida. Son las ANN conocidas como *feedforward*, pero, en realidad, nada impide que haya conexiones desde una capa posterior a una anterior. Empleando esta particularidad podemos hacer que cuando la ANN procese una observación concreta, utilice también información que generó la red en un paso anterior a modo de memoria.  La figura mostrada a continuación muestra el esquema de una RNN simple compacta y desplegada. \n",
    "\n",
    "\n",
    "![MIoT_GDPI](img/RNNSimple.png)\n",
    "<p style=\"text-align:center; font-style:italic;\">Esquema de una RNN simple compacta y desplegada en 'n' instantes temporales.</p>\n",
    "\n",
    "Las RNNs simples, en su forma más básica,  están formadas por una capa de entrada, una capa de recurrencia y una capa de salida. **La capa de recurrencia es la que almacena un estado interno o memoria que permite \"recordar\" los eventos pasados**. Al desplegar la RNN, cada iteración procesa una nueva observación de la secuencia. Cada observación procesada por la capa recurrente, tiene en cuenta los datos actuales y el estado interno generado por la observación anterior. Al inicio de la secuencia, el estado interno es 0.\n",
    "\n",
    "La forma de calcular el estado interno/oculto en una RNN es:\n",
    "\n",
    "\n",
    "$$h_t=f(x_t\\cdot u+h_{t-1}\\cdot v+b_h)$$\n",
    "\n",
    "\n",
    "\n",
    "considerando que $h_0=0$ y siendo:\n",
    "\n",
    "*  $f(\\cdot)$ la función de activación que, normalmente, es $\\tanh$ (en RNN simples).\n",
    "* $h_t$ el estado interno actual.\n",
    "* $h_{t-1}$ el estado interno del paso anterior.\n",
    "* $u$ la matriz de pesos que conecta la capa de entrada ($x_t$) con la capa recurrente (para calcular $h_t$).\n",
    "* $v$ la matriz de pesos que pondera el estado interno anterior ($h_{t−1}$) para el cálculo del estado interno actual $h_t$.\n",
    "* $b_h$ el vector de sesgo (bias) de la capa recurrente.\n",
    "\n",
    "\n",
    "\n",
    "Esta celda recurrente, se llama tantas veces como longitud tiene la serie temporal. Así, si la serie temporal tiene la forma:\n",
    "\n",
    "$$\\{x_1,x_2,x_3\\}$$\n",
    "\n",
    "los cálculos que se ejecutan son:\n",
    "\n",
    "$$h_1=f(x_1\\cdot u+h_{0}\\cdot v+b_1)$$\n",
    "$$h_2=f(x_2\\cdot u+h_{1}\\cdot v+b_h)$$\n",
    "$$h_3=f(x_3^T\\cdot u+h_{2}\\cdot *v+b_h)$$\n",
    "   \n",
    "La salida generada sería la secuencia (*many-to-many*): $[h_1,h_2,h_3]$\n",
    "\n",
    "o la última salida (*many-to-one*): $[h_3]$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Veremos durante las clases de teoría más detalles sobre este tipo de redes, pero entre las características interesantes cara a su uso en plantas industriales podemos destacar: \n",
    "* **La longitud de las secuencias no está limitada**. Una vez entrenadas pueden procesar secuencias de diferentes longitudes.\n",
    "    * Aunque en la práctica se suele fijar un tamaño de la secuencia cara a un uso y entrenamiento efectivo.\n",
    "* **Los pesos internos son compartidos para todas las iteraciones o pasos de la RNN**, independientemente de procesar secuencias mayores o menores.\n",
    "    * Cuanto más despleguemos una RNN más dificil será entrenarla.\n",
    "* Existen diferentes tipos de RNNs con **distintos tipos de celdas recurrentes**.\n",
    "    * Las RNNs simples tienen un problema potencial de desvanecimiento/explosión de gradiente, que dificulta su uso si es necesario \"recordar\" relaciones muy distantes en el tiempo\n",
    "    * Otro tipo de RNNs como las LSTMs o las GRUs, suelen ser más empleadas en la práctica por solucionar el problema del desvanecimiento/explosión del gradiente.\n",
    "* Dependiendo de las entradas y salidas que queremos considerar, podemos emplear **diferentes tipos de arquitecturas** para las RNNs. Los principales son: \n",
    "  -  *many-to-many*.\n",
    "  -   *one-to-many*.\n",
    "  -   *many-to-one*.\n",
    "\n",
    "**Nota**: Particularmente, en el control de procesos sobre líneas de producción secuenciales, la forma más típica es *many-to-one*. Una secuencia de operaciones es analizada con el objetivo de predecir algún tipo de variable de calidad final que ha sido influenciada por las operaciones previas.\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/RNNSimple_many_to_one.png\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd344c-521f-45f8-acf5-bf21dc3a0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import generales\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as err:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as err:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError as err:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "# Asegurarnos de usar Keras 3 con backend TensorFlow\n",
    "# Es necesario hacerlo antes de cargar Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Importaciones de Keras y TensorFlow\n",
    "try:\n",
    "    import keras\n",
    "except ImportError as err:\n",
    "    !pip install keras\n",
    "    import keras\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf \n",
    "except ImportError as err:\n",
    "    !pip install tensorflow\n",
    "    import tensorflow as tf \n",
    "\n",
    "\n",
    "\n",
    "# Importaciones de Scikit-learn\n",
    "#Solo para facilitarnos el uso de algunas operaciones típicas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# Configuraciones para visualización. No es necesario\n",
    "plt.style.use('seaborn-v0_8-whitegrid') # Estilo de gráficos\n",
    "\n",
    "\n",
    "\n",
    "# Verificación de versiones\n",
    "#Existen diferentes versiones y compatibilidades. Es importantes saber lo que estamos ejecutando\n",
    "#Para estos ejemplos queremos ejecutar el API de Keras3 (standalone) y tensorflo3>2.16\n",
    "print(f\"Versión de Keras: {keras.__version__}\")\n",
    "print(f\"Backend de Keras: {keras.backend.backend()}\")\n",
    "print(f\"Versión de TensorFlow: {tf.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "# Verificar si TensorFlow puede usar GPU en nuestro sistema (opcional, pero bueno saberlo)\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"GPU disponible: {gpu_devices}\")\n",
    "else:\n",
    "    print(\"GPU no encontrada, se usará CPU.\")\n",
    "\n",
    "\n",
    "#Establecemos una semilla para todos los procesos en los que lo necesitemos\n",
    "SEED=1234\n",
    "\n",
    "print(f\"la semilla que emplearemos para todos los procesos pseudoaleatorios es:{SEED}\")\n",
    "\n",
    "# Establece la semilla para los números aleatorios con  keras.utils.set_random_seed. Esto establecerá:\n",
    "# 1) `numpy` seed\n",
    "# 2) backend random seed\n",
    "# 3) `python` random seed\n",
    "#Permitir la reproducibilidad es clave para poder comparar\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ebecf-b625-45d1-a004-c0fb7efff0a3",
   "metadata": {},
   "source": [
    "## 2. Generación de un *dataset* sintético\n",
    "Para poder probar las RNNs vamos a crear un dataset sintético  que imite una fábrica con una línea de producción  secuencial dividida en **5 secciones**. \n",
    "\n",
    "1.  **Sensores en cada sección:**\n",
    "    * En cada sección, una máquina realiza una operación (ej: apretar un tornillo, calentar una pieza, etc.) y un sensor mide un parámetro clave de esa operación (ej: las revoluciones por minuto de un motor, la temperatura aplicada, etc.). Tendremos 5 series de datos, una por cada sensor de sección.\n",
    "    * Cada sección puede influir en la siguiente; por ejemplo, si una máquina en la sección 2 trabajó muy rápido, la sección 3 podría tener que ajustarse ligeramente.\n",
    "\n",
    "2.  **Variable de Calidad Final:**\n",
    "    * Al final de toda la línea, se inspecciona el producto y se mide una **variable de calidad** (ej: la resistencia de una soldadura, la humedad interna, el perfil de densidad, etc).\n",
    "    * Esta calidad final **NO depende solo de la última operación**. Más bien, es el resultado de una *combinación* de cómo se aplicaron los procesos previos en las diferentes secciones cuando ese producto pasó por ellas.\n",
    "        * Por ejemplo, la calidad podría depender mucho de lo que hizo la máquina de la sección 1 cuando el producto estuvo allí (hace 5 \"pasos de tiempo\"), un poco de la sección 2 (hace 3 \"pasos de tiempo\"), y así sucesivamente. Algunas secciones podrían no influir nada en esta variable de calidad particular.\n",
    "\n",
    "\n",
    "3.  **El Reto para la Red Neuronal:**\n",
    "    * Nuestro DataFrame mostrará, para cada producto, las lecturas actuales de los 5 sensores y su calidad final.\n",
    "    * La Red Neuronal Recurrente (RNN) recibirá secuencias de las lecturas de los sensores de varios productos consecutivos. Su tarea será **aprender y descubrir por sí misma cómo esas lecturas pasadas (y de qué secciones específicas y con qué \"retraso\") afectan a la calidad del siguiente producto**, sin que le digamos explícitamente \"fíjate en la sección 1 de hace 5 pasos\".\n",
    "\n",
    "Este dataset nos permitirá simular un problema realista donde es necesario analizar secuencias temporales para predecir un resultado futuro que tiene dependencias complejas con el pasado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cf7ce-d929-4653-97fd-cdb8fefc9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generación de Datos Sintéticos (Línea de Producción Secuencial)\n",
    "\n",
    "N_SAMPLES_FINAL = 20000 # Número de muestras finales en nuestro dataset\n",
    "N_SECTIONS = 5 #secciones de nuestra línea de producción\n",
    "SEQUENCE_LENGTH = 5 # Usaremos secuencias de 5 timesteps para predecir el siguiente.\n",
    "EXTRA_LENGTH_TO_EXERCISE=0 #¡ESTA VARIABLE SOLO LA MODIFICARÉIS PARA DESARROLLAR EL EJERCICIO!!\n",
    "SEQUENCE_LENGTH+=EXTRA_LENGTH_TO_EXERCISE\n",
    "# Parámetros para la generación de datos de sensores por sección\n",
    "# Lags (en pasos de tiempo para la contribución de cada sección a la calidad final.\n",
    "# L_s = k significa que el valor del sensor de la sección 's' hace k observaciones afectó la calidad del producto actual.\n",
    "# Ejemplo: L_s = 1 significa que P_{s, t-1} afecta a Q_t.\n",
    "SECTION_LAGS = np.array([5, 4, 3, 2, 1]) # Lags para S1, S2, S3, S4, S5 respectivamente\n",
    "\n",
    "SECTION_LAGS=SECTION_LAGS+EXTRA_LENGTH_TO_EXERCISE\n",
    "print(SECTION_LAGS)\n",
    "\n",
    "L_max = np.max(SECTION_LAGS) if len(SECTION_LAGS) > 0 else 0\n",
    "\n",
    "# Necesitamos generar más datos al principio para acomodar los retardos máximos.\n",
    "N_RAW_SAMPLES = N_SAMPLES_FINAL + L_max\n",
    "\n",
    "# Inicializar arrays para los valores de los sensores\n",
    "P_raw = np.zeros((N_SECTIONS, N_RAW_SAMPLES))\n",
    "time_raw = np.arange(N_RAW_SAMPLES)#pasos de tiempo\n",
    "\n",
    "# Parámetros base para cada sección (amplitud, frecuencia, offset, ruido)\n",
    "base_params_amplitude = [10, 8, 12, 7, 10]\n",
    "base_params_period = [50, 180, 10, 70, 45]\n",
    "base_params_offset = [100, 10, 120, 90, 110]\n",
    "base_params_noise_std = [2, 1.5, 2, 1, 1.8]\n",
    "\n",
    "\n",
    "\n",
    "# Sección 1 (S1) - Comportamiento base\n",
    "P_raw[0, :] = (base_params_amplitude[0] * np.sin(time_raw / base_params_period[0])) +  base_params_offset[0] + \\\n",
    "                5 * np.cos(time_raw / 20) + np.random.randn(N_RAW_SAMPLES) * base_params_noise_std[0]\n",
    "\n",
    "# Secciones 2 a 5 (S2-S5) - Influenciadas por la sección anterior\n",
    "# Coeficientes de influencia de la sección anterior (pequeños)\n",
    "alpha_influence = [0.15, -0.2, 0, 0.35] # Para S2 por S1, S3 por S2, etc.\n",
    "\n",
    "\n",
    "\n",
    "for s in range(1, N_SECTIONS): # Para S2, S3, S4, S5\n",
    "    influence_term = alpha_influence[s-1] * P_raw[s-1, :]#influencia de la sección anterior\n",
    "    base_behavior = (base_params_amplitude[s-1] * np.sin(time_raw / base_params_period[s-1]) +\n",
    "                     base_params_offset[s-1] +\n",
    "                     np.random.randn(N_RAW_SAMPLES) * base_params_noise_std[s-1])#añadimos algo de ruido típico en los sensores\n",
    "    P_raw[s, :] = influence_term + base_behavior#añadimos la influencia de la sección anterior\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generación de la Variable de Calidad Final (Q_t)\n",
    "#la variable de calidad está influenciada por el proceso 'n' hace 'k' pasos de tiempo definidos en SECTION_LAGS\n",
    "# Q_t = f(P_{1,t-L1}, P_{2,t-L2}, P_{3,t-L3}, P_{4,t-L4}, P_{5,t-L5}) \n",
    "# Los pesos (w_s) indican la contribución de cada sección a la variable final (medida de hace 'k' pasos de tiempo). Algunos pueden ser cero.\n",
    "quality_weights = np.array([2, 0.10, -3, 0.1, 0.1]) \n",
    "\n",
    "\n",
    "Q_raw = np.zeros(N_RAW_SAMPLES)\n",
    "base_quality_offset = 0  #valor base de la variable de calidad al que se le añade el efecto de las otras secciones\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Generamos una variable de calidad con dependencias temporales con un comportamiento lineal. \n",
    "#Se podría complicar más añadiendo dependencias no lineales y ruido\n",
    "#Recordad que las redes RNN simples tienen sus limitaciones\n",
    "\n",
    "for t in range(L_max, N_RAW_SAMPLES):   \n",
    "    Q_raw[t] = base_quality_offset\n",
    "    for i in range(N_SECTIONS):\n",
    "        Q_raw[t] += quality_weights[i] * P_raw[0, t - SECTION_LAGS[i]] #podríamos crear un comportamiento lineal\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#Generamos un dataframe por comodidad\n",
    "\n",
    "df_cols = {}\n",
    "for s in range(N_SECTIONS):\n",
    "    df_cols[f'Seccion_{s+1}'] = P_raw[s, L_max:]\n",
    "df_cols['Calidad'] = Q_raw[L_max:]\n",
    "\n",
    "data_df = pd.DataFrame(df_cols)\n",
    "print(f\"Estructura del dataset: {data_df.shape}\")\n",
    "\n",
    "print(\"\\nPrimeras filas del dataset generado:\")\n",
    "print(data_df.head(n=10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36961add-b6cf-4993-82e4-c047c7605c69",
   "metadata": {},
   "source": [
    "### 2.1. Exploración visual de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782ee56-d4b3-4bf8-9028-7e4ca2a97cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploración y Visualización de Datos\n",
    "# Visualizar las series temporales generadas para entender sus patrones.\n",
    "\n",
    "fig, axs = plt.subplots(N_SECTIONS + 1, 1, figsize=(14, 18), sharex=True)\n",
    "features_to_plot = [f'Seccion_{s+1}' for s in range(N_SECTIONS)] + ['Calidad']\n",
    "\n",
    "for i, feature_name in enumerate(features_to_plot):\n",
    "    axs[i].plot(data_df[feature_name], label=feature_name)\n",
    "    axs[i].set_ylabel(feature_name)\n",
    "    axs[i].legend(loc=\"upper right\")\n",
    "axs[-1].set_xlabel(\"Tiempo\")\n",
    "fig.suptitle(\"Series Temporales de Parámetros de Sección y Calidad del Producto\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2ebc6-14c1-467e-aa36-5d037b33b6a4",
   "metadata": {},
   "source": [
    "## 3. Preparación de los datos\n",
    "\n",
    "La generacion, configuración y el entrenamiento de las RNNs lo realizaréis, habitualmente, con librerías especializadas como, por ejemplo Keras, por lo que  esa dificultad queda \"externalizada\". La parte, tipicamente, más compleja es el preprocesado y preparación de los datos. La parte de preprocesado es similar a la que nos podemos encontrar en cualquier desarrollo de modelos de aprendizaje automática PERO debemos tener en cuenta las dependencias temporales intrínsicas de los datos a la hora de realizar operaciones. En este caso tan simplificado, simplemente realizaremos una estandarización de los entradas.\n",
    "\n",
    "### 3.1 División de los datos (entrenamiento, validación y test)\n",
    "\n",
    "\n",
    "**La división de los datos entre entrenamiento, validación y test SÍ que es necesario planteársela cuidadosamente** cuando se trabaja en plantas industriales en general y en particular cuando existen dependencias temporales.\n",
    "\n",
    "**IMPORTANTE**: *Los siguientes comentarios surgen del uso práctico de RNNs en entornos de factorías reales. Consultad cualquier duda respecto a ellos*.\n",
    "\n",
    "\n",
    "* La típica **división aleatoria no podemos hacerla directamente, ya que perderíamos las dependencias temporales**.\n",
    "* Si repartimos los **datos secuencialmente** no perderíamos las dependencias temporales pero puede ser **problemático** si el caso de uso  que estamos modelando está afectado por **estacionalidades**.\n",
    "    - *Imaginad la situación en que se entrena con datos de invierno y se testea con datos de verano. Si el problema está afectado por estacionalidades, el modelo no habrá generalizado correctamente*.\n",
    "\n",
    "* En caso de generar un **dataset formado por secuencias** (agrupando observaciones para no perder las dependencias temporales), en lugar de observaciones independientes, podríamos **repartir aleatoriamente las secuencias para no tener problemas con la estacionalidad a la vez que mantenemos las dependencias temporales PERO** debemos tener en cuenta varias situaciones:\n",
    "1. Típicamente, en las líneas de procesamiento, **el proceso cambia de forma suave. No existen grandes saltos del instante $t$ al $t+1$**, eso tiene como consecuencia que si las secuencias se reparten completamente aleatorias, caeran en el conjunto de validación secuencias muy parecidas a las que están en entrenamiento, generando métricas muy optimistas, ya que la validación mejora al mismo ritmo que el entrenamiento. Este problema no ocurre si las secuencias se reparte de forma secuencial.\n",
    "    * Una opción para evitar este problema es \"alejar\" las secuencias unas de otras. Forzar a que exista un mínimo de pasos temporales entre secuencias.\n",
    "    * Otra opción es generar pequeños lotes secuencias repartidos aleatoriamente por el dataset.\n",
    "     ![MIoT_GDPI](img/reparto_aleatorio_lotes.png)\n",
    "<p style=\"text-align:center; font-style:italic;\">Reparto aleatorio de lotes secuenciales para evitar perder las dependencias temporales y solucionar el problema de la estacionalidad.</p>\n",
    "\n",
    "2. **¿Las secuencias se generan con solape o sin solape?** (problema intimamente relacionado con el anterior) Si se generan con solape tendremos muchas secuencias muy parecidas y podemos caer facilmente en un sobreentrenamiento. **En general, mi recomendación es emplear solape en los conjuntos de prueba**, para poder ver la evolución en el tiempo, **y sin solape en el entrenamiento.**\n",
    "\n",
    "|                                               |\n",
    "| -------------------------------------------- |\n",
    "| $$s_1=\\{x_i,x_{i+1},...,x_{n-1},x_{n}\\}$$ |\n",
    "|$$s_2=\\{x_{i+1},x_{i+2},...,x_{n},x_{n+1}\\}$$|\n",
    "| *Ejemplo de 2 secuencias solapadas.* |\n",
    "\n",
    "\n",
    "|                                               |\n",
    "| -------------------------------------------- |\n",
    "| $$s_1=\\{x_i,x_{i+1},...,x_{n-1},x_{n}\\}$$ |\n",
    "|$$s_2=\\{x_{i+1},x_{i+2},...,x_{n},x_{n+1}\\}$$|\n",
    "| *Ejemplo de 2 secuencias sin solape.* |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "El problema que estamos modelando en este ejemplo **no tienen problemas de estacionalidad**, por lo que vamos a repartir los datos en secuencias (para no perder las dependencias temporales) y, a continuación, generaremos las secuencias para unir las observaciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ee358-4b08-4e1d-bba4-6b2e99c18958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separar_inputs_outputs(dataset):\n",
    "    \"\"\"Separa un DataFrame de Pandas en características (inputs) y la columna objetivo 'Calidad'.\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): El DataFrame de Pandas que se va a procesar.\n",
    "            Se espera que este DataFrame contenga las características de entrada\n",
    "            y, potencialmente, la columna objetivo 'Calidad'.\n",
    "\n",
    "    Returns:\n",
    "          Una tupla con dos elementos:\n",
    "            1. inputs (pd.DataFrame | None): Un DataFrame con las características\n",
    "               (todas las columnas del dataset original excepto 'Calidad').\n",
    "               Retorna `None` si la columna 'Calidad' no se encuentra en el\n",
    "               `dataset`.\n",
    "            2. outputs (pd.Series | None): Una Serie de Pandas con la columna\n",
    "               'Calidad'. Retorna `None` si la columna 'Calidad' no se\n",
    "               encuentra en el `dataset`.\n",
    "    \"\"\"\n",
    "    if 'Calidad' in dataset.columns: \n",
    "        return dataset.drop('Calidad', axis=1), dataset['Calidad']\n",
    "    else: return None, None\n",
    "\n",
    "def separar_conjuntos(dataset, perc_entrenamiento=0.7):\n",
    "    \"\"\"Divide un conjunto de datos secuencial en entrenamiento, validación y prueba.\n",
    "    - El conjunto de entrenamiento obtiene `perc_entrenamiento` del total de muestras.\n",
    "    - El porcentaje restante, `(1 - perc_entrenamiento)`, se divide equitativamente\n",
    "      entre los conjuntos de validación y prueba.\n",
    "    Args:\n",
    "        dataset: El conjunto de datos completo a dividir.\n",
    "        perc_entrenamiento (float, optional): Proporción del dataset a asignar al\n",
    "            conjunto de entrenamiento. \n",
    "\n",
    "    Returns:\n",
    "        Tuple[Sequence, Sequence, Sequence]: Una tupla con tres subconjuntos de datos:\n",
    "            1. El subconjunto de entrenamiento.\n",
    "            2. El subconjunto de validación.\n",
    "            3. El subconjunto de prueba.\n",
    "    \"\"\"\n",
    "    perc_entrenamiento=perc_entrenamiento if 0<perc_entrenamiento<1 else 0.7\n",
    "    \n",
    "    num_train_samples = int(perc_entrenamiento * len(dataset))\n",
    "    num_val_samples =int((1-perc_entrenamiento)/2 * len(dataset))\n",
    "    num_test_samples = len(dataset) - num_train_samples - num_val_samples\n",
    "    return dataset[:num_train_samples], \\\n",
    "            dataset[num_train_samples : num_train_samples + num_val_samples],\\\n",
    "            dataset[num_train_samples + num_val_samples:]\n",
    "    \n",
    "#Dependiendo la estrategia deberemos crear las secuencias antes o después del reparto    \n",
    "#separamos en 3 conjuntos en base al porcentaje de entrenamiento\n",
    "train_set, val_set, test_set=separar_conjuntos(data_df,perc_entrenamiento=0.7)\n",
    "\n",
    "#separamos los conjuntos en inputs y outputs\n",
    "#podría haberse realizado todo desde la misma función\n",
    "train_features, train_targets=separar_inputs_outputs(train_set)\n",
    "val_features, val_targets=separar_inputs_outputs(val_set)\n",
    "test_features, test_targets=separar_inputs_outputs(test_set)\n",
    "\n",
    "print(f\"Estructura de entrenamiento (features): {train_features.shape}\")\n",
    "print(f\"Estructura de entrenamiento (outputs): {train_targets.shape}\")\n",
    "\n",
    "print(f\"Estructura de validación (features): {val_features.shape}\")\n",
    "print(f\"Estructura de validación (outputs): {val_targets.shape}\")\n",
    "\n",
    "print(f\"Estructura de test (features): {test_features.shape}\")\n",
    "print(f\"Estructura de test (outputs): {test_targets.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb0350-3fd2-4d65-9be1-9198d45c0e87",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Estandarización de los datos con Keras\n",
    "\n",
    "\n",
    "En lugar de estandarizar los datos a través de una operación de `scikit-learn` (sistema que ya conocéis), lo haremos desde el propio Keras generando una [capa de preprocesado](https://keras.io/api/layers/preprocessing_layers/). Esto tiene como ventaja poder incluirlo en el propio modelo y que este sea autosuficiente. Debéis de recordar que el entrenamiento de los modelos es solo la primera fase, luego es necesario desplegarlos en producción y trabajar con ellos. Poder guardar con el modelo toda la información necesaria para poder emplearlo (ej. capa de estandarización), simplifica su empleo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180a8ac-63d9-4fb7-bb36-cedd97bceeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarización de las características de entrada\n",
    "\n",
    "normalization_layer = keras.layers.Normalization(axis=-1)\n",
    "normalization_layer.adapt(train_features.values) # ajustar SOLO con datos de entrenamiento las características de entrada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566a7fa-0c03-4439-bd38-607dccd644b8",
   "metadata": {},
   "source": [
    "## 3.2 Preparación de las secuencias de datos\n",
    "Para poder  entrenar modelos recurrentes es necesario crear  las secuencias que vamos a emplear para entrenarlos. Aunque las RNNs pueden soportar secuencias de diferentes tamaños, lo habitual es fijar un tamaño $k$, para poder entrenarlos de forma eficiente.\n",
    "\n",
    "Generar las secuencias para los modelos RNN de keras implica preparar los datos de forma que se generen tensores de la forma `(batch_size, timesteps, features)`:\n",
    "* `batch_size` (tamaño del lote): Es el número de secuencias que se procesan simultáneamente en una pasada de entrenamiento.\n",
    "* `timesteps` (pasos de tiempo o longitud de la secuencia): Es el número de observaciones consecutivas en cada secuencia individual que se van a emplear para entrenar.\n",
    "* `features` (características): Es el número de variables o características que observas en cada paso de tiempo.\n",
    "\n",
    "Aunque se puede realizar a mano, para preparar las secuencias respecto a nuestros *datasets* emplearemos [keras.utils.timeseries_dataset_from_array](https://keras.io/api/data_loading/timeseries/), ya que facilita el desarrollo de los tensores. Su principal objetivo es facilitar la preparación de datos para modelos que aprenden de secuencias, como las RNNs simples, LSTMs, GRUs o Transformers, cuando se trabaja con datos de series temporales.\n",
    "\n",
    "\n",
    "\n",
    "Importante tener claro cómo se le pasan los datos a `keras.utils.timeseries_dataset_from_array` ([ver ejemplo 2 manual](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array)).\n",
    "\n",
    "*Considera un array data de valores escalares, con forma (*steps*,). Para generar un conjunto de datos que utilice los k pasos de tiempo anteriores para predecir el siguiente paso de tiempo, usarías:*\n",
    "\n",
    "<code>\n",
    "k=10\n",
    "input_data = data[:-k]\n",
    "targets = data[k:]\n",
    "dataset = timeseries_dataset_from_array(input_data, targets, sequence_length=k)\n",
    "</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d8dcd-e7b5-4d6c-9a57-afe87687e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Creación de Secuencias (Ventanas de Tiempo)**\n",
    "# empleando `tf.keras.utils.timeseries_dataset_from_array`.\n",
    "# el formato generado es (batch_size, timesteps, features)\n",
    "\n",
    "\n",
    "#para generar la salida del instante 't+1' \n",
    "#necesitamos la secuencia [i:i+SEQUENCE_LENGTH-1]\n",
    "#x[i], ..., x[i + SEQUENCE_LENGTH - 1] es la secuencia que termina en el instante t = i + SEQUENCE_LENGTH - 1)\n",
    "# se usa para predecir y[i + sequence_length] (que es el valor del objetivo en t+1).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generacion_secuencias_temporales(features, targets, seq_len, solape=False, shuffle=True):\n",
    "    BATCH_SIZE = 32    \n",
    "    seq_stride=1 if  solape else seq_len\n",
    "\n",
    "    return  keras.utils.timeseries_dataset_from_array(\n",
    "        data=features,\n",
    "        targets=targets,\n",
    "        sequence_length=seq_len,\n",
    "        sequence_stride= seq_stride,#importante decidir si queremos o no secuencias con solape\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=shuffle,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#preparamos los datos a pasar (fijaos en la explicación anterior. SEQUENCE_LENGTH=k)\n",
    "train_features_rnn=train_features[:-SEQUENCE_LENGTH]\n",
    "val_features_rnn=val_features[:-SEQUENCE_LENGTH]\n",
    "test_features_rnn=test_features[:-SEQUENCE_LENGTH]\n",
    "train_targets_rnn=train_targets[SEQUENCE_LENGTH:]\n",
    "val_targets_rnn=val_targets[SEQUENCE_LENGTH:]\n",
    "test_targets_rnn=test_targets[SEQUENCE_LENGTH:]\n",
    "\n",
    "#genemos los datasets con secuencias\n",
    "train_dataset=generacion_secuencias_temporales(train_features_rnn, train_targets_rnn, solape=False, seq_len=SEQUENCE_LENGTH,shuffle=False)\n",
    "val_dataset=generacion_secuencias_temporales(val_features_rnn, val_targets_rnn, solape=True, seq_len=SEQUENCE_LENGTH, shuffle=False)\n",
    "test_dataset=generacion_secuencias_temporales(test_features_rnn, test_targets_rnn, solape=True,  seq_len=SEQUENCE_LENGTH, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nInspección de un batch del dataset de entrenamiento:\")\n",
    "for batch_inputs, batch_targets in train_dataset.take(1):\n",
    "    print(\"Forma de las entradas del lote (batch_inputs):\", batch_inputs.shape) # (BATCH_SIZE, SEQUENCE_LENGTH, N_SECTIONS)\n",
    "    print(\"Forma de las etiquetas del lote (batch_targets):\", batch_targets.shape) # (BATCH_SIZE,)\n",
    "    print(\"\\nVisualización de un elemento (inputs):\")\n",
    "    print(\"\\nVisualización de un elemento (outputs):\")\n",
    "    print(batch_inputs[0])\n",
    "    print(batch_targets[0])\n",
    "\n",
    "\n",
    "#comprobamos que hemos ajustado bien las entradas y las salidas\n",
    "for batch in train_dataset:\n",
    "  inputs, targets = batch\n",
    "  assert np.array_equal(inputs[0], train_features[:SEQUENCE_LENGTH])  # First sequence: steps [0-SEQUENCE_LENGTH]\n",
    "  assert np.array_equal(targets[0], train_targets[SEQUENCE_LENGTH])\n",
    "  break\n",
    "\n",
    "print(f\"\\nBatches de entrenamiento: {len(train_dataset)}\")\n",
    "print(f\"Batches de validación: {len(val_dataset)}\")\n",
    "print(f\"Batches de test: {len(test_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e1808-d45a-426d-8131-4840eb50fc19",
   "metadata": {},
   "source": [
    "## 4. Definición del modelo \n",
    "\n",
    "Desarrollaremos un modelo simple RNN con una capa de entrada, una oculta RNN y una densa de salida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f013c93-6781-46b9-8d4b-93e3b596b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del Modelo RNN\n",
    "# Modelo secuencial con: Input, Normalization, SimpleRNN, Dense (salida).\n",
    "#Si type_rnn=False, generaría un modelo denso feedforward\n",
    "\n",
    "def get_model(seq_len, num_model_features, preprocesing_layers, name_model, type_rnn=True):\n",
    "    hidden_units=64\n",
    "\n",
    "\n",
    "    #input layer\n",
    "    model = keras.Sequential(name=name_model)\n",
    "\n",
    "    shape=(seq_len, num_model_features) if type_rnn else (num_model_features, )\n",
    "    model.add( keras.Input(shape=shape, name=\"input_layer\"))\n",
    " \n",
    "    #preprocesing layers\n",
    "    for layer in preprocesing_layers:\n",
    "       model.add(layer)\n",
    "\n",
    "    #Hidden layer\n",
    "    hidden_layer= keras.layers.SimpleRNN(units=hidden_units, activation='tanh', name=\"hidden_layer\") if type_rnn \\\n",
    "                                    else keras.layers.Dense(hidden_units, activation='tanh', name=\"hidden_layer\")\n",
    "\n",
    "    model.add(hidden_layer)\n",
    "    model.add( keras.layers.Dense(units=1,  activation='linear', name=\"output\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb512a1-1bc2-4615-a116-29a68db2d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model=get_model(SEQUENCE_LENGTH, N_SECTIONS, [normalization_layer], \"rnn_model\", type_rnn=True)\n",
    "rnn_model.summary()\n",
    "modelo_reseteado=True #para gestionar si el modelo se reentrena desde cero o desde un entreno previo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006ff94-0454-421e-ba2e-540147aa5462",
   "metadata": {},
   "source": [
    "### 4. Compilación del modelo\n",
    "\n",
    "\n",
    "Antes de entrenar, necesitamos \"compilar\" el modelo para configurar su proceso de entrenamiento. Esto implica definir:\n",
    "\n",
    "* **Optimizador**: Algoritmo que ajusta los pesos de la red durante el entrenamiento (ej. Adam, SGD, RMSprop). Adam es una opción robusta y popular. Tenéis [aquí](https://keras.io/api/optimizers/) un listado de los disponibles. La configuración de los parámetros del optimizador sería algo para optimizar en la búsqueda hiperparamétrica.\n",
    "\n",
    "* **Función de Pérdida** (*Loss Function*): Mide qué tan bien se desempeña el modelo durante el entrenamiento. Para regresión, mean_squared_error (MSE) es una opción muy típica. Tenéis [otras](https://keras.io/api/losses/) disponibles.\n",
    "\n",
    "\n",
    "* **Métricas**: Funciones para evaluar el rendimiento del modelo (ej. mean_absolute_error - MAE). **Las métricas no se usan para optimizar el modelo, solo para monitorizarlo**. Tenéis [aquí](https://keras.io/api/metrics/) un listado de las disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc0292-f7a4-4f56-83b9-dd62b38e402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del Modelo\n",
    "# Optimizador Adam, pérdida MSE, métrica MAE.\n",
    "\n",
    "rnn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8264694e-2378-44e8-b73d-036f4d91bb42",
   "metadata": {},
   "source": [
    "### 5. Entrenamiento del modelo\n",
    "\n",
    "Ya tenemos definido y configurado el modelo para poder entrenarse con nuestro  dataset de entrada. Recordad que el **modelo contiene una capa de normalización**, por lo que **los datos que se le pasen para entrenar NO pueden estar normalizados** ( lo hará el propio modelo).\n",
    "\n",
    "\n",
    "El método `fit` es el empleado para entrenar. Tenéis [aquí](https://keras.io/api/models/model_training_apis/#fit-method) disponible todos los parámetros posibles. En este caso emplearemos:\n",
    "\n",
    "* **epochs**: Número de veces que el modelo verá el conjunto de datos completo.\n",
    "* **batch_size**: Número de muestras procesadas antes de actualizar los pesos (entrenamiento con *mini-batches*).\n",
    "* **validation_data**: conjunto de datos sobre los que validaremos el entrenamiento del modelo. Lo podemos emplear para comparar configuraciones o para parar el entrenamiento de forma prematura (*early_stopping*), lo que puede ayudar a detectar el *overfitting*.\n",
    "\n",
    "El método `fit` devuelve un objeto *History* que contiene información sobre el proceso de entrenamiento (pérdida y métricas en cada época) que podremos utilizar posteriormente para graficar el proceso de entreno y poder valorar si tenemos *overfitting* o *underfitting*.\n",
    "\n",
    "En este caso definiremos un *callback* que permita parar el entrenamiento cuando las métricas sobre el conjunto de validación dejen de mejorar y nos quedamos con el mejor modelo visto.\n",
    "\n",
    "**IMPORTANTE**: cada vez que ejecutéis el entrenamiento, el modelo parte de su último estado. Si queréis volver a entrenarlo desde cero, entonces tenéis que volver a crearlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d344b8-132a-4295-a691-b1600f6ec614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del Modelo\n",
    "EPOCHS = 1000 # Aumentamos un poco por la posible mayor complejidad\n",
    "# Considerar EarlyStopping en una implementación real.\n",
    "\n",
    "if not modelo_reseteado:\n",
    "    print(\"IMPORTANTE: el modelo se está reentrenando sin resetear!!!!\")\n",
    "callback_early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10, # Número de épocas sin mejora antes de detener\n",
    "    restore_best_weights=True # Restaurar los pesos del modelo de la mejor época\n",
    ")\n",
    "\n",
    "rnn_history = rnn_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[callback_early_stopping] # Añadimos el callback\n",
    ")\n",
    "modelo_reseteado=False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac55e6f-7e28-4e23-b6db-75984a059272",
   "metadata": {},
   "source": [
    "### 6.  Evaluación del Modelo\n",
    "\n",
    "Una vez entrenado el modelo, tenemos que evaluar su rendimento. Lo haremos desde dos puntos de vista:\n",
    "\n",
    "1. **Visualizando el historial de entrenamiento**: Graficaremos la pérdida y la/s métrica/s disponibles (el MAE en este caso) tanto para el conjunto de entrenamiento como para el de validación a lo largo de las épocas. Esto nos ayuda a diagnosticar problemas como el *overfitting* (cuando la pérdida de validación empieza a aumentar mientras la de entrenamiento sigue bajando) o el *underfitting* (cuando ambas pérdidas son muy altas).\n",
    "2. **Evaluando en el conjunto de Test**: Usaremos el método `evaluate` con los datos de Test, que el modelo nunca ha visto, para generar la  estimación final e imparcial del rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9fe177-490e-43ed-bc3f-d7fc69915d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficas_entrenamiento(history, model_name=\"\"):\n",
    "    \"\"\"Genera y muestra gráficos de pérdida y MAE del entrenamiento de un modelo Keras.\n",
    "\n",
    "    Esta función toma un objeto `History` de Keras (devuelto por el método `fit()`)\n",
    "    y grafica dos subplots:\n",
    "    1.  La pérdida de entrenamiento ('loss') y la pérdida de validación ('val_loss')\n",
    "        contra el número de épocas.\n",
    "    2.  El MAE de entrenamiento  y el MAE de validación contra el número de épocas.\n",
    "\n",
    "    Args:\n",
    "        history_obj (keras.callbacks.History): El objeto `History` que Keras\n",
    "            retorna después de entrenar un modelo con `model.fit()`.\n",
    "        model_name: nombre del modelo que generó los datos históricos de entrenamiento\n",
    "    \"\"\"      \n",
    "    \n",
    "    hist=pd.DataFrame(history.history)\n",
    "    #hist['epoch'] = hist.epoch\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot( hist['loss'], label='Pérdida Entrenamiento')\n",
    "    plt.plot( hist['val_loss'], label = 'Pérdida Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida (MSE)')\n",
    "    plt.title(f'{model_name} - Pérdida (MSE) durante Entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Asegúrate de usar el nombre correcto de la métrica (puede variar ligeramente)\n",
    "    mae_key = 'mae' if 'mae' in hist.columns else list(hist.columns)[1] # Intenta encontrar la clave MAE\n",
    "    val_mae_key = 'val_' + mae_key\n",
    "    plt.plot(hist[mae_key], label='MAE Entrenamiento')\n",
    "    plt.plot( hist[val_mae_key], label = 'MAE Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Error Absoluto Medio (MAE)')\n",
    "    plt.title(f'{model_name} - MAE durante Entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Graficar el entrenamiento por épocas. Ayuda a valorar el posible (sobre/infra)entrenamiento\n",
    "\n",
    "graficas_entrenamiento(rnn_history, model_name=rnn_model.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031eeb8f-7d5a-4df6-b47a-bf412f36f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_metrics(model, test_dataset, targets=None):\n",
    "    \n",
    "    print(f\"Metricas del modelo: {model.name}\")\n",
    "    if targets is None: #se pasan los datos como un dataset de tensorflow con inputs y ouputs juntos (secuencias)\n",
    "        test_loss, test_mae = model.evaluate(test_dataset, verbose=0)#para las redes recurrentes\n",
    "    else:\n",
    "        test_loss, test_mae = model.evaluate(test_dataset, targets,verbose=0)#para las redes densas\n",
    "        \n",
    "    print(f\"{model.name} - Pérdida (MSE) en Test: {test_loss:.4f}\")\n",
    "    print(f\"{model.name} - Error Absoluto Medio (MAE) en Test: {test_mae:.4f}\")\n",
    "    \n",
    "def print_seq__pred_vs_real(model, test_dataset, targets=None):\n",
    "    print(f\"Comparativa del modelo: {model.name}\")\n",
    "    predictions = model.predict(test_dataset, verbose=0)\n",
    "\n",
    "    if targets is None: \n",
    "        real_test=[batch_targets_test.numpy() for _, batch_targets_test in test_dataset]#devuelve una lista de lista. Los outputs están agrupados en batches de 32\n",
    "        #print(f\"numero de batches: {len(real_test)} - Elementos en el batch: {len(real_test[0])}\")\n",
    "        actual_real_test = np.concatenate(real_test)#contactenamos todas las listas dentro de la lista (numero de batches) en una sola lista\n",
    "    else:\n",
    "        actual_real_test=targets.reset_index()\n",
    "\n",
    "\n",
    "    print(actual_real_test[:10])\n",
    "    r2 = r2_score(actual_real_test, predictions)\n",
    "\n",
    "    plot_range = min(300, len(actual_real_test))\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(actual_real_test[:plot_range], label='Valores Reales ', color='blue', marker='.', linestyle='-')\n",
    "    plt.plot(predictions[:plot_range, 0], label='Predicciones del Modelo', color='red', marker='x', linestyle='--')\n",
    "    plt.title(f'{model.name} - R² en el conjunto de Test: {r2:.4f} - Comparación de Calidad Real vs. Predicha (Primeros {plot_range} productos del Test Set)')\n",
    "    plt.xlabel('Observaciones ordenadas en el tiempo')\n",
    "    plt.ylabel('Variable de Calidad')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "print_test_metrics(rnn_model, test_dataset)\n",
    "print_seq__pred_vs_real(rnn_model, test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0e845-d92e-4eba-920f-6b09734be3da",
   "metadata": {},
   "source": [
    "#### EJERCICIO PARA ENTREGAR EN EL AULA VIRTUAL\n",
    "\n",
    "Vais a forzar el problema de las RNNs simples (desvanecimiento/exposión de gradiente). Para ello tenéis que hacer lo siguiente:\n",
    "1. Asignar un valor a EXTRA_LENGTH_TO_EXERCISE para que genere secuencias y dependencias temporales mayores (ej. 10)\n",
    "2. Ejecutaréis y compararéis las siguientes versiones del modelo:\n",
    "    * Un modelo con una RNN simple con una función de activación `tanh`. Es el que puedes generar por defecto empleando las funciones existentes en este Notebook. Ese modelo no debería poder predecir correctamente al tener dependencias temporales tan alejadas.\n",
    "    * Un modelo con una RNN simple con una función de activación `relu`. Esa función de activación debería mejorar el entrenamiento y atenuar el problema del desvanecimiento de gradiente. **Necesitaréis modificar la función `get_model` para poder crear modelos diferentes en base a algún parámetro y compararlos.**\n",
    "    * Un modelo con una capa LSTM en lugar de la RNN simple y una función de activación `tanh`. Las LSTMs y las GRUs son los modelos RNN más empleados **Necesitaréis modificar la función `get_model` para poder crear modelos diferentes en base a algún parámetro y compararlos.**\n",
    "    * Un modelo con una capa GRU en lugar de la RNN simple y una función de activación `tanh`. Las LSTMs y las GRUs son los modelos RNN más empleados. **Necesitaréis modificar la función `get_model` para poder crear modelos diferentes en base a algún parámetro y compararlos.**\n",
    "3. Despues de ejecutar y comparar los modelos, cread una celda de texto en la que resumáis los resultados obtenidos.\n",
    "    * Incluid imágenes y métricas para apoyaros.\n",
    "  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
