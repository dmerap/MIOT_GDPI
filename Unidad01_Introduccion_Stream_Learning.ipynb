{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c81d538",
   "metadata": {},
   "source": [
    "![MIoT_GDPI](img/MIOT_GDPI_header.png)\n",
    "\n",
    "# Introducción al concepto de Stream Learning\n",
    "\n",
    "El tiempo es una característica clave de la mayoría de las actividades humanas de interés. Ya sea la evolución temporal del mercado de valores y la economía, la dinámica de las redes sociales y otros sistemas complejos, o la entrada sensorial a los robots, el flujo del tiempo es un factor importante para la comprensión del sistema y la toma de decisiones. Con este flujo de tiempo, la información se actualiza y evoluciona. Ante estos cambios, nos adaptamos a la nueva información, interpretamos su significado y modificamos, si es necesario, nuestras suposiciones subyacentes. Por lo tanto, los sistemas de IA en tiempo real deben ser capaces de adaptarse de manera similar para ser efectivos.\n",
    "\n",
    "Los datos continuos basados en el tiempo se consideran \"flujos\". Los algoritmos de aprendizaje de IA que pueden adaptarse a estos flujos de manera \"incremental\" han dado lugar a un nuevo paradigma, conocido como aprendizaje en flujo en tiempo real o aprendizaje incremental. Debido a que los sistemas reales están caracterizados por fenómenos que pueden ocurrir en escalas de tiempo cortas o largas, estos sistemas pueden analizar información en múltiples escalas. Por ejemplo, el comportamiento a corto plazo podría ser las fluctuaciones de temperatura en un flujo de datos a lo largo de un día, mientras que el comportamiento a largo plazo podría considerar el promedio de temperaturas a lo largo de décadas.\n",
    "\n",
    "Entonces, ¿cómo podría un sistema de IA proporcionar predicciones precisas en estas dos escalas de tiempo radicalmente diferentes? Más importante aún, ¿cómo podría un sistema de IA comprender que el flujo de datos entrante está evolucionando continuamente y que los fenómenos subyacentes pueden deberse a nuevas tendencias que no estaban presentes en los datos de entrenamiento originales? Estas son precisamente las preguntas que abordan los sistemas de aprendizaje incremental o en flujo.\n",
    "\n",
    "###  Comparativa con el ML clásico\n",
    "\n",
    "Para comprender mejor el aprendizaje en flujo o incremental, es útil compararlo con un enfoque tradicional de aprendizaje automático. Como ejemplo, consideremos una señal basada en el tiempo, como la de un sensor de temperatura, que consta de dos elementos de datos: la marca de tiempo y la amplitud correspondiente (un voltaje escalado, correlacionado con la temperatura).\n",
    "\n",
    "Un algoritmo clásico de aprendizaje automático podría obtener información en múltiples escalas temporales analizando primero la relación a corto plazo de la señal; esto se logra mediante el uso de una ventana deslizante y analizando las características de la señal dentro de cada una de estas ventanas. Con ventanas más grandes y datos históricos, el algoritmo clásico podría estudiar estructuras temporales más largas.\n",
    "\n",
    "De esta manera, un enfoque tradicional de aprendizaje automático generalmente construiría un conjunto de datos para entrenamiento y prueba. Este conjunto de datos incluiría varias secciones de la señal que corresponden a características y salidas en diferentes escalas temporales, proporcionando cierto grado de predictibilidad tanto para eventos futuros a corto como a largo plazo.\n",
    "\n",
    "Sin embargo, las limitaciones de este enfoque se hacen evidentes de inmediato. En primer lugar, las predicciones se vuelven más complejas porque el sistema debe distinguir entre las diferentes escalas temporales (y los límites entre ellas). Si el sistema debe hacer una predicción basada en un solo punto de datos de entrada, ¿cómo se incorpora la información multiescala en el algoritmo de decisión? En otras palabras, la evolución a largo plazo de la entrada no es evidente en un solo punto de datos, lo que dificulta capturar predicciones a largo plazo sobre cambios subyacentes.\n",
    "\n",
    "Como resultado, los enfoques clásicos de aprendizaje automático para la predicción multitemporal suelen ser ad-hoc y dependen de modelos diseñados específicamente para adaptarse a la evolución de los fenómenos subyacentes, lo que hace que el entrenamiento previo sea incompleto o inexacto. Por lo tanto, el enfoque común para abordar este problema es reentrenar y volver a desplegar el modelo de aprendizaje automático de manera periódica. En algunos sistemas, estas actualizaciones del modelo se realizan en escalas de tiempo cortas, como minutos o incluso segundos.\n",
    "\n",
    "### el nuevo Paradigma: \n",
    "\n",
    "Para abordar de manera efectiva los flujos temporales y los problemas asociados descritos anteriormente, surge el aprendizaje en flujo e incremental (anteriormente llamado aprendizaje en línea). Estos son modelos de IA específicos diseñados para manejar flujos de datos continuos (y, a menudo, infinitos). Con cada nuevo punto de datos que estos sistemas reciben, ajustan continuamente el modelo de predicción en lugar de utilizar conjuntos de datos estáticos (o en grandes lotes).\n",
    "\n",
    "Si bien la diferencia entre el aprendizaje en flujo y el aprendizaje clásico puede parecer sutil, existen varias diferencias técnicas estructurales que son importantes de considerar. Estas conferencias describen tales cuestiones y los métodos para implementar el aprendizaje incremental o en flujo.\n",
    "\n",
    "Además, este enfoque también abre nuevas oportunidades, que se abordarán en las siguientes secciones junto con los desafíos asociados.\n",
    "\n",
    "Sin embargo, antes de comenzar, proporcionaremos algunas definiciones formales y conceptos básicos.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b9fb515",
   "metadata": {},
   "source": [
    "## Flujos de datos (Data Streams)\n",
    "\n",
    "En el núcleo del aprendizaje en flujo se encuentra el término “flujo de datos” (data stream), que se define como la recopilación secuencial y continua de elementos individuales coherentes. En el contexto de la información procesada por un algoritmo o sistema de aprendizaje automático, este flujo de datos normalmente consiste en datos (o metadatos) que pueden utilizarse para generar un conjunto de características medidas simultáneamente en el momento de su recepción. Estos datos suelen ser dependientes del tiempo o estar correlacionados temporalmente mediante una marca de tiempo.\n",
    "\n",
    "Una observación (o muestra) se define como el conjunto de características medidas en un instante específico. Estas muestras pueden tener una estructura de datos estable, es decir, en cada muestra están disponibles los mismos parámetros para ser incluidos en la medición o modelo, o bien, la estructura de datos de la muestra puede ser “dinámica” o flexible, donde los parámetros aparecen intermitentemente a lo largo del tiempo.\n",
    "\n",
    "Por lo tanto, en el contexto del procesamiento de información, entendemos un flujo de datos como un conjunto continuo de muestras de datos a lo largo del tiempo.\n",
    "\n",
    "\n",
    "\n",
    "### Flujos de datos reactivos y proactivos\n",
    "\n",
    "\n",
    "Los flujos de datos pueden clasificarse en dos tipos: reactivos y proactivos, dependiendo de su relación con el usuario.\n",
    "\n",
    "Los flujos de datos reactivos son aquellos en los que el sistema \"recibe\" datos de un productor. Un ejemplo típico es un sitio web que genera flujos de datos, como el servidor de datos de 'X'. A través de la interacción con el punto de acceso del sitio web, se obtiene el flujo de datos, que puede usarse para procesamiento posterior. En este caso, la forma en que se produce el flujo de datos no está bajo el control del sistema de procesamiento; un sistema de análisis como el aprendizaje en flujo simplemente actúa como receptor y no tiene influencia ni control más allá de recibir y leer los datos. En este sentido, el sistema de aprendizaje y predicción en flujo \"reacciona\" a la entrada de datos.\n",
    "\n",
    "Los flujos de datos proactivos son aquellos en los que el sistema de análisis (es decir, el motor de aprendizaje/predicción en flujo) controla cómo y cuándo se obtiene o recibe el flujo de datos. Por ejemplo, se puede controlar el momento y la manera en que se leen los datos de un archivo; el motor de análisis podría decidir a qué velocidad leer los datos, en qué orden, etc.\n",
    "\n",
    "\n",
    "\n",
    "## Procesamiento *online*\n",
    "\n",
    "El concepto de “en línea” (online) se refiere al procesamiento de un flujo de datos por observación. Debido a ciertas confusiones con el término “educación en línea”, ahora suele utilizarse el término “incremental”. Sin embargo, en el contexto del aprendizaje automático, se refiere al entrenamiento de un modelo mediante la incorporación de una muestra a la vez y la actualización de los pesos del modelo de manera continua.\n",
    "\n",
    "De este modo, el aprendizaje en flujo es significativamente diferente de los enfoques tradicionales de aprendizaje automático, que procesan datos en lotes. Como se mencionó antes, al manejar flujos de datos, los métodos tradicionales deben ajustar constantemente el modelo tras procesar nuevos datos para compensar la posible obsolescencia del comportamiento previo.\n",
    "\n",
    "Este cambio de metodología implica nuevos requisitos computacionales y técnicos. Por ejemplo, dado que en el procesamiento en línea las muestras de datos se manejan una a la vez, bibliotecas como NumPy y PyTorch presentan un costo computacional adicional, ya que han sido optimizadas para procesar datos en lotes mediante vectorización. En el enfoque en línea, las muestras de datos pasadas no se procesan nuevamente, por lo que dicha vectorización no es necesaria.\n",
    "\n",
    "Por lo tanto, el modelo de aprendizaje en línea es un proceso dinámico y con estado. Representa un nuevo paradigma en el aprendizaje automático y, como cualquier método técnico, tiene ventajas y desventajas cuando se aplica a problemas del mundo real.\n",
    "\n",
    "\n",
    "##  Conjuntos de datos en el entrenamiento\n",
    "\n",
    "En la mayoría de las aplicaciones del mundo real en producción, aproximadamente el 90% de los flujos de datos son reactivos. Sin embargo, para construir, entrenar y evaluar el rendimiento de un modelo de aprendizaje automático, se crea un conjunto de datos que simula el comportamiento del flujo de datos. Como veremos, la aplicación river maneja esto de manera elegante al convertir todo el conjunto de datos en un objeto generador de Python. Una vez hecho esto, el flujo de datos en tiempo real puede reemplazar sin inconvenientes la versión de desarrollo basada en un archivo monolítico.\n",
    "\n",
    "Además, hay una diferencia fundamental en la forma en que el aprendizaje automático tradicional y el aprendizaje en flujo entrenan modelos con los datos. En el aprendizaje tradicional, un conjunto de datos se divide en entrenamiento y evaluación. En el aprendizaje en flujo, el enfoque en línea utiliza todo el conjunto de datos en ambas etapas.\n",
    "\n",
    "Así, cuando llega una nueva observación al flujo de datos, el enfoque en línea la usa para evaluar el modelo y luego ajusta los pesos del modelo en consecuencia. Este proceso se repite continuamente para todo el flujo, por lo que el orden temporal de las muestras es crucial. En contraste, en el aprendizaje tradicional, los datos suelen dividirse y mezclarse aleatoriamente para reducir correlaciones estadísticas. El orden temporal garantiza que el escenario simulado sea el mismo que el encontrado en la realidad. También es clave para descubrir la estructura causal subyacente de la información, lo que podría requerir ajustes para realizar predicciones precisas en el futuro.\n",
    "\n",
    "\n",
    "\n",
    "### Concept Drift\n",
    "\n",
    "En el contexto de los flujos de datos, la razón principal por la que el aprendizaje automático tradicional falla (o al menos no es tan adecuado y requiere procesos complicados de reentrenamiento) es el fenómeno conocido como *concept drift*. Este término se refiere a la situación en la que los datos cambian fundamentalmente con el tiempo, haciendo que los modelos previamente entrenados queden obsoletos. Es un problema desafiante porque la estructura causal a largo plazo debe capturarse en el modelo, mientras que el comportamiento a corto plazo debe interpretarse rápidamente dentro de estos contextos cambiantes.\n",
    "\n",
    "Si bien la deriva de concepto también es un reto para los enfoques en línea, la ventaja de estos es que los ajustes en los pesos del modelo se realizan de manera continua sin necesidad de métodos ad-hoc. En este sentido, los métodos en línea son inherentemente coherentes y no requieren reentrenamiento del modelo base.\n",
    "\n",
    "No obstante, este sigue siendo un campo de investigación activo, que exploraremos con mayor profundidad más adelante en esta sección del curso.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f324f102",
   "metadata": {},
   "source": [
    "# Librerías para Stream Machine Learning \n",
    "\n",
    "Dado que el ML online es un paradigma relativamente nuevo, todavía existen pocas implementaciones. Como se mencionó en la introducción, las implementaciones en producción suelen ser ad-hoc y específicas para un problema o entidad en particular. Sin embargo, recientemente han surgido algunas bibliotecas que contienen muchas de las características necesarias para un sistema de procesamiento de flujo a nivel de producción.\n",
    "\n",
    "A continuación, se presenta un resumen de algunas de las principales bibliotecas:\n",
    "\n",
    "* **[Apache SAMOA](https://incubator.apache.org/projects/samoa.html)**, un proyecto para realizar análisis y minería de datos en flujos de datos. Cuenta con módulos específicos para aprendizaje automático. No obstante, el proyecto no se ha actualizado desde 2020 y sigue en la fase de incubación de la Fundación Apache (AP). ~~Además, se rumorea que la AP tiene planes de abandonar su desarrollo.~~ Finalmente, el proyecto SAMOA fue retirado.\n",
    "\n",
    "* **[MOA](https://moa.cms.waikato.ac.nz/)**, desarrollado por los mismos autores del proyecto WEKA, escrito en Java y con una estrecha relación con dicho proyecto. Incluye una colección de algoritmos de aprendizaje automático (clasificación, regresión, clustering, detección de valores atípicos, detección de deriva de concepto y sistemas de recomendación) y herramientas para evaluación. Sin embargo, su uso está principalmente limitado a la interfaz que proporciona o a la implementación de extensiones para integrarlo con el resto del ecosistema.\n",
    "\n",
    "* **[Vowpal wabbit](https://vowpalwabbit.org/)**, una librería de Python patrocinada por Microsoft. También ofrece herramientas enfocadas en el aprendizaje en flujo, pero con un énfasis particular en problemas de aprendizaje por refuerzo. Una desventaja significativa es que requiere un formato de entrada de datos específico, lo que limita en gran medida su usabilidad.\n",
    "\n",
    "* **[River](https://riverml.xyz/)**, una librería de Python enfocada en el aprendizaje en flujo, que ha adoptado un enfoque más general en comparación con Vowpal Wabbit. En esta biblioteca, el número de modelos es similar a los de scikit-learn, y además ofrece herramientas para interactuar con el ecosistema de scikit-learn. También permite desarrollar enfoques de aprendizaje por refuerzo. Como principal ventaja, puede trabajar con los tipos de datos más comunes, como Pandas DataFrames.\n",
    "\n",
    "En esta clase, utilizaremos la biblioteca River.\n",
    "\n",
    "Mediante el uso de River, describiremos varios casos de uso y abordaremos muchos de los problemas comunes en el aprendizaje en flujo o incremental.\n",
    "\n",
    "El resto de este cuaderno explorará algunos ejemplos comunes utilizando River.\n",
    "\n",
    "- El repositorio de código de River está disponible en el siguiente enlace: [River Repository](https://github.com/online-ml/river/tree/main/river)\n",
    "- La referencia de la API de River está disponible en el siguiente enlace: [River API](https://riverml.xyz/latest/api/overview/)\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "205a5777",
   "metadata": {},
   "source": [
    "## Clasificación binaria\n",
    "\n",
    "Este es quizás la aproximación más elemental del aprendizaje automático, y proporciona un buen punto de partida para discutir el aprendizaje *online*. En este caso, el modelo tiene una única salida que describe a cuál de las dos clases pertenece la muestra.\n",
    "\n",
    "1. Primer paso:  Instalar la librería (si no está instalada, se llama al comando pip).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It would required to use a version of Python >3.8\n",
    "try:\n",
    "    import river\n",
    "except ImportError as err:\n",
    "    !pip install river\n",
    "\n",
    "    \n",
    "# this library is only to improve the redability of some structures\n",
    "# https://rich.readthedocs.io/en/stable/introduction.html\n",
    "try:\n",
    "    from rich import print\n",
    "except ImportError as err:\n",
    "    !pip install rich\n",
    "    from rich import print"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4288d5f7",
   "metadata": {},
   "source": [
    "2. Importar el conjunto de datos. para este ejemplo, hemos seleccionado un conjunto de datos para desarrollar un modelo enfocado a detectar fraudes bancarios con tarjetas de crédito.  Este [dataset](https://riverml.xyz/latest/api/datasets/CreditCard/) es parte de los datasets de ejemplo de River y es fácil de cargarlo.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from river import datasets\n",
    "\n",
    "dataset = datasets.CreditCard()\n",
    "print(f\"The object contains the information of the dataset, such as, number of samples and features\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look on the first example\n",
    "sample, target = next(iter(dataset)) #An interator is created from the dataset and the first item is obtained\n",
    "print(sample)\n",
    "print(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e8d48a",
   "metadata": {},
   "source": [
    "Trabajar con clases desbalanceadas es una situación bastante común en el aprendizaje en línea para tareas como la detección de fraude y la clasificación de spam.\n",
    "\n",
    "Como se observa aquí, el conjunto de datos CreditCard está claramente desbalanceado y, por lo tanto, nos proporciona información sobre sus clases en la descripción.\n",
    "\n",
    "Sin embargo, podemos calcular fácilmente el porcentaje de representación de los datos dentro de cada clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections #python library\n",
    "\n",
    "counts = collections.Counter(target for _, target in dataset)#it generates a dictionary with labels and counts\n",
    "\n",
    "for label, count in counts.items():\n",
    "    print(f'{label}: {count} ({count / sum(counts.values()):.2%})')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dd4bf02",
   "metadata": {},
   "source": [
    "En este ejemplo, nos estamos centrando en el método de flujo y no abordamos el problema del desbalance de clases. No obstante, existen varios enfoques para manejar este tipo de problemas y mejorar el rendimiento del modelo de aprendizaje automático."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31abb124",
   "metadata": {},
   "source": [
    "3. Desarrollar un modelo para clasificación binaria. Emplearemos un modelo simple ([logisticRegression](https://riverml.xyz/latest/api/linear-model/LogisticRegression/)) a modo de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import linear_model\n",
    "\n",
    "model = linear_model.LogisticRegression()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "036c1fe5",
   "metadata": {},
   "source": [
    "Sin entrenar adecuadamente el modelo, el resultado de las probabilidades para cada clase es exactamente el mismo, como se puede ver en la llamada a la función `predict_proba_one`. Veamos qué respuesta obtenemos con la observación anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0fb760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_proba_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66530209",
   "metadata": {},
   "source": [
    "Para cada clase, tenemos un clasificador aleatorio sin ningún conocimiento. Aquí es donde el método difiere del aprendizaje automático tradicional. La misma muestra que se usó para probar, también se usará para ajustar el modelo. Debéis tener en cuenta que las métricas de rendimiento deben calcularse antes de ajustar el modelo.\n",
    "\n",
    "4. Entrenar el modelo con la observación actual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn_one(sample, target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "015c33ac",
   "metadata": {},
   "source": [
    "Si probamos nuevamente la misma observación, veremos que las probabilidades han cambiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153346d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_proba_one(sample))\n",
    "\n",
    "#We are using the same example for learning and, afterward, for prediction again with it. \n",
    "#However, this is neither correct nor fair; it is solely done for academic purposes.\n",
    "#The correct sequence for each sample should be: prediction - evaluation - train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d70980bd",
   "metadata": {},
   "source": [
    "Para obtener la etiqueta de salida, ejecutadla siguiente función: <code>predict_one()</code>, que devuelve la etiqueta sin las probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82663a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a1ceef8",
   "metadata": {},
   "source": [
    "\n",
    "Para integrar los pasos en un solo bucle y ver un proceso completo, el siguiente fragmento de código muestra cómo usar un bucle y cómo integrar una medida móvil para este tipo de sistema. Varias otras métricas también están disponibles en River. En este caso, usamos la métrica estándar que consiste en el área bajo la curva [ROC](https://riverml.xyz/latest/api/metrics/ROCAUC/). No obstante, podríamos haber seleccionado cualquier otra métrica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "for sample, target in dataset:\n",
    "    prediction = model.predict_proba_one(sample)\n",
    "    metric.update(target, prediction)\n",
    "    model.learn_one(sample, target)\n",
    "   \n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5910867",
   "metadata": {},
   "source": [
    "A common and simple approach to improve the model performance is to scale the data. There are different preprocessing operations available in River including methods for scaling data. One approach is the data standardization using the [preprocessing.StandardScaler](https://riverml.xyz/latest/api/preprocessing/StandardScaler/). \n",
    "\n",
    "The integration with  `scikit-learn` is a powerful feature of River.   Not only can models be wrapped to behave in a similar way to `scikit-learn`, but the pipelines object (which we will discuss later)  provides facilities to link different processes. For example, here is a pipeline with two operators: StandardScaler and LogisticRegression. \n",
    "\n",
    "Also, in this example, we didn’t write an explicit loop because the built-in function , `evaluate. progressive_val_score`,  performs this internally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d9503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import evaluate, compose, preprocessing\n",
    "\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LogisticRegression()\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "evaluate.progressive_val_score(dataset, model, metric)\n",
    "\n",
    "#progressive_val_score is equivalent to:\n",
    "#for sample, target in dataset:\n",
    "#    prediction = model.predict_proba_one(sample)\n",
    "#    metric.update(target, prediction)\n",
    "#    model.learn_one(sample, target)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b26c3662",
   "metadata": {},
   "source": [
    "### Play with River\n",
    "\n",
    "The selected metric used to evaluate a model that work with imbalanced classes is critical. **You can try to evaluate the model using the Accuracy metric** (it is part of the River API). You will get impressive metrics even without scaling the data!!! A simple model that always predict \"non-fraud\" would get a high accuracy because \"non-fraud\" is the majority class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline using the Accuracy metric\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60ced8b7",
   "metadata": {},
   "source": [
    "The Cohen's Kappa coefficient is a useful metric to evaluate models with imbalanced classes. This coefficient measures the agreement between the desired label and the label given by the model output excluding the probability of agreement by chance.  This metric is commonly considered more robust than the accuracy and its value is usually lower.\n",
    "The Cohen's Kappa metric is also available in River. \n",
    "\n",
    "**Try to evaluate the model using this metric. Check how the metric changes using the standardization.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478377a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline using the Cohen's Kappa metric\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d4777bb",
   "metadata": {},
   "source": [
    "What about other models? **Try to repeat the pipeline with another one**. For instance you can check the [Perceptron](https://riverml.xyz/latest/api/linear-model/Perceptron/) linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb46f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline using another model \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efcae40f",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "\n",
    "Using stream learning to perform multiclass classification is the next step of complexity we consider.  In this case,  the data sample could belong to one of many other unique classes (with their associated label).   \n",
    "\n",
    "For this case, the steps for implementing stream learning are similar to those employed in  binary classification,  with the difference of modifying the loss functions to take into account the multiple outputs. \n",
    "\n",
    "In the following example, we use another River dataset that consists of a set of images that represent 7 possible classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from river import datasets\n",
    "\n",
    "dataset = datasets.ImageSegments()\n",
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6376c72e",
   "metadata": {},
   "source": [
    "As in binary classification, the dataset consists of the samples associated with a particular target in a tuple-like structure. One difference with our analysis of the binary classifier problem is our choice of classifier. In this case, we employ a new classification method, called the [Hoeffding tree](https://riverml.xyz/latest/api/tree/HoeffdingTreeClassifier/). \n",
    "\n",
    "\n",
    "In the example below, the classifier is loaded with “tree” module.  The specific classifier is instantiated into the object, model.  Next, we print  the class probabilities for a specific sample (<code>predict_proba_one</code>), however, it produces an empty dictionary.   The reason is that the model has not already seen any sample. Therefore, it has no information about the \"possible\" classes. If this were a binary classifier, it would output a probability of 50% for True and False because the classes would be implicit. However,  in this case, we're doing multiclass classification and the output is null.\n",
    "\n",
    "\n",
    "Therefore, the <code>predict_one</code> method initially returns None because the model hasn't seen any labelled data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107832ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import tree\n",
    "\n",
    "data_stream = iter(dataset)\n",
    "sample, target = next(data_stream)\n",
    "\n",
    "model = tree.HoeffdingTreeClassifier()\n",
    "print(model.predict_proba_one(sample))\n",
    "print(model.predict_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb45f2c4",
   "metadata": {},
   "source": [
    "However, after the model learns from examples, it adds those classes to the probabilities of the model. For example, learning the first sample will associate 100% of probability that  the sample belongs to a class.  At this point, no other options are possible, since only one sample was observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6931677",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn_one(sample, target)\n",
    "print(model.predict_proba_one(sample))\n",
    "print(model.predict_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1aa28b04",
   "metadata": {},
   "source": [
    "If a second sample is used to train, we can see how the probabilies change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e91e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target = next(data_stream) # Next sample on the list\n",
    "\n",
    "model.learn_one(sample, target)\n",
    "print(model.predict_proba_one(sample))\n",
    "print(model.predict_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6a3517c",
   "metadata": {},
   "source": [
    "This is one of the key points of online classifiers:  the models can deal with new classes which appear in the data stream.\n",
    "\n",
    "Typically, the data is used once to make a prediction. When the prediction is made, the ground-truth will emerge later and it can be used first to train the model and also to evaluate. This schema is usually called **progressive validation**. Once the model is evaluated, the same observation is used to adjust the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd262130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "\n",
    "model = tree.HoeffdingTreeClassifier()\n",
    "\n",
    "metric = metrics.ClassificationReport()\n",
    "\n",
    "for sample, target in dataset:\n",
    "    prediction = model.predict_one(sample)\n",
    "    if prediction is not None:# The first iteration, the prediction is None\n",
    "        metric.update(target, prediction)\n",
    "    model.learn_one(sample, target)\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48b1f532",
   "metadata": {},
   "source": [
    "In this case, [ClassificationReport](https://riverml.xyz/latest/api/metrics/ClassificationReport/) retrieves the precision, recall, and F1 for each class that the model has seen. Additionally, the Support column indicates the number of instances identified in the stream. Finally, the function calculates and prints the three different aggregated measures together with the general accuracy of the system. \n",
    "\n",
    "This example demonstrates  a typical pipeline in stream learning. It is so frequent that  River has encapsulated the whole process in a single instance, as in the binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14dae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import evaluate\n",
    "\n",
    "model = tree.HoeffdingTreeClassifier()\n",
    "metric = metrics.ClassificationReport()\n",
    "\n",
    "print(evaluate.progressive_val_score(dataset, model, metric))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15c9264c",
   "metadata": {},
   "source": [
    "### Play with River\n",
    "\n",
    "River provides neighbor-based models for multiclass classification. Check the available models and try the appropriate one. Configure the evaluation process to print the metrics every 1,000 observations\n",
    "\n",
    "\n",
    "This is a heavy time consuming process because of the neighbor-based model. You can stop it once you check the first metric results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6af401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try a neighbor-based model for multiclass classification\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfdd8ee2",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "As a final example that is typical of ML problems,  we study regression.  For these types of problems, the ML model must predict a numerical output given a particular sample that represents the evolution of a time-series.  \n",
    "\n",
    "A  regression sample consists of several features and a target,  which is usually encoded as a continuous number (although it may also be discrete).   A useful example, included in the River library,  is the Trump approval rating dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff884e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "\n",
    "dataset = datasets.TrumpApproval()\n",
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de8d757d",
   "metadata": {},
   "source": [
    "As seen above,  each sample has 6 features that are used to make a prediction in $[0,1]$. For this problem,  we shall use a regression model.  In particular, we use  an adapted [KNN](https://riverml.xyz/0.14.0/api/neighbors/KNNRegressor/) that is included in the River  library.\n",
    "\n",
    "Note that the regression models do not have the <code>predict_proba_one()</code> method,  since it does not calculate class probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import neighbors\n",
    "\n",
    "data_stream = iter(dataset)\n",
    "sample, target = next(data_stream)\n",
    "\n",
    "model = neighbors.KNNRegressor()\n",
    "print(model.predict_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4a3c6d2",
   "metadata": {},
   "source": [
    "As it can be seen, the model has not been trained already and, therefore, the default output is $0.0$. Now, we are going to train the model and repeat the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn_one(sample, target)\n",
    "print(model.predict_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f355c53",
   "metadata": {},
   "source": [
    "Utilizing **progressive validation**,  as in the previous cases, we can employ the same sequence of operations:   prediction, evaluation,  and train that we have used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "\n",
    "model = neighbors.KNNRegressor()\n",
    "\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for sample, target in dataset:\n",
    "    prediction = model.predict_one(sample)\n",
    "    metric.update(target, prediction)\n",
    "    model.learn_one(sample, target)\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "339df89d",
   "metadata": {},
   "source": [
    "Or, in the compact notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import evaluate\n",
    "\n",
    "model = neighbors.KNNRegressor()\n",
    "metric = metrics.MAE()\n",
    "\n",
    "evaluate.progressive_val_score(dataset, model, metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfd21f7f",
   "metadata": {},
   "source": [
    "### Play with River\n",
    "\n",
    "It's important to highlight that models relying on distance metrics are highly sensitive to variations in feature scales. Establish a preprocessing pipeline for the datasets to standardize the data, and then reassess the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bebaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try a pipeline with a standardization preprocessing step and a KNN model\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
