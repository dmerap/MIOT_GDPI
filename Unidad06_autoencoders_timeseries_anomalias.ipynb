{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c7ec5e3-3036-4430-90bf-03326b724a66",
   "metadata": {},
   "source": [
    "![MIoT_GDPI](img/MIOT_GDPI_header.png)\n",
    "\n",
    "# Unidad 06 - Autoencoders para la detección de anomalías en series temporales\n",
    "\n",
    "En el desarrollo de modelos de aprendizaje automático para líneas de producción industriales, la detección de anomalías es clave y tiene un impacto transversal en las diferentes etapas de la vida de los modelos. Implementar sistemas robustos de detección de anomalías nos permitirá, en primer lugar, filtrar el conjunto de datos de entrenamiento, eliminando observaciones erróneas que podrían sesgar el aprendizaje del modelo y mermar su capacidad de generalización. En segundo lugar, una vez el modelo está desplegado en producción, la detección de anomalías actúa como un vigilante, filtrando los datos de entrada en tiempo real para identificar comportamientos inesperados o fallos en la maquinaria, previniendo así predicciones erróneas del modelo (o con mucha incertidumbre) y posibles fallos posteriores en el sistema de control, debido a las malas predicciones,  que podrían llevar a paradas de producción o productos defectuosos. Finalmente, en escenarios donde se emplean modelos de aprendizaje automático *online*, que se reentrenan continuamente con nuevos datos, la detección de anomalías es crucial para filtrar el flujo de datos de entrenamiento en tiempo real, garantizando que el modelo aprenda de información válida y relevante, manteniendo así su precisión y adaptabilidad a las condiciones cambiantes de la planta.\n",
    "\n",
    "La detección de anomalías es compleja en cualquier campo pero destaca especialmente en la Industria por el gran número de variables y casuísticas posibles.  En esta práctica veremos como los  **autoencoders** pueden aprender el comportamiento \"normal\" de las plantas industriales analizando y entrenando con datos históricos y así \"marcar\" las observaciones \"diferentes\" en tiempo de producción.\n",
    "\n",
    "\n",
    "El contenido de este Notebook está dedicado a explicar un ejemplo de detección de anomalías en series temporales empleando *autoencoders*.\n",
    "Os invitamos a experimentar modificando y variando el código proporcionado para que podáis explorar las distintas opciones y profundizar en cada uno de los conceptos mostrados.\n",
    "\n",
    "**Importante**: Este Notebook NO contiene ejercicios para entregar. \n",
    "\n",
    "\n",
    "## Referencias útiles para la práctica\n",
    "1. A. Bosch Rué, J. Casas-Roma, T. Lozano Bagén (2019): [Deep learning : principios y fundamentos](https://elibro-net.ezbusc.usc.gal/es/ereader/busc/126167/).\n",
    "2. Ejemplo Keras: [Timeseries anomaly detection using an Autoencoder](https://keras.io/examples/timeseries/timeseries_anomaly_detection/)\n",
    "\n",
    "\n",
    "## Introducción a los autoencoders\n",
    "Los *autoencoders* son un tipo de ANN cuyo objetivo es reproducir los datos de entrada de la red en su salida.  Están formados por dos partes principales: un **codificador** que mapea unas entradas a una representación interna, llamada **código latente** (que se encuentra en el **espacio latente**), y un  **decodificador** que reconstruye las entradas desde el código latente. \n",
    "\n",
    "\n",
    "\n",
    "Aunque en principio podríamos pensar que el proceso de reconstruir las entradas de una red no tiene utilidad, la realidad es que esta arquitectura nos permite entrenar una ANN para que aprenda las características más importantes y los patrones inherentes de los datos de entrada y esto lo hace útil para realizar tareas muy interesantes como:\n",
    "\n",
    "* Reducción de dimensionalidad.\n",
    "* Eliminación del ruido de los datos.\n",
    "* **Detección de anomalías**.\n",
    "* Generación de nuevos datos sintéticos\n",
    "* etc.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/Autoencoder_schema.png\" width=\"300\"/>\n",
    "    <p style=\"text-align:center; font-style:italic;\">Esquema simple de un autoencoder. Fuente Wikipedia. By Michela Massi - Own work, CC BY-SA 4.0</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Los *autoencoders* son ANN del tipo *feedforward* y *fully-connected* con varias características que las diferencian respecto a las ANN \"normales\":\n",
    "* Los *autoencoders* tienen el mismo número de neuronas de entrada que de salida, ya que su objetivo es reconstruir las *inputs*.\n",
    "* Las capas ocultas deben tener un número menor de neuronas que las capas de entrada y salida, ya que si no la tarea de reconstruir los datos sería trivial. La representación interna debe preservar la información de entrada en un formato de **menor dimensionalidad**.\n",
    "\n",
    "La mayoría de los conceptos típicos asociados a las redes del tipo *feedforward* y *fully-connected*  (inicialización, entrenamiento, regularización, etc.), son de aplicación en los *autoencoders*.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/autoencoder_architecture.png\" width=\"500\"/>\n",
    "    <p style=\"text-align:center; font-style:italic;\">Arquitectura de un autoencoder. Fuente: https://www.assemblyai.com/blog/introduction-to-variational-autoencoders-using-keras</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7beacb8-a059-406d-b9de-9748f9f082ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import generales\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as err:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as err:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError as err:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "# Asegurarnos de usar Keras 3 con backend TensorFlow\n",
    "# Es necesario hacerlo antes de cargar Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Importaciones de Keras y TensorFlow\n",
    "try:\n",
    "    import keras\n",
    "except ImportError as err:\n",
    "    !pip install keras\n",
    "    import keras\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf \n",
    "except ImportError as err:\n",
    "    !pip install tensorflow\n",
    "    import tensorflow as tf \n",
    "\n",
    "\n",
    "#simplemente por comodidad a la hora de crear los modelos\n",
    "from keras.layers import GRU, TimeDistributed,RepeatVector,Input, Dense, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26cec66-fa9b-46b3-8280-3bd251dd928b",
   "metadata": {},
   "source": [
    "## Dataset sintético\n",
    "\n",
    "Con el objetivo de probar un detector de anomalías basado en un *autoencoder*, hemos preparado un dataset sintético que simula, de forma muy básica, un sistema de secado de fibra en una línea de producción de tableros MDF.\n",
    "\n",
    "En este proceso de secado, la fibra pasa por una cinta transportadora. En un punto de esta cinta se mide con un sensor NIR la humedad de entrada al proceso de la fibra y, en un punto posterior, se mide la temperatura de los gases empleados  para secarla (los valores de temperatura están en un rango entre 120° y 200°). En la salida del proceso de secado, existe otro  sensor NIR que mide la humedad resultante de la fibra. \n",
    "\n",
    "En este ejemplo tan sencillo, el valor resultante de humedad tiene una  relación directa con la humedad de entrada y la temperatura de los gases pero es importante tener en cuenta que la capacidad de secado de los gases no es lineal.\n",
    "\n",
    "Las observaciones de nuestra línea de producción se toman cada segundo y son \"fotos fijas\" del proceso sin alinear, es decir, en cada instante $t$ se miden los valores de todos los sensores desplegados en la línea (3 en nuestro caso). Esta es la forma típica de trabajar en una planta y tiene como consecuencia que los valores de humedad de salida en el instante $t$ tienen relación con los valores de temperatura de los gases en el instante $t-n$ y con los valores de humedad de entrada en el instante $t-m$, siendo $m<n$. Debido a esta casuística, necesitamos **trabajar con secuencias de datos** para \"recordar\" el pasado. Es importante mencionar que los *autoencoders* pueden emplearse también con redes recurrentes (RNNs).\n",
    "\n",
    "\n",
    "\n",
    "En este tipo de procesos, es habitual que alguno de los sensores (ej. sensor de humedad NIR para la humedad de entrada) se deterioren y generen datos anómalos. Las anomalías más típicas en un sensor de este tipo son:\n",
    "1. El sensor queda bloqueado en alguna medida de humedad.\n",
    "2. El sensor mide con un *bias* incrementando o decrementando el valor de humedad, respecto a la realidad.\n",
    "\n",
    "En nuestro dataset hemos generado datos \"normales\" presentes en el fichero *train\\_without\\_anomaly.csv* y datos de test (*test\\_without\\_anomaly.csv*) a los que hemos introducido, en el sensor que mide la humedad de entrada, anomalías de los dos tipos previamente descritos (*test\\_with\\_anomaly.csv*).\n",
    "Es importante destacar que, aunque el sensor tenga medidas anómalas, el proceso funciona correctamente, es decir, la humedad de salida tendrá una relación directa con la humedad de entrada real y la temperatura de los gases. El \"problema\" es que desconocemos la humedad de entrada real, ya que el sensor lo mide incorrectamente.\n",
    "\n",
    "\n",
    "## Objetivo\n",
    "Nuestro objetivo es analizar las secuencias de datos formadas por los 3 sensores (humedad de entrada, temperatura de gases y humedad de salida) y destacar las secuencias anómalas, es decir, las secuencias que no tienen el comportamiento esperado, ya que sus 3 medidas no están \"armonizadas\".\n",
    "\n",
    "**Nota**: los problemas asociados a los *autoencoders* se consideran no supervisados, ya que no tenemos etiquetas externas describiendo que es cada observación.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Carga de los datos desde el repositorio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a36400-c6d9-4dd4-8d3d-22d398d9d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carga de los datos\n",
    "trainset=pd.read_csv(\"./datasets/train_without_anomaly.csv\")\n",
    "testset_sin_anomalias=pd.read_csv(\"./datasets/test_without_anomaly.csv\")#solo para visualización\n",
    "testset=pd.read_csv(\"./datasets/test_with_anomaly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe557a6-bbd9-44ab-a51b-53e203570bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Vemos como son los datos cargados\n",
    "\n",
    "print(\"Datos de entrenamiento:\")\n",
    "print(trainset.head())\n",
    "\n",
    "\n",
    "##Los datos de test tienen una columna que marca los datos anómalos para poder comparar los resultados con la realidad\n",
    "## Normalmente es dificil tener un dataset bien etiquetado de anomalías, ya que, en general, cuándo aparecen\n",
    "print(\"Datos de test:\")\n",
    "print(testset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4663179c-9000-4c6f-83b8-a2aa5669d8ec",
   "metadata": {},
   "source": [
    "### Visualización de los datos de entrenamiento\n",
    "Generamos un *plot* de las primeras 500 observaciones del dataset para cada uno de los 3 sensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb8e5d-6e42-4431-ba08-4fc8c3e6a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar datos de entrenamiento (sensores y salida)\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(trainset['Sensor1_HumedadEntrada'][:500], label='Sensor 1 (Humedad Entrada)',color='green')\n",
    "plt.title('Datos de Entrenamiento')\n",
    "plt.legend()\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(trainset['Sensor2_TemperaturaGases'][:500], label='Sensor 2 (Temperatura Gases)', color='red')\n",
    "plt.legend()\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(trainset['Output_HumedadSalida'][:500], label='Output (Humedad Salida) - Normal', color='blue')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951ea6d-e891-4e0a-8c61-ae5ecf853076",
   "metadata": {},
   "source": [
    "### Visualización de los datos de Test\n",
    "\n",
    "Generamos un *plot* con los datos de test. En este ejemplo tan sencillo, tenemos información de cómo deberían ser las secuencias de datos sin anomalías y cuáles son los períodos de tiempo anómalos.\n",
    "La primera anomalía del sensor de humedad entrante, representa un sensor bloqueado. La segunda anomalía representa un *bias* en las medidas generadas por el sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235cdc5-4cf2-440d-b244-6a8e0be54a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar datos de test (con anomalías en sensores vs salida normal)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(testset_sin_anomalias['Sensor1_HumedadEntrada'], label='Humedad entrada ', linestyle='--', alpha=0.7)\n",
    "plt.plot(testset['Sensor1_HumedadEntrada'], label='Humedad entrada (Con Anomalías)', color='green')\n",
    "plt.title('Datos de Test (Con Anomalías en Sensores)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(testset_sin_anomalias['Sensor2_TemperaturaGases'], label='TemperaturaGases (Base)', linestyle='--', alpha=0.7)\n",
    "plt.plot(testset['Sensor2_TemperaturaGases'], label='TemperaturaGases (Con Anomalías)', color='red')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(testset['Output_HumedadSalida'], label='Output (Humedad Salida) - Se mantiene normal', color='blue')\n",
    "# Marcar zonas de anomalía para referencia visual\n",
    "anomaly_zones = testset[testset['Es_Anomalia']].index\n",
    "for idx in anomaly_zones:\n",
    "    plt.axvspan(idx - 0.5, idx + 0.5, color='yellow', alpha=0.3, lw=0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0e80f-3373-494f-8b96-850758a493f0",
   "metadata": {},
   "source": [
    "## Preprocesado de datos\n",
    "\n",
    "En este ejemplo tan sencillo solo realizaremos la estandarización de los datos y lo haremos, en este caso, de forma manual. \n",
    "\n",
    "Dado que no vamos a optimizar hiperparámetros, no emplearemos el conjunto de validación, aunque dejaremos un pequeño porcentaje durante el entrenamiento dedicado a \"parar\" el entrenamiento para evitar el sobreentrenamiento (*early stop*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4e773-8255-479b-90a9-5fabc9d6e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la media y la desviación típica.\n",
    "training_mean = trainset.mean()\n",
    "training_std = trainset.std()\n",
    "scaled_trainset=(trainset - training_mean) / training_std\n",
    "scaled_testset=(testset- training_mean) / training_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441043de-e061-4d84-8a21-a738d47296e4",
   "metadata": {},
   "source": [
    "## Preparación de las secuencias\n",
    "\n",
    "\n",
    "En nuestro caso de uso necesitamos procesar secuencias temporales, ya que la salida está influenciada por las medidas de los sensores en los pasos anteriores. Vamos a generar secuencias con solape, para que el autoencoder aprenda el comportamiento \"normal\".\n",
    "\n",
    "Las secuencias en este caso son un poco diferentes a lo que normalmente estamos acostumbrados, ya que estamos trabajando en un problema no etiquetado. Lo que buscamos es reproducir las secuencias de entrada, por lo que  las *inputs* y las *outputs* serán las mismas y las dos serán secuencias. En este caso no podemos emplear la librería de Keras que nos ayudaba a generar secuencias (`keras.utils.timeseries_dataset_from_array`), ya que no permite asociar a los *targets* secuencias. Crearemos, por lo tanto, una pequeña función que nos ayude a procesar las secuencias de la forma correcta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514b56e-0f12-4034-a58b-5469132ddf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no podemos emplear keras.utils.timeseries_dataset_from_array porque no genera secuencias para la salida (targets)\n",
    "WINDOW_SIZE=10 #tamaño de la secuencia que queremos generar. En principio el tamaño necesario de la secuencia no es conocido\n",
    "NUM_INPUTS=3 #Numero de características: sensor humedad de entrada, temperatura de gases, sensor humedad de salida\n",
    "\n",
    "def crear_secuencias(data, window_size):\n",
    "    X = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        X.append(data[i:(i + window_size)])\n",
    "    return np.array(X)\n",
    "\n",
    "# Crear secuencias para entrenamiento y test\n",
    "# El autoencoder intentará reconstruir la secuencia completa\n",
    "X_train_seq = crear_secuencias(scaled_trainset, WINDOW_SIZE)\n",
    "X_test_seq = crear_secuencias(scaled_testset[[\"Sensor1_HumedadEntrada\",\"Sensor2_TemperaturaGases\",\"Output_HumedadSalida\"]], WINDOW_SIZE)#quito la etiqueta de anomalia que es solo para visualización\n",
    "\n",
    "print(\"Forma de secuencias de entrenamiento:\", X_train_seq.shape) # (samples, window_size, n_features)\n",
    "print(\"Forma de secuencias de test:\", X_test_seq.shape)\n",
    "\n",
    "#visualizamos una secuencia\n",
    "print(X_train_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3ba17-aca1-4fff-a668-e61b8abf602f",
   "metadata": {},
   "source": [
    "## Creación del *autoencoder* para series temporales\n",
    "\n",
    "El *autoencoder* estará compuesto por una capa de entrada con el mismo número de neuronas que la capa de salida, ya que la red tiene que reconstruir las entradas. En este caso, lo que se intenta reconstruir es las secuencias de entrada con sus respectivas características. \n",
    "\n",
    "Las capas ocultas se crearán de forma simétrica, una primera parte, llamada codificador, reduce la dimensionalidad de los datos de entrada en el espacio latente (capa oculta central). Una segunda parte, llamada decodificador, procesa el espacio latente y reconstruye las secuencias de entrada.\n",
    "\n",
    "En nuestro caso, al procesar series temporales, emplearemos capas LSTM para construir el codificador y el decodificador.\n",
    "Descripción de las capas del modelo generado:\n",
    "* `Input()`: capa de Entrada al modelo. Esta capa no realiza cálculos, sino que define la forma esperada de los datos de entrada.\n",
    "    - `shape=(seq_len, num_model_features)`: Especifica que la entrada será un tensor 3D. Para las LSTMs, esto típicamente se interpreta como (batch_size, timesteps, features). Keras maneja batch_size implícitamente. Aquí, seq_len corresponde a timesteps (pasos de tiempo) y num_model_features corresponde a features (características).\n",
    "* `LSTM()`: capas LSTM apiladas para generar el autoencoder.\n",
    "    - `return_sequences=True`: Es necesario para apilar LSTMs. Significa que la capa LSTM devolverá la secuencia completa de salidas (una salida para cada paso de tiempo en la secuencia de entrada) en lugar de solo la salida del último paso de tiempo. Esto es necesario porque la siguiente capa LSTM en el codificador espera una secuencia completa como entrada.\n",
    "* `RepeatVector()`: En nuestro caso la capa central o *bottleneck* genera solo la salida del último paso de tiempo `return_sequences=False`, comprimiendo el espacio latente. Para poder reconstruir nuevamente la secuencia, es necesario convertir la salida de la capa central en una secuencia, por lo que se repite la salida anterior, tantas veces como pasos de la secuencia.\n",
    "*  `TimeDistributed(Dense(num_model_features),...)`: es un *wrapper* que permite aplicar la capa `Dense()` a cada una de las salidas de la capa LSTM. La capa *TimeDistributed* recibe como entrada secuencias de la forma `(batch_size, seq_len, units_ultima_lstm_decoder)` y aplica `Dense(num_model_features)` a cada una de las secuencias para generar nuevas de la forma `(batch_size, seq_len, num_model_features)` que es la forma de los datos de entrada que estamos reconstruyendo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42337e-ba96-43de-acff-d9108ba8babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(seq_len, num_model_features, name_model,units=[12,6,3]):\n",
    "\n",
    "    model = keras.Sequential(name=name_model)\n",
    "    model.add( keras.Input(shape=(seq_len, num_model_features), name=\"input_layer\"))\n",
    "    num_layer=0\n",
    "    #Codificador\n",
    "    for num_layer,layer_units in enumerate(units[:-1]):\n",
    "        model.add(LSTM(layer_units, activation='relu', return_sequences=True, name=f\"L_{num_layer}\"))\n",
    "\n",
    "    num_layer+=1\n",
    "    #bottleneck\n",
    "    model.add(LSTM( units[-1], activation='relu', return_sequences=False,name=f\"L_{num_layer}\"))\n",
    "    num_layer+=1\n",
    "    model.add(RepeatVector(seq_len,name=f\"L_{num_layer}\"))\n",
    "    #decodificador\n",
    "    for num_layer, layer_units in enumerate(units[::-1], start=num_layer+1):\n",
    "        model.add(LSTM(layer_units, activation='relu', return_sequences=True,name=f\"L_{num_layer}\"))\n",
    "    num_layer+=1\n",
    "    model.add(TimeDistributed(Dense(num_model_features),name=f\"L_{num_layer}\"))\n",
    "    return model\n",
    "    \n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370508d8-b8af-4221-a2a8-fae171eccbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder=get_model(WINDOW_SIZE,NUM_INPUTS, \"autoencoder_LSTM\", units=[10,5,2])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0003d40-be74-42b6-8893-feae0525e717",
   "metadata": {},
   "source": [
    "## Compilación del modelo\n",
    "Compilamos el modelo con una configuración fija. En un problema real deberíamos optimizar los hiperparámetros (ej. LR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508777d-5079-4a1f-bcfc-529812e31265",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse') # Mean Squared Error para reconstrucción\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f067a143-0b07-4b53-a37f-e3077edc6171",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n",
    "\n",
    "Activamos un *callback* para *early stop* y reservamos un 10% de datos para dicho *callback*. Fijaos como los datos de entrada al entrenamiento (*inputs*) y los datos de salida (*outputs*) son los mismos (en este caso secuencias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af6082-846c-4b01-b417-427e777d63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10, # Número de épocas sin mejora antes de detener el entrenamiento\n",
    "    restore_best_weights=True # Restaurar los pesos del modelo de la mejor época\n",
    ")\n",
    "\n",
    "history = autoencoder.fit(X_train_seq,X_train_seq,    epochs=50, batch_size=32,validation_split=0.1 ,callbacks=[callback_early_stopping]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e7e56-ba3e-4a51-a229-4a053a1fb110",
   "metadata": {},
   "source": [
    "## Visualización de las métricas de entrenamiento\n",
    "\n",
    "Visualizamos las métricas de entrenamiento y validación a lo largo de las épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b83d6-c334-4c5f-b3fd-7f1c8962071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training losses\n",
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(history.history['loss'], 'b', label='Train', linewidth=2)\n",
    "ax.plot(history.history['val_loss'], 'r', label='Validation', linewidth=2)\n",
    "ax.set_title('Model loss', fontsize=16)\n",
    "ax.set_ylabel('Loss (mae)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb008962-5cc1-418f-b23f-cd83d38ca1f5",
   "metadata": {},
   "source": [
    "## Detección de anomalías\n",
    "\n",
    "Para detectar las anomalías sobre el conjunto de Test, debemos reconstruir las entradas de Test y calcular el error  entre la predicción y los datos reales. Presuponemos que las secuencias con errores más altos tienen algo (anomalías) que las hace diferentes y que no  permite reconstruirlas correctamente.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ba2a2-806a-451c-bf5f-c74e213f5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pred_seq = autoencoder.predict(X_test_seq)\n",
    "\n",
    "# Calcula el error de reconstrucción (ej. MAE) para cada secuencia\n",
    "\n",
    "reconstruction_errors = np.mean(np.abs(X_test_seq - X_test_pred_seq), axis=(1,2)) # MAE por secuencia\n",
    "\n",
    "# Para alinear los errores con el dataframe original, necesitamos tener en cuenta el window_size\n",
    "# El error de reconstrucción para la secuencia que TERMINA en el tiempo `t` se asocia con `t`.\n",
    "df_test_analysis = testset.iloc[WINDOW_SIZE-1:].copy()\n",
    "df_test_analysis['Reconstruction_Error'] = reconstruction_errors\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df_test_analysis.index, df_test_analysis['Reconstruction_Error'], label='Error de Reconstrucción (MAE)')\n",
    "plt.title('Error de Reconstrucción en el Conjunto de Test')\n",
    "plt.xlabel('Índice de Tiempo')\n",
    "plt.ylabel('Error Medio Absoluto')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f61b8-844b-45ff-aee6-fa00b91e970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umbral simple: basado en un percentil del error de reconstrucción en los datos de test\n",
    "UMBRAL_ANOMALIA = np.percentile(reconstruction_errors, 90) # Ej: 90º percentil como umbral\n",
    "print(f\"Umbral de anomalía establecido (MAE): {UMBRAL_ANOMALIA:.4f}\")\n",
    "\n",
    "df_test_analysis['Anomalia_Detectada'] = df_test_analysis['Reconstruction_Error'] > UMBRAL_ANOMALIA\n",
    "\n",
    "# Visualizar detecciones\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(testset.index, testset['Sensor1_HumedadEntrada'], label='Sensor 1 (Real)', alpha=0.7)\n",
    "plt.scatter(df_test_analysis[df_test_analysis['Anomalia_Detectada']].index,\n",
    "            testset.loc[df_test_analysis[df_test_analysis['Anomalia_Detectada']].index, 'Sensor1_HumedadEntrada'],\n",
    "            color='red', label='Anomalía Detectada en S1', marker='x', s=50)\n",
    "plt.title('Detección de Anomalías en Sensores')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(testset.index, testset['Sensor2_TemperaturaGases'], label='Sensor 2 (Real)', alpha=0.7)\n",
    "plt.scatter(df_test_analysis[df_test_analysis['Anomalia_Detectada']].index,\n",
    "            testset.loc[df_test_analysis[df_test_analysis['Anomalia_Detectada']].index, 'Sensor2_TemperaturaGases'],\n",
    "            color='red', label='Anomalía Detectada en S2', marker='x', s=50)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(df_test_analysis.index, df_test_analysis['Reconstruction_Error'], label='Error de Reconstrucción (MAE)')\n",
    "plt.axhline(UMBRAL_ANOMALIA, color='r', linestyle='--', label='Umbral')\n",
    "# Marcar las anomalías reales para comparación visual\n",
    "true_anomalies_indices = testset[testset['Es_Anomalia']].index\n",
    "# Filtrar los que están dentro del rango de df_test_analysis\n",
    "true_anomalies_in_analysis = [idx for idx in true_anomalies_indices if idx in df_test_analysis.index]\n",
    "\n",
    "for idx in true_anomalies_in_analysis:\n",
    "    if df_test_analysis.loc[idx, 'Es_Anomalia']:\n",
    "         plt.axvspan(idx - 0.5, idx + 0.5, color='yellow', alpha=0.3, lw=0, label='Anomalía Real (una vez)' if 'Anomalía Real (una vez)' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "plt.xlabel('Índice de Tiempo')\n",
    "plt.ylabel('Error / Valor Sensor')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77585503-ef74-4770-b0ac-6c3abcfe7e4a",
   "metadata": {},
   "source": [
    "## Reconstrucción de los datos\n",
    "\n",
    "Teniendo en cuenta lo que el *autoencoder* reconstruye dadas unas observaciones anómalas, podemos tratar de analizar cómo deberían ser los datos del sensor \"estropeado\" para que las secuencias pudiesen recontruirse. \n",
    "\n",
    "\n",
    "**Nota**: *esta parte es solo informativa y deben analizarse los resultados con mucha prudencia.  Esta aproximación es simplista y puede tener sentido solo si las anomalías están en un solo sensor. No vamos a conseguir los datos correctos pero se pueden acercar más a la realidad que lo medido erroneamente por el sensor.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e34e1-4857-4ce6-91a2-4fd4d2319e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(X_test_pred_seq[:,0,0].flatten(), 'b', label='Corregido', linewidth=2)\n",
    "ax.plot(scaled_testset[\"Sensor1_HumedadEntrada\"], 'r', label='Medido', linewidth=2, alpha=0.6)\n",
    "ax.set_title('Humedad de secadero', fontsize=16)\n",
    "ax.set_ylabel('humedad')\n",
    "ax.set_xlabel('tiempo')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
