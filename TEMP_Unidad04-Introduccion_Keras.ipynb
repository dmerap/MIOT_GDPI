{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b8f294-6212-4eb5-9e97-5e5c897be44f",
   "metadata": {},
   "source": [
    "![MIoT_GDPI](img/MIOT_GDPI_header.png)\n",
    "\n",
    "# Unidad 04 - Introducción a la librería Keras\n",
    "\n",
    "Una vez finalizado la introducción al paradigma de aprendizaje incremental, nos centraremos ahora en el desarrollo y aplicación de modelos basados en redes neuronales profundas, ya que nos permitirán generar modelos que tengan en cuenta el tiempo. Para poder desarrollar  este tipo de redes, emplearemos Tensorflow y el API de Keras3. Dado que la experiencia en este máster estuvo centrada en en `scikit-learn`, emplearemos esta práctica a modo de introducción al uso de Keras con Tensorflow.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "La mayor parte del contenido de este Notebook se dedica a explicar el uso básico del API Keras empleando como *backend* Tensorflow. Es crucial que dediquéis tiempo a leer y comprender el material, en lugar de simplemente ejecutar el código. Os invitamos a experimentar modificando y variando el código proporcionado para que podáis explorar las distintas opciones y profundizar en su funcionamiento.\n",
    "\n",
    "\n",
    "\n",
    "**Importante**: El Notebook contiene varios ejercicios sencillos. Deberéis desarrollarlos durante la clase y enviarlos por el aula virtual del curso, en la tarea correspondiente\n",
    "\n",
    "\n",
    "## Referencias útiles para la práctica\n",
    "\n",
    "1. [Documentación oficial](https://www.tensorflow.org/learn?hl=es-419) de Tensorflow\n",
    "2. [Guías](https://keras.io/guides/) de Keras\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Introducción \n",
    "**Es habitual que en las plantas industriales nos encontremos con procesos secuenciales altamente no lineales y con grandes dependencias temporales**. Pensad que un subproceso que se está aplicando ahora mismo en una parte de la línea a un producto intermedio, no tendrá efecto en el producto final hasta un tiempo después, ya que es necesario que el producto intermedio avance y  atraviese los diferentes subprocesos productivos hasta llegar al final.\n",
    "\n",
    "En general los modelos vistos hasta ahora no tienen en cuenta el tiempo. Típicamente son \"fotos fijas\" de un proceso en el que se recogen las medidas o características de ese momento dado. Por ejemplo, cada observación puede estar compuesta de las medidas de todos los sensores desplegados en la factoría en un segundo concreto.\n",
    "En un proceso secuencial con dependencias temporales, estaríamos asociando a la variable/s de salida/s, características de entrada que no fueron las que realmente afectaron  y generaron dicha salida.\n",
    "\n",
    "Típicamente. para poder trabajar en esta situación en una línea de producción tenemos 2 opciones:\n",
    "1. Podemos **alinear los datos**, es decir, tratar de crear observaciones que cuadren las entradas concretas que afectaron a la salida escogida. Esto es algo tremendamente complicado si no se conoce perfectamente el proceso productivo y sus características.\n",
    "2. Podemos **trabajar con modelos que recuerden y tengan en cuenta lo que ha pasado en instantes anteriores**.\n",
    "\n",
    "Trabajar con modelos recurrentes que tengan en cuenta el tiempo es algo que suele beneficiar el desarrollo de predictores en plantas industriales pero son más complicados de desarrollar y entrenar. Librerías típicas como `scikit-learn`no contemplan el  desarrollo de este tipo de modelos pero existen otros frameworks centrados en el desarrollo de redes neuronales profundas en general y que permiten crear redes recurrentes de diferentes tipos. [Tensorflow](https://www.tensorflow.org/?hl=es-419) es uno de los *frameworks* más populares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7de20c-fb35-425b-b0cf-232e7d70749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importaciones generales\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as err:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as err:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except ImportError as err:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError as err:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "# Asegurarnos de usar Keras 3 con backend TensorFlow\n",
    "# Es necesario hacerlo antes de cargar Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Importaciones de Keras y TensorFlow\n",
    "try:\n",
    "    import keras\n",
    "except ImportError as err:\n",
    "    !pip install keras\n",
    "    import keras\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf \n",
    "except ImportError as err:\n",
    "    !pip install tensorflow\n",
    "    import tensorflow as tf \n",
    "\n",
    "\n",
    "\n",
    "# Importaciones de Scikit-learn\n",
    "#Solo para facilitarnos el uso de algunas operaciones típicas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# Configuraciones para visualización. No es necesario\n",
    "plt.style.use('seaborn-v0_8-whitegrid') # Estilo de gráficos\n",
    "sns.set_palette('viridis') # Paleta de colores\n",
    "\n",
    "\n",
    "# Verificación de versiones\n",
    "#Existen diferentes versiones y compatibilidades. Es importantes saber lo que estamos ejecutando\n",
    "#Para estos ejemplos queremos ejecutar el API de Keras3 (standalone) y tensorflo3>2.16\n",
    "print(f\"Versión de Keras: {keras.__version__}\")\n",
    "print(f\"Backend de Keras: {keras.backend.backend()}\")\n",
    "print(f\"Versión de TensorFlow: {tf.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "# Verificar si TensorFlow puede usar GPU en nuestro sistema (opcional, pero bueno saberlo)\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"GPU disponible: {gpu_devices}\")\n",
    "else:\n",
    "    print(\"GPU no encontrada, se usará CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d3959-43bb-482b-bd25-cca6ef8940dd",
   "metadata": {},
   "source": [
    "Para el desarrollo del ejemplo emplearemos un *dataset* público de Kaggle: [Concrete Compressive Strength Data Set](https://www.kaggle.com/datasets/elikplim/concrete-compressive-strength-data-set/data).\n",
    "El repositorio contiene una descripción del mismo y de sus campos:\n",
    "\n",
    "\n",
    "**Concrete Compressive Strength Data Set**\n",
    "\n",
    "\n",
    "*Abstract: Concrete is the most important material in civil engineering. The\n",
    "concrete compressive strength is a highly nonlinear function of age and\n",
    "ingredients. These ingredients include cement, blast furnace slag, fly ash,\n",
    "water, superplasticizer, coarse aggregate, and fine aggregate.*\n",
    "\n",
    "| **Nombre** | **Tipo de dato** | **Medida** | **Descripción** |\n",
    "|---|---|---|---|\n",
    "| Cement (component 1) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Blast Furnace Slag (component 2) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Fly Ash (component 3) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Water (component 4) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Superplasticizer (component 5) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Coarse Aggregate (component 6) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Fine Aggregate (component 7) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Age | quantitative | Day (1\\~365) | Input Variable |\n",
    "| **Concrete compressive strength** | quantitative | MPa | Output Variable |\n",
    "\n",
    "\n",
    "\n",
    "Descargamos la última versión del dataset empleando `kagglehub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e850ba4-cf85-4370-b308-e73c19ee09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"elikplim/concrete-compressive-strength-data-set\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "#Nombre del fichero descargado: concrete_data.csv\n",
    "df_concrete=pd.read_csv(path+\"/concrete_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a339a-f9b9-4464-9215-4bd7ad1c668a",
   "metadata": {},
   "source": [
    "Visualizamos los datos cargados. \n",
    "\n",
    "**IMPORTANTE**: *el objetivo  de este Notebook es desarrollar modelos con Keras y Tensorflow, por lo que simplificaremos al máximo las fases de análisis y preprocesado. El rendimiento del modelo no es prioritario. Conocéis de materias previas la importancia de analizar y preparar los datos  para mejorar la precisión de los modelos.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b69820-bab2-4d2e-9cd8-25594a913c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numero de filas: {len(df_concrete)}\")\n",
    "print(f\"Numero de características: {len(df_concrete.columns)}\")\n",
    "\n",
    "print(\"Información general del dataset:\")\n",
    "df_concrete.info()\n",
    "\n",
    "\n",
    " # Vistazo rápido a los datos\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(df_concrete.head())\n",
    "\n",
    "\n",
    "print(\"Estadísticas descriptivas:\")\n",
    "print(df_concrete.describe().transpose())\n",
    "\n",
    "# Comprobar valores nulos \n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df_concrete.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b01ef-5c0d-4882-abf0-947365e107db",
   "metadata": {},
   "source": [
    "Preparamos los datos para poder generar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae639ba2-93a6-4939-b2ab-8fe25bc93178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Separamos las entradas de las salidas\n",
    "df_concrete_inputs = df_concrete.drop('concrete_compressive_strength', axis=1)\n",
    "df_concrete_outputs = df_concrete['concrete_compressive_strength']\n",
    "\n",
    "\n",
    "# 2. Dividimos en entrenamiento, validación y test (test)\n",
    "# Usaremos un 70% para entrenamiento y  un 15% para Validación y Test.\n",
    "# random_state asegura reproducibilidad.\n",
    "SEED=42\n",
    "X_train, X_pruebas, y_train, y_pruebas = train_test_split(df_concrete_inputs, df_concrete_outputs, test_size=0.30, random_state=SEED)\n",
    "\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_pruebas, y_pruebas, test_size=0.50, random_state=SEED)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Datos de entrenamiento:\", X_train.shape, y_train.shape)\n",
    "print(\"Datos de validación:\", X_val.shape, y_val.shape)\n",
    "print(\"Datos de test:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# 3. Escalado de las características \n",
    "# Usaremos StandardScaler: Z = (x - mean) / std_dev\n",
    "# Es crucial ajustar el escalador SÓLO con los datos de entrenamiento\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val) # Usar el mismo scaler ajustado\n",
    "X_test_scaled = scaler.transform(X_test) # Usar el mismo scaler ajustado\n",
    "\n",
    "# Convertimos a DataFrames para mejor visualización (opcional)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=df_concrete_inputs.columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=df_concrete_inputs.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=df_concrete_inputs.columns)\n",
    "\n",
    "print(\"Primeras filas de X_train escalado:\")\n",
    "print(X_train_scaled.head())\n",
    "print(\"Media de X_train escalado (debería ser cercana a 0):\")\n",
    "print(X_train_scaled.mean())\n",
    "print(\"Desviación estándar de X_train escalado (debería ser cercana a 1):\")\n",
    "print(X_train_scaled.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894c639-0a9a-40f5-bf3f-bd6ad7c8bac1",
   "metadata": {},
   "source": [
    "## Construcción del modelo\n",
    "\n",
    "Keras permite general modelos empleando dos opciones:\n",
    "1. El [API secuencial](https://keras.io/guides/sequential_model/): Permite generar redes densas con diferentes capas donde cada capa tiene exactamente un tensor de entrada y un tensor de salida.\n",
    "2. El [API funcional](https://keras.io/guides/functional_api/): permite crear modelos más flexibles que los secuenciales. Esta API permite gestionar modelos con topologías no lineales. Permite crear \"grafos de capas\".\n",
    "\n",
    "\n",
    "\n",
    "Vamos a definir nuestra red neuronal usando el API Secuencial de Keras (`keras.Sequential`), que es suficiente crear un modelo que \"apile\"  capas una tras otra.\n",
    "\n",
    "\n",
    "Definiremos una red sencilla:\n",
    "- Capa de Entrada: Definida implícitamente por input_shape en la primera capa densa. Debe coincidir con el número de características (8 en nuestro caso).\n",
    "- Capas Ocultas: Usaremos un par de capas densas (`keras.layers.Dense`) con activación ReLU (`relu`). ReLU es una opción común y eficiente para capas ocultas. El número de neuronas es un hiperparámetro a optimizar (empezaremos con 64 y 32).\n",
    "- Capa de Salida: Una única neurona (Dense(1)) ya que es un problema de regresión (predecir un solo valor continuo). No usaremos función de activación (o equivalentemente, activación 'linear'), porque queremos que la salida pueda tomar cualquier valor real, no limitado a un rango específico.\n",
    "\n",
    "Explicación del código:\n",
    "- keras.Sequential([...]): Crea un modelo donde las capas se ejecutan en secuencia.\n",
    "- keras.Input(shape=(n_features,)): Define formalmente la forma de los datos de entrada. Ayuda a Keras a construir el modelo correctamente.\n",
    "- keras.layers.Dense(units, activation=...): Define una capa totalmente conectada (densa). units es el número de neuronas. activation es la función de activación.\n",
    "- model.summary(): Muestra las capas, la forma de salida de cada capa y el número de parámetros entrenables. Es útil para verificar la arquitectura.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a7445-d1f5-4c85-b582-96c741c4a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Obtenemos el número de características para la capa de entrada\n",
    "n_features = X_train_scaled.shape[1]\n",
    "print(f\"Número de características de entrada: {n_features}\")\n",
    "\n",
    "# Construcción del modelo secuencial\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(n_features,), name=\"input_layer\"), # Capa de entrada explícita. No es obligatoria pero nos permite ver el resumen completo de la red\n",
    "        keras.layers.Dense(64, activation='relu', name='hidden_layer_1'), # 1ra capa oculta con 64 neuronas y activación ReLU\n",
    "        keras.layers.Dense(32, activation='relu', name='hidden_layer_2'), # 2da capa oculta con 32 neuronas y activación ReLU\n",
    "        keras.layers.Dense(1, activation='linear', name='output_layer')   # Capa de salida con 1 neurona (regresión) y activación lineal\n",
    "    ],\n",
    "    name=\"Concrete_Strength_Predictor\" # Nombre opcional para el modelo\n",
    ")\n",
    "\n",
    "\n",
    "# Mostrar un resumen de la arquitectura del modelo\n",
    "print(\"\\nResumen del modelo:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8adfe7-9359-4736-a376-471a92789aaf",
   "metadata": {},
   "source": [
    "## Compilación del modelo\n",
    "\n",
    "\n",
    "Antes de entrenar, necesitamos \"compilar\" el modelo. Esto implica configurar:\n",
    "\n",
    "* Optimizador: Algoritmo que ajusta los pesos de la red durante el entrenamiento(ej. Adam, SGD, RMSprop). Adam es una opción robusta y popular. Tenéis [aquí](https://keras.io/api/optimizers/) un listado de los disponibles.\n",
    "\n",
    "* Función de Pérdida (*Loss Function*): Mide qué tan bien se desempeña el modelo durante el entrenamiento. Para regresión, mean_squared_error (MSE) es una opción muy típica. Tenéis [aquí](https://keras.io/api/losses/) un listado de los disponibles.\n",
    "\n",
    "\n",
    "* Métricas: Funciones para evaluar el rendimiento del modelo (ej. mean_absolute_error - MAE). **Las métricas no se usan para optimizar el modelo, solo para monitorizarlo**. Tenéis [aquí](https://keras.io/api/metrics/) un listado de las disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83ba4c-96ae-4f43-a0bf-975aab3e91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001), # Optimizador Adam con tasa de aprendizaje típica\n",
    "        loss='mean_squared_error',          # Función de pérdida para regresión (MSE)\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name='mae')] # Métrica para monitorear (MAE)\n",
    "    )\n",
    "\n",
    "print(\"Modelo compilado exitosamente.\")\n",
    "print(f\"Optimizador: {type(model.optimizer).__name__}\")\n",
    "print(f\"Función de pérdida: {model.loss}\")\n",
    "print(\"Métricas: \")\n",
    "#metricas incluye la empleada como función de pérdida\n",
    "for m in model.metrics:\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5382ae0-48da-4660-8024-68b25e859350",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n",
    "Ahora entrenamos el modelo con nuestros datos de entrenamiento escalados. \n",
    "El método `fit` nos permite hacerlo. Tenéis [aquí](https://keras.io/api/models/model_training_apis/#fit-method) disponible todos los parámetros posibles. en este caso emplearemos:\n",
    "\n",
    "* epochs: Número de veces que el modelo verá el conjunto de datos completo.\n",
    "* batch_size: Número de muestras procesadas antes de actualizar los pesos (entrenamiento con *mini-batches*).\n",
    "* validation_data: conjunto de datos sobre los que validaremos el entrenamiento del modelo. Lo podemos emplear para comparar configuraciones o para parar el entrenamiento de forma prematura (*early_stopping*), lo que puede ayudar a detectar el *overfitting*.\n",
    "\n",
    "El método `fit` devuelve un objeto *History* que contiene información sobre el proceso de entrenamiento (pérdida y métricas en cada época)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d48fb-1be1-4f8c-8722-f180332683e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definimos el número de épocas y el tamaño del batch\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1 # Muestra una barra de progreso por época (0=silencioso, 1=barra, 2=una línea por época)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a473300d-2525-47ba-88f7-7b10ad83eb49",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo\n",
    "\n",
    "Evaluaremos el rendimiento del modelo de dos maneras:\n",
    "\n",
    "1. Visualizando el historial de entrenamiento: Graficaremos la pérdida y la métrica (MAE) tanto para el conjunto de entrenamiento como para el de validación a lo largo de las épocas. Esto nos ayuda a diagnosticar problemas como el overfitting (cuando la pérdida de validación empieza a aumentar mientras la de entrenamiento sigue bajando) o el underfitting (cuando ambas pérdidas son altas).\n",
    "2. Evaluando en el conjunto de Test: Usaremos el método `evaluate` con los datos de Test (X_test_scaled, y_test), que el modelo nunca ha visto. Esto nos da una estimación final e imparcial del rendimiento del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4ad87-f428-4630-a859-a6b78f74c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist['epoch'], hist['loss'], label='Pérdida Entrenamiento')\n",
    "plt.plot(hist['epoch'], hist['val_loss'], label = 'Pérdida Validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida (MSE)')\n",
    "plt.title('Pérdida (MSE) durante Entrenamiento')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Asegúrate de usar el nombre correcto de la métrica (puede variar ligeramente)\n",
    "mae_key = 'mae' if 'mae' in hist.columns else list(hist.columns)[1] # Intenta encontrar la clave MAE\n",
    "val_mae_key = 'val_' + mae_key\n",
    "plt.plot(hist['epoch'], hist[mae_key], label='MAE Entrenamiento')\n",
    "plt.plot(hist['epoch'], hist[val_mae_key], label = 'MAE Validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Error Absoluto Medio (MAE)')\n",
    "plt.title('MAE durante Entrenamiento')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Evaluar en el conjunto de prueba\n",
    "print(\"\\nEvaluando el modelo en el conjunto de prueba...\")\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(f\"Pérdida en el conjunto de Test (MSE): {test_loss:.4f}\")\n",
    "print(f\"Error Absoluto Medio (MAE) en el conjunto de Test: {test_mae:.4f}\")\n",
    "\n",
    "# Calcular R^2 score (opcional, pero útil en regresión)\n",
    "y_pred_test = model.predict(X_test_scaled).flatten() # Aplanar para que tenga la misma forma que y_test\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(f\"Coeficiente de Determinación (R²) en el conjunto de Test: {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
