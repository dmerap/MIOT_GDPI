{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b8f294-6212-4eb5-9e97-5e5c897be44f",
   "metadata": {},
   "source": [
    "![MIoT_GDPI](img/MIOT_GDPI_header.png)\n",
    "\n",
    "# Unidad 04 - Introducción a la librería Keras\n",
    "\n",
    "Una vez finalizado la introducción al paradigma de aprendizaje incremental, nos centraremos ahora en el desarrollo y aplicación de modelos basados en redes neuronales profundas, ya que estos nos permitirán generar sistemas de predicción que tengan en cuenta el tiempo. Para poder desarrollar  este tipo de modelos, emplearemos Tensorflow y el API de Keras3. Dado que la experiencia en este máster estuvo centrada en en `scikit-learn`, emplearemos esta práctica a modo de introducción al uso de Keras con Tensorflow.\n",
    "\n",
    "\n",
    "\n",
    "La mayor parte del contenido de este Notebook se dedica a explicar el uso básico del API Keras empleando como *backend* Tensorflow. Es crucial que dediquéis tiempo a leer y comprender el material, en lugar de simplemente ejecutar el código. Os invitamos a experimentar modificando y variando el código proporcionado para que podáis explorar las distintas opciones y profundizar en su funcionamiento.\n",
    "\n",
    "\n",
    "\n",
    "**Importante**: El Notebook contiene un ejercicio sencillo que deberéis desarrollar y enviar por el aula virtual del curso, en la tarea correspondiente\n",
    "\n",
    "\n",
    "## Referencias útiles para la práctica\n",
    "\n",
    "1. [Documentación oficial](https://www.tensorflow.org/learn?hl=es-419) de Tensorflow\n",
    "2. [Guías](https://keras.io/guides/) de Keras\n",
    "\n",
    "\n",
    "\n",
    "## Tensorflow\n",
    "TensorFlow es una plataforma de código abierto para machine learning, desarrollada por Google. Permite la computación numérica eficiente, especialmente optimizada para el entrenamiento y despliegue de modelos de aprendizaje automático y redes neuronales profundas a gran escala, soportando su ejecución en una amplia variedad de dispositivos y sistemas, incluyendo CPUs, GPUs y TPUs. Originalmente, Tensorflow requería el desarrollo de los modelos a través de tensores de forma poco amigable, lo que hizo que apareciesen APIs que facilitasen una interfaz para poder emplear esta librería. En particular, Keras fue muy popular hasta el punto que Tensorflow la integró dentro de su desarrollo a partir de la versión 2.\n",
    "\n",
    "## Keras\n",
    "\n",
    "\n",
    "Keras es una API de alto nivel escrita en Python que actúa como una interfaz simplificada y amigable para construir y entrenar modelos de aprendizaje profundo. Keras puede  ejecutarse sobre diferentes \"motores\" (*backends*) como JAX, TensorFlow y PyTorch. Particularmente, la versión Keras 2  está totalmente integrado en TensorFlow 2 a través del módulo de `tf.keras`. En esta práctica usaremos la última versión disponible (Keras 3), lo que requiere instalarla de forma independiente y permite seleccionar el *backend* que usará para generar los modelos.\n",
    "\n",
    "## 1. Introducción \n",
    "\n",
    "**Los procesos industriales secuenciales suelen ser altamente no lineales y presentar importantes dependencias temporales**. Una intervención en una fase temprana sobre un producto intermedio no impacta en el resultado final de forma inmediata, sino después del tiempo que tarda en recorrer el resto de la cadena productiva.\n",
    "\n",
    "En general los modelos vistos hasta ahora no tienen en cuenta el tiempo. Típicamente son \"fotos fijas\" de un proceso en el que se recogen las medidas o características de ese momento dado. Por ejemplo, cada observación puede estar compuesta de las medidas de todos los sensores desplegados en la factoría en un segundo concreto.\n",
    "En un proceso secuencial con dependencias temporales, estaríamos asociando a la variable/s de salida/s, características de entrada que no fueron las que realmente afectaron  y generaron dicha salida.\n",
    "\n",
    "Típicamente,  para poder trabajar en esta situación en una línea de producción tenemos 2 opciones:\n",
    "1. Podemos **alinear los datos**, es decir, tratar de crear observaciones que cuadren las entradas concretas que afectaron a la salida escogida. Esto es algo tremendamente complicado si no se conoce perfectamente el proceso productivo, características, tiempos, retrasos, etc.\n",
    "2. Podemos **trabajar con modelos que recuerden y tengan en cuenta lo que ha pasado en instantes anteriores**.\n",
    "\n",
    "Trabajar con modelos recurrentes que tengan en cuenta el tiempo es algo que suele beneficiar el desarrollo de predictores en plantas industriales pero son más complicados de desarrollar y entrenar. Librerías típicas como `scikit-learn`no contemplan el  desarrollo de este tipo de modelos pero existen otros frameworks centrados en el desarrollo de redes neuronales profundas en general y que permiten crear redes recurrentes de diferentes tipos. [Tensorflow](https://www.tensorflow.org/?hl=es-419) es uno de los *frameworks* más populares y [Keras](https://keras.io/) es el API más empleada para el desarrollo de modelos con Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7de20c-fb35-425b-b0cf-232e7d70749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importaciones generales\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as err:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as err:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except ImportError as err:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError as err:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "# Asegurarnos de usar Keras 3 con backend TensorFlow\n",
    "# Es necesario hacerlo antes de cargar Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Importaciones de Keras y TensorFlow\n",
    "try:\n",
    "    import keras\n",
    "except ImportError as err:\n",
    "    !pip install keras\n",
    "    import keras\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf \n",
    "except ImportError as err:\n",
    "    !pip install tensorflow\n",
    "    import tensorflow as tf \n",
    "\n",
    "\n",
    "\n",
    "# Importaciones de Scikit-learn\n",
    "#Solo para facilitarnos el uso de algunas operaciones típicas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# Configuraciones para visualización. No es necesario\n",
    "plt.style.use('seaborn-v0_8-whitegrid') # Estilo de gráficos\n",
    "sns.set_palette('viridis') # Paleta de colores\n",
    "\n",
    "\n",
    "# Verificación de versiones\n",
    "#Existen diferentes versiones y compatibilidades. Es importantes saber lo que estamos ejecutando\n",
    "#Para estos ejemplos queremos ejecutar el API de Keras3 (standalone) y tensorflo3>2.16\n",
    "print(f\"Versión de Keras: {keras.__version__}\")\n",
    "print(f\"Backend de Keras: {keras.backend.backend()}\")\n",
    "print(f\"Versión de TensorFlow: {tf.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "# Verificar si TensorFlow puede usar GPU en nuestro sistema (opcional, pero bueno saberlo)\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"GPU disponible: {gpu_devices}\")\n",
    "else:\n",
    "    print(\"GPU no encontrada, se usará CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d3959-43bb-482b-bd25-cca6ef8940dd",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "Para el desarrollo del ejemplo emplearemos el *dataset* público de Kaggle: [Concrete Compressive Strength Data Set](https://www.kaggle.com/datasets/elikplim/concrete-compressive-strength-data-set/data).\n",
    "El repositorio contiene una descripción del conjunto de datos y de sus características:\n",
    "\n",
    "\n",
    "**Concrete Compressive Strength Data Set**\n",
    "\n",
    "\n",
    "*Abstract: Concrete is the most important material in civil engineering. The\n",
    "concrete compressive strength is a highly nonlinear function of age and\n",
    "ingredients. These ingredients include cement, blast furnace slag, fly ash,\n",
    "water, superplasticizer, coarse aggregate, and fine aggregate.*\n",
    "\n",
    "| **Nombre** | **Tipo de dato** | **Medida** | **Descripción** |\n",
    "|---|---|---|---|\n",
    "| Cement (component 1) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Blast Furnace Slag (component 2) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Fly Ash (component 3) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Water (component 4) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Superplasticizer (component 5) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Coarse Aggregate (component 6) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Fine Aggregate (component 7) | quantitative | kg in a m³ mixture | Input Variable |\n",
    "| Age | quantitative | Day (1\\~365) | Input Variable |\n",
    "| **Concrete compressive strength** | quantitative | MPa | **Output Variable** |\n",
    "\n",
    "\n",
    "Para facilitar el acceso a los datos, descargamos la última versión del dataset empleando `kagglehub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e850ba4-cf85-4370-b308-e73c19ee09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"elikplim/concrete-compressive-strength-data-set\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "#Nombre del fichero descargado: concrete_data.csv\n",
    "df_concrete=pd.read_csv(path+\"/concrete_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a339a-f9b9-4464-9215-4bd7ad1c668a",
   "metadata": {},
   "source": [
    "## 3. Análisis de los datos\n",
    "\n",
    "\n",
    "**IMPORTANTE**: *el objetivo  de este Notebook es desarrollar modelos con Keras y Tensorflow, por lo que simplificaremos al máximo las fases de análisis y preprocesado. El rendimiento del modelo no es prioritario. Conocéis de materias previas la **importancia de analizar y preparar los datos  para mejorar la precisión de los modelos**.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b69820-bab2-4d2e-9cd8-25594a913c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numero de filas: {len(df_concrete)}\")\n",
    "print(f\"Numero de características: {len(df_concrete.columns)}\")\n",
    "print(\"#\"*50)\n",
    "print(\"\\nInformación general del dataset:\")\n",
    "df_concrete.info()\n",
    "\n",
    "print(\"#\"*50)\n",
    "\n",
    " # Vistazo rápido a los datos\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(df_concrete.head())\n",
    "print(\"#\"*50)\n",
    "\n",
    "\n",
    "# Comprobar valores nulos (aunque ya se ven en el info) \n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df_concrete.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad129cf-3034-425e-86bf-4ef78cf40e66",
   "metadata": {},
   "source": [
    "Aunque el análisis detallado de los datos no es el objetivo de este Notebook, **dividiremos**, ya en este punto, **el dataset para evitar el  llamado \"sesgo de espionaje\"** y así poder estudiar el conjunto y con ello generar operaciones de preprocesado que permitan mejorar el futuro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6a44b-5f75-48b9-aac6-1515f4b2ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecemos una semilla para la reproducibilidad. \n",
    "#Emplearemos esta semilla para todos los procesos pseudoaleatorios\n",
    "SEED=42\n",
    "\n",
    "\n",
    "\n",
    "#Dividimos en entrenamiento, validación y test \n",
    "# Usaremos un 70% para entrenamiento, un 15% para Validación y un 15% para Test.\n",
    "#Nos devuelve un array de numpy\n",
    "\n",
    "concrete_train, concrete_pruebas_aux=train_test_split(df_concrete, test_size=0.30, random_state=SEED)#%70% para entrenamiento\n",
    "concrete_val, concrete_test=train_test_split(concrete_pruebas_aux, test_size=0.50, random_state=SEED)#15% Val y 15% Test\n",
    "\n",
    "\n",
    "\n",
    "#Convertimos a DataFrames para mejor visualización (opcional)\n",
    "df_concrete_train = pd.DataFrame(concrete_train, columns=df_concrete.columns)\n",
    "df_concrete_val = pd.DataFrame(concrete_val, columns=df_concrete.columns)\n",
    "df_concrete_test = pd.DataFrame(concrete_test, columns=df_concrete.columns)\n",
    "\n",
    "print(f\"Datos de entrenamiento: ({len(df_concrete_train)},{len(df_concrete_train.columns)})\")\n",
    "print(f\"Datos de validacion: ({len(df_concrete_val)},{len(df_concrete_val.columns)})\")\n",
    "print(f\"Datos de test: ({len(df_concrete_test)},{len(df_concrete_test.columns)})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Analizamos solo el conjunto de entrenamieto para evitar el sesgo de espionaje\n",
    "print(\"Estadísticas descriptivas del conjunto de entrenamiento:\")\n",
    "print(df_concrete_train.describe().transpose())\n",
    "\n",
    "\n",
    "for col in df_concrete_train.columns:\n",
    "    sns.histplot(data=df_concrete_train[col], bins=50)\n",
    "    plt.show() # Seaborn configura implícitamente el objeto plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(df_concrete_train.corr(),vmin=-1, vmax=1, annot=True, cmap='BrBG')#colores más oscuros marcan más correlación\n",
    "plt.title('Correlation Heatmap');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b01ef-5c0d-4882-abf0-947365e107db",
   "metadata": {},
   "source": [
    "## 4. Preparación de los datos\n",
    "tal y como se mencionó anteriormente, realizaremos una preparación mínima de los datos cara al desarrallo del modelo. En particular estandarizaremos los valores de las características. Sería conveniente realizar otro tipo de operaciones cara a continuar mejorando el modelo.\n",
    "\n",
    "### 4.1 Estandarización de los datos con Keras\n",
    "\n",
    "\n",
    "En lugar de estandarizar los datos a través de una operación de `scikit-learn` (sistema que ya conocéis), lo haremos desde el propio Keras generando una [capa de preprocesado](https://keras.io/api/layers/preprocessing_layers/). Esto tiene como ventaja poder incluirlo en el propio modelo y que este sea autosuficiente. Debéis de recordar que el entrenamiento de los modelos es solo la primera fase, luego es necesario desplegarlos en producción y trabajar con ellos. Poder guardar con el modelo toda la información necesaria para poder emplearlo (ej. capa de estandarización), simplifica su empleo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae639ba2-93a6-4939-b2ab-8fe25bc93178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separamos las entradas de las salidas para los 3 conjuntos\n",
    "def separar_inputs_outputs(dataset):\n",
    "    if 'concrete_compressive_strength' in dataset.columns: \n",
    "        return dataset.drop('concrete_compressive_strength', axis=1), dataset['concrete_compressive_strength']\n",
    "    else: return None, None\n",
    "\n",
    "\n",
    "df_concrete_train_X, df_concrete_train_y=separar_inputs_outputs(df_concrete_train)\n",
    "df_concrete_val_X, df_concrete_val_y=separar_inputs_outputs(df_concrete_val)\n",
    "df_concrete_test_X, df_concrete_test_y=separar_inputs_outputs(df_concrete_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#creamos una capa de preprocesado para normalizar\n",
    "normalization_layer = keras.layers.Normalization(axis=-1) # Normaliza a través del eje de características\n",
    "\n",
    "#adapt es el método que calcula los datos de estandarización pero\n",
    "# Keras espera un array NumPy o Tensor, no un DataFrame directamente.\n",
    "#df.values genera un array de numpy\n",
    "normalization_layer.adapt(df_concrete_train_X.values)\n",
    "print(f\"  Medias aprendidas: {normalization_layer.mean.numpy()}\")\n",
    "print(f\"  Varianzas aprendidas: {normalization_layer.variance.numpy()}\")\n",
    "\n",
    "#Probamos la estandarización con los datos de entrenamiento\n",
    "df_concrete_train_X_scaled=normalization_layer(df_concrete_train_X.values)\n",
    "print(\"Media de las entradas del dataset de entrenamiento escalado (debería ser cercana a 0):\")\n",
    "#La capa devuelve un tensor que es necesario pasar a un array de numpy\n",
    "print(df_concrete_train_X_scaled.numpy().mean())\n",
    "print(\"Desviación estándar de las entradas del dataset de entrenamiento escalado (debería ser cercana a 1):\")\n",
    "print(df_concrete_train_X_scaled.numpy().std())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894c639-0a9a-40f5-bf3f-bd6ad7c8bac1",
   "metadata": {},
   "source": [
    "## 5. Desarrollo  del modelo con Keras y Tensorflow\n",
    "### 5.1 Definición del modelo\n",
    "\n",
    "Keras permite general modelos empleando dos opciones:\n",
    "1. El [API secuencial](https://keras.io/guides/sequential_model/): Permite generar redes densas con diferentes capas donde cada capa tiene exactamente un tensor de entrada y un tensor de salida. Se generan capas que se unen (apilan) secuencialmente.\n",
    "2. El [API funcional](https://keras.io/guides/functional_api/): permite crear modelos más flexibles que los secuenciales. Esta API permite gestionar modelos con topologías no lineales. Permite crear \"grafos de capas\".\n",
    "\n",
    "\n",
    "\n",
    "Vamos a definir nuestra red neuronal usando el API Secuencial de Keras (`keras.Sequential`), ya que es suficiente crear un modelo que \"apile\"  capas una tras otra y cubre nuestras necesidades.\n",
    "\n",
    "\n",
    "Definiremos una red simple:\n",
    "- **Capa de Entrada**: Definida de forma explícita a través de  `keras.Input`. Su parámetro `shape` debe coincidir con el número de características de nuestro dataset (8 en nuestro caso). Es importante destacar que no es necesario crear de forma explícita esta capa, se puede obtener la información de forma indirecta en el  proceso de entrenamiento pero definirla de forma explícita nos permite ver un resumen completo de la red generada (incluyendo los pesos), en el momento que se define.\n",
    "- **Capas Ocultas**: Usaremos un par de capas densas (`keras.layers.Dense`) con activación ReLU (`relu`). ReLU es una opción común y eficiente para capas ocultas en redes profundas. El número de neuronas es un hiperparámetro a optimizar (empezaremos con 64 y 32).\n",
    "- **Capa de Salida**: Una única neurona (Dense(1)) ya que es un problema de regresión (predecir un solo valor continuo). Usaremos la función de activación `linear`), ya que  queremos que la salida pueda tomar cualquier valor real, no limitado a un rango específico.\n",
    "\n",
    "Explicación del código:\n",
    "- keras.Sequential([...]): Crea un modelo donde las capas se ejecutan en secuencia.\n",
    "- keras.Input(shape=(n_features,)): Define formalmente la forma de los datos de entrada. Ayuda a Keras a construir el modelo correctamente.\n",
    "- keras.layers.Dense(units, activation=...): Define una capa totalmente conectada (densa). `units` es el número de neuronas. `activation` es la función de activación para cada una de las neuronas de esa capa.\n",
    "- model.summary(): Muestra las capas, la forma de salida de cada capa y el número de parámetros entrenables. Es útil para verificar la arquitectura definida.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a7445-d1f5-4c85-b582-96c741c4a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establece la semilla para los números aleatorios con  keras.utils.set_random_seed. Esto establecerá:\n",
    "# 1) `numpy` seed\n",
    "# 2) backend random seed\n",
    "# 3) `python` random seed\n",
    "#Permitir la reproducibilidad es clave para poder comparar\n",
    "keras.utils.set_random_seed(SEED)\n",
    "\n",
    "\n",
    "# Obtenemos el número de características para la capa de entrada\n",
    "n_features = len(df_concrete_train_X.columns)\n",
    "print(f\"Número de características de entrada: {n_features}\")\n",
    "\n",
    "# Construcción del modelo secuencial\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(n_features,), name=\"input_layer\"), # Capa de entrada explícita. \n",
    "                                                              #No es obligatoria pero nos permite ver el resumen \n",
    "                                                              #completo de la red al crearla, incluyendo los pesos\n",
    "        normalization_layer, #capa de normalización que generamos anteriormente. El modelo es autosuficiente\n",
    "        keras.layers.Dense(64, activation='tanh', name='hidden_layer_1'), # 1ra capa oculta con 32 neuronas y activación ReLU\n",
    "        keras.layers.Dense(32, activation='tanh', name='hidden_layer_2'), # 2da capa oculta con 16 neuronas y activación ReLU\n",
    "        keras.layers.Dense(1, activation='linear', name='output_layer')   # Capa de salida con 1 neurona (regresión) y activación lineal\n",
    "    ],\n",
    "    name=\"Concrete_Strength_Predictor\" # Nombre opcional para el modelo\n",
    ")\n",
    "\n",
    "\n",
    "# Mostrar un resumen de la arquitectura del modelo\n",
    "print(\"\\nResumen del modelo:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61903b4-20de-4e60-a613-513c07e1d84d",
   "metadata": {},
   "source": [
    "Keras permite añadir capas a un modelo con el método `add`, lo que puede ser útil para implementar funciones que nos permitan **generar modelos dinámicamente**, lo que podremos usar para la optimización de hiperparámetros.La siguiente función os muestra un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006fe9f-db6f-429c-b2b1-d627a8995915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_modelo(input_neurons, hidden_layer_neurons, preprocesing_layers, name_model):\n",
    "    #input layer\n",
    "    model = keras.Sequential( [keras.Input(shape=(input_neurons,), name=\"input_layer\")], name=name_model)\n",
    "   \n",
    " \n",
    "    #preprocesing layers\n",
    "    for layer in preprocesing_layers:\n",
    "       model.add(layer)\n",
    "        \n",
    "    #Hidden layers\n",
    "    for i, neurons in   enumerate(hidden_layer_neurons,start=1):\n",
    "       model.add(keras.layers.Dense(neurons, activation='relu', name=f'hidden_layer_{i}'))\n",
    "\n",
    "    #output layer\n",
    "    model.add(keras.layers.Dense(1, activation='linear', name='output_layer'))\n",
    "    \n",
    "    return model \n",
    "       \n",
    "\n",
    "\n",
    "modelo2=crear_modelo(input_neurons=8, hidden_layer_neurons=[64,32,16], preprocesing_layers=[normalization_layer], name_model=\"Concrete_Strength_Predictor2\")\n",
    "print(\"\\nResumen del modelo2:\")\n",
    "modelo2.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8adfe7-9359-4736-a376-471a92789aaf",
   "metadata": {},
   "source": [
    "### 5.2 Compilación del modelo\n",
    "\n",
    "\n",
    "Antes de entrenar, necesitamos \"compilar\" el modelo para configurar su proceso de entrenamiento. Esto implica definir:\n",
    "\n",
    "* **Optimizador**: Algoritmo que ajusta los pesos de la red durante el entrenamiento (ej. Adam, SGD, RMSprop). Adam es una opción robusta y popular. Tenéis [aquí](https://keras.io/api/optimizers/) un listado de los disponibles. La configuración de los parámetros del optimizador sería algo para optimizar en la búsqueda hiperparamétrica.\n",
    "\n",
    "* **Función de Pérdida** (*Loss Function*): Mide qué tan bien se desempeña el modelo durante el entrenamiento. Para regresión, mean_squared_error (MSE) es una opción muy típica. Tenéis [otras](https://keras.io/api/losses/) disponibles.\n",
    "\n",
    "\n",
    "* **Métricas**: Funciones para evaluar el rendimiento del modelo (ej. mean_absolute_error - MAE). **Las métricas no se usan para optimizar el modelo, solo para monitorizarlo**. Tenéis [aquí](https://keras.io/api/metrics/) un listado de las disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83ba4c-96ae-4f43-a0bf-975aab3e91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001), # Optimizador Adam con tasa de aprendizaje típica\n",
    "        loss='mean_squared_error',          # Función de pérdida para regresión (MSE)\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name='mae')] # Métrica para monitorear (MAE)\n",
    "    )\n",
    "\n",
    "print(\"Modelo compilado exitosamente.\")\n",
    "print(f\"Optimizador: {type(model.optimizer).__name__}\")\n",
    "print(f\"Función de pérdida: {model.loss}\")\n",
    "print(\"Métricas: \")\n",
    "#metricas incluye la empleada como función de pérdida\n",
    "for m in model.metrics:\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5382ae0-48da-4660-8024-68b25e859350",
   "metadata": {},
   "source": [
    "### 5.3  Entrenamiento del modelo\n",
    "\n",
    "Ya tenemos definido y configurado el modelo para poder entrenarse con nuestro  dataset de entrada. Recordad que el **modelo contiene una capa de normalización**, por lo que **los datos que se le pasen para entrenar NO pueden estar normalizados**(lo hará el propio modelo).\n",
    "\n",
    "\n",
    "El método `fit` es el empleado para entrenar. Tenéis [aquí](https://keras.io/api/models/model_training_apis/#fit-method) disponible todos los parámetros posibles. En este caso emplearemos:\n",
    "\n",
    "* **epochs**: Número de veces que el modelo verá el conjunto de datos completo.\n",
    "* **batch_size**: Número de muestras procesadas antes de actualizar los pesos (entrenamiento con *mini-batches*).\n",
    "* **validation_data**: conjunto de datos sobre los que validaremos el entrenamiento del modelo. Lo podemos emplear para comparar configuraciones o para parar el entrenamiento de forma prematura (*early_stopping*), lo que puede ayudar a detectar el *overfitting*.\n",
    "\n",
    "El método `fit` devuelve un objeto *History* que contiene información sobre el proceso de entrenamiento (pérdida y métricas en cada época) que podremos utilizar posteriormente para graficar el proceso de entreno y poder valorar si tenemos *overfitting* o *underfitting*.\n",
    "\n",
    "\n",
    "**IMPORTANTE**: cada vez que ejecutéis el entrenamiento, el modelo parte de su último estado. Si queréis volver a entrenarlo desde cero, entonces tenéis que volver a crearlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d48fb-1be1-4f8c-8722-f180332683e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##IMPORTANTE: si se vuelve a ejecutar esta celda sin generar el modelo desde 0\n",
    "##Partiréis de los pasos del último entrenamiento\n",
    "# Definimos el número de épocas y el tamaño del batch\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history = model.fit(\n",
    "    df_concrete_train_X.values,\n",
    "    df_concrete_train_y.values,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(df_concrete_val_X.values, df_concrete_val_y.values),\n",
    "    verbose=1 # Muestra una barra de progreso por época (0=silencioso, 1=barra, 2=una línea por época)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a473300d-2525-47ba-88f7-7b10ad83eb49",
   "metadata": {},
   "source": [
    "### 5.4 Evaluación del Modelo\n",
    "\n",
    "Una vez entrenado el modelo, tenemos que evaluar su rendimento. Lo haremos desde dos puntos de vista:\n",
    "\n",
    "1. **Visualizando el historial de entrenamiento**: Graficaremos la pérdida y la/s métrica/s disponibles (el MAE en este caso) tanto para el conjunto de entrenamiento como para el de validación a lo largo de las épocas. Esto nos ayuda a diagnosticar problemas como el *overfitting* (cuando la pérdida de validación empieza a aumentar mientras la de entrenamiento sigue bajando) o el *underfitting* (cuando ambas pérdidas son muy altas).\n",
    "2. **Evaluando en el conjunto de Test**: Usaremos el método `evaluate` con los datos de Test, que el modelo nunca ha visto, para generar la  estimación final e imparcial del rendimiento del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4ad87-f428-4630-a859-a6b78f74c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficas_entrenamiento(hist):\n",
    "    hist=pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(hist['epoch'], hist['loss'], label='Pérdida Entrenamiento')\n",
    "    plt.plot(hist['epoch'], hist['val_loss'], label = 'Pérdida Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida (MSE)')\n",
    "    plt.title('Pérdida (MSE) durante Entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Asegúrate de usar el nombre correcto de la métrica (puede variar ligeramente)\n",
    "    mae_key = 'mae' if 'mae' in hist.columns else list(hist.columns)[1] # Intenta encontrar la clave MAE\n",
    "    val_mae_key = 'val_' + mae_key\n",
    "    plt.plot(hist['epoch'], hist[mae_key], label='MAE Entrenamiento')\n",
    "    plt.plot(hist['epoch'], hist[val_mae_key], label = 'MAE Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Error Absoluto Medio (MAE)')\n",
    "    plt.title('MAE durante Entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#1. Graficar el entrenamiento por épocas. Ayuda a valorar el posible (sobre/infra)entrenamiento\n",
    "graficas_entrenamiento(history)\n",
    "\n",
    "# 2. Evaluar en el conjunto de prueba\n",
    "print(\"Evaluando el modelo en el conjunto de prueba. Métrica general\")\n",
    "test_loss, test_mae = model.evaluate(df_concrete_test_X.values, df_concrete_test_y.values, verbose=0)\n",
    "\n",
    "print(f\"Pérdida en el conjunto de Test (MSE): {test_loss:.4f}\")\n",
    "print(f\"Error Absoluto Medio (MAE) en el conjunto de Test: {test_mae:.4f}\")\n",
    "\n",
    "y_pred_test = model.predict(df_concrete_test_X.values) # Aplanar para que tenga la misma forma que y_test\n",
    "\n",
    "# Calcular R^2 score (opcional, pero útil en regresión)\n",
    "r2 = r2_score(df_concrete_test_y.values, y_pred_test)\n",
    "print(f\"Coeficiente de Determinación (R²) en el conjunto de Test: {r2:.4f}\")\n",
    "\n",
    "#Scatterplot entre los valores reales y los predichos\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.regplot(x=df_concrete_test_y.values, y=y_pred_test.flatten())\n",
    "plt.title(\"Concrete_Strength_Predictor\")\n",
    "plt.xlabel('Valores reales')\n",
    "plt.ylabel('Predicciones')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69e4d4c-8263-4f4c-b917-c9e604134f1e",
   "metadata": {},
   "source": [
    "### 5.5 Almacenamiento del modelo\n",
    "Una vez entrenado el modelo es necesario guardarlo para poder desplegarlo en producción en el futuro. Este es un proceso sencillo en Keras y que se realiza a través del método `save` que podéis consultar [aquí](https://keras.io/api/models/model_saving_apis/model_saving_and_loading/#save-method). Este método guarda toda la información necesaria para el modelo el un archivo comprimido `.keras` con el siguiente contenido:\n",
    "\n",
    "- La arquitectura del modelo.\n",
    "- Los pesos del modelo.\n",
    "- El estado del optimizador (si es necesario).\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e36c3f-1ac8-40da-adbd-2e615def5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_SAVED_MODEL=\"./concrete_model.keras\"\n",
    "\n",
    "model.save(filepath=PATH_SAVED_MODEL)\n",
    "del model #eliminamos de memoria el modelo para estar seguros de que no lo tenemos cargado\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b14a45-5d12-4b76-8837-6f5c9eace6b4",
   "metadata": {},
   "source": [
    "### 5.6 Cargar el modelo\n",
    "Una vez tenemos un modelo entrenado y almacenado, deberemos desplegarlo en producción. Para ello cargaremos el modelo a través del método `load_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d05656-e6a0-4f43-8804-bbf4a3121d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.saving.load_model(PATH_SAVED_MODEL)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527b14e-2208-41a0-ac74-d8fd8a465a0f",
   "metadata": {},
   "source": [
    "Una vez tenemos cargado el modelo nuevamente, podemos emplearlo para predecir en producción pero también podríamos usarlo para recuperar los pesos y realizar un reentrenamiento para minimizar un posible *concept drift*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3afbc2-ccd8-4b87-b34b-c2c54d910976",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Podemos acceder a los pesos de la red (capa a capa)\n",
    "model.layers[1].get_weights()[0] #pesos de la capa oculta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3afc88-13e6-4a5b-9f7b-e948dcf595a1",
   "metadata": {},
   "source": [
    "### 5.7 Predecir con el modelo en producción\n",
    "Una vez cargado el modelo podemos emplearlo para generar predicciones de los datos entrantes en producción. Simularemos con flujo de datos con el conjunto de Test para ver cómo se realizaría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b37fb-953f-4ad8-a29d-24aa77e25704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suponiendo que obtenemos datos desde un flujo de datos en tiempo real\n",
    "#en este caso no tenemos acceso a la salida (output) \n",
    "\n",
    "for observacion in range(5):\n",
    "    fila=df_concrete_test_X.iloc[[observacion]].values\n",
    "    prediction=model.predict(fila, verbose=0)\n",
    "    print(f\"La predicción es {prediction}\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f2eaa5-b7d9-4802-b010-ac1e318e94a1",
   "metadata": {},
   "source": [
    "#### EJERCICIO PARA DESARROLLAR Y ENTREGAR EN EL AULA VIRTUAL\n",
    "\n",
    "Con el objetivo de aprender como realizar ciertas operaciones con la interfaz de Keras, generaréis y probaréis un nuevo modelo con los siguientes cambios:\n",
    "* **Modificaréis la función de activación** de las capas ocultas empleando la **tangente hiperbólica**\n",
    "* **Añadiréis regularización** al modelo para evitar el sobreentrenamiento empleando  **DropOut** en cada una de las capas ocultas.\n",
    "    - Un `rate' del 20% es un buen punto de partida.\n",
    "    - La capa de entrada no tendrá el **DropOut**.\n",
    "\n",
    "* Añadiréis la **métrica de Root Mean Squared Error** (junto al MAE).     \n",
    "  \n",
    "* Estudiaréis el concepto de ***callbacks*** en Keras y **añadiréis un *EarlyStop*** a vuestro modelo.\n",
    "    - Monitoriza la pérdida de validación.\n",
    "    - Esperarés 6 rondas para terminar el entrenamiento.\n",
    "* Evaluaréis el modelo y visualizaréis las métricas de entrenamiento\n",
    "\n",
    "Podéis escoger los valores que consideréis oportunos para los parámetros para los que no se especifica un cambio (ej. capas ocultas y neuronas)\n",
    "\n",
    "**IMPORTANTE**: el modelo se generará dinámicamente a través de una función `crear_modelo_ej` (como en el ejemplo previo). Deberéis modificar la función de forma oportuna.\n",
    "\n",
    "Este ejercicio contendrá los diferentes pasos en el desarrollo del modelo:\n",
    "- Definición del modelo.\n",
    "- Compilación del modelo.\n",
    "- Entrenamiento del modelo.\n",
    "- Evaluación del Modelo.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
