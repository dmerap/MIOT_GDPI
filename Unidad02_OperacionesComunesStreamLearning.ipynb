{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde64848-c46c-473a-8d70-8a215f907131",
   "metadata": {},
   "source": [
    "![MIoT_GDPI](img/MIOT_GDPI_header.png)\n",
    "\n",
    "# Unidad 02 - Operaciones comunes en Stream Learning\n",
    "\n",
    "El objetivo principal de esta práctica es que os familiaricéis con las operaciones básicas para trabajar con el aprendizake incremental y como integrarlas en un *pipeline* de [River](https://riverml.xyz/). Comprobareis que su uso es similar al de `scikit-learn` que ya conocéis.\n",
    "\n",
    "La mayor parte del contenido de este Notebook se dedica a explicar el uso de los comandos y de su integración en un *pipeline* apoyándose en ejemplos concretos que ilustran su aplicación a un problema real. Es crucial que dediquéis tiempo a leer y comprender el material, en lugar de simplemente ejecutar el código. Os invitamos a experimentar modificando y variando el código proporcionado para que podáis explorar las distintas opciones y profundizar en su funcionamiento.\n",
    "\n",
    "\n",
    "\n",
    "**Importante**: El Notebook contiene varios ejercicios sencillos. Deberéis desarrollarlos durante la clase y enviarlos por el aula virtual del curso, en la tarea correspondiente.\n",
    "\n",
    "\n",
    "\n",
    "## Referencias útiles para la práctica\n",
    "1. API Pandas: [https://pandas.pydata.org/docs/reference/index.html](https://pandas.pydata.org/docs/reference/index.html)\n",
    "2. Api River: [https://riverml.xyz](https://riverml.xyz)\n",
    "3. Api Scikit-Learn: [https://scikit-learn.org/stable/api/index.html](https://scikit-learn.org/stable/api/index.html)\n",
    "4. Bahri, M., Bifet, A., Gama, J., Gomes, H. M., & Maniu, S. (2021). [Data stream analysis: Foundations, major tasks and tools](https://doi.org/10.1002/widm.1405). Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 11(3), e1405.\n",
    "5. Gomes, H. M., Read, J., Bifet, A., Barddal, J. P., & Gama, J. (2019). [Machine learning for streaming data: state of the art, challenges, and opportunities](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://kdd.org/exploration_files/3._CR_7._Machine_learning_for_streaming_data_state_of_the_art-Final.pdf). ACM SIGKDD Explorations Newsletter, 21(2), 6-22.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d32499-9479-4eb3-bfca-eea5f0ee6b7c",
   "metadata": {},
   "source": [
    "## 1. Del Aprendizaje Automático tradicional (*batch*) al  Aprendizaje Automático incremental\n",
    "\n",
    "Este tutorial está inspirado en los [ejemplos](https://riverml.xyz/latest/recipes/reading-data/) proporcionados por River en su [página web](https://riverml.xyz/latest/), por  tanto, os animamos a consultar la documentación original para una exploración más profunda de cada uno de los temas particulares que se muestren a continuación.\n",
    "\n",
    "En general, casi cualquier enfoque asociado al aprendizaje automático contiene los siguientes pasos:\n",
    "\n",
    "1. Entender y definir el problema y su contexto.\n",
    "2. Obtener los datos.\n",
    "3. Explorar, analizar y entender los datos.\n",
    "4. Preprocesar los datos.\n",
    "5. Seleccionar, optimizar y entrenar los modelos ML.\n",
    "6. Evaluar y el modelo seleccionado.\n",
    "7. Desplegar, monitorizar y mantener la solución.\n",
    "\n",
    "Antes de comenzar, vamos a examinar un ejemplo clásico que utiliza todos estos pasos utilizando la librería [scikit-learn](https://scikit-learn.org/stable/):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028cc4e-86ba-41fb-ab0f-20cb17c700d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from rich import print\n",
    "\n",
    "\n",
    "# Carga de datos\n",
    "dataset = load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target #inputs,outputs\n",
    "\n",
    "# Preparación del pipeline con los pasos de preprocesado y el modelo seleccionado\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),                       # Estandarización de los datos avg 0 y std 1\n",
    "    ('extractor', PCA(0.95)),                          # Reducción de la dimensionalidad con PCA (0.95 de varianza)\n",
    "    ('classifier', LogisticRegression(solver='lbfgs')) # Selección y configuración del modelo\n",
    "])\n",
    "\n",
    "# Definición de una validación cruzada estableciendo una semilla. Importante poder replicar!\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluación del modelo para cada fold de la validación cruzada\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "scores = cross_val_score(pipe, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Imprime la media de la métrica y su desviación estándard a través de todos los folds\n",
    "print(f'ROC AUC: {scores.mean():.4f} (± {scores.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881aa86b-be0c-418b-bb7e-f1b9e0c2ddfc",
   "metadata": {},
   "source": [
    "**¿Cuál es el problema con esta aproximación?** En realidad, no tiene ninguno, pero existen algunas desventajas potenciales, particularmente con respecto al tamaño del conjunto de los datos, la aparición de datos nuevos y la variabilidad de las características a lo largo del tiempo.\n",
    "\n",
    "* **Tamaño del conjunto de datos**: imagina que los datos para el entrenamiento fueran demasiado grandes para caber en la memoria de tu ordenador. En ese caso, el programa, probablemente,  se bloquearía. Aunque se pueden aplicar diferentes técnicas para minimizar este problema,  existe un límite para estas optimizaciones. De hecho, si el dataset constara de millones de observaciones (ej.  cientos de gigabytes), podría haberse requerido hardware especial solo para cargar el dataset.\n",
    "\n",
    "* **Datos nuevos**: otro problema potencial es la incorporación de  datos  nuevos al modelo, una vez se ha entrenado. Los enfoques tradicionales requieren comenzar a entrenar desde cero con un nuevo conjunto de datos resultante de la combinación de los datos antiguos con las nuevas observaciones disponibles. Esto es particularmente complicado en aplicaciones que se ejecutan en tiempo real para las que tienes datos nuevos de forma continua. En muchas aplicaciones reales, la solución es desarrollar un pipeline de integración continua que pueda generar y desplegar un nuevo modelo cada poco. es importante destacar que por datos nuevos nos referimos a observaciones que son radicalmente diferentes de las empleadas para entrenar, dónde las relaciones entre las entradas (*inputs*) y salidas (*outputs*) ha cambiado de forma que el modelo no puede gestionarlas.\n",
    "\n",
    "* **Variabilidad de las características**: finalmente, otra desventaja con el enfoque tradicional es que las características con las que se ha entrenado el modelo deberán estar siempre disponibles durante el tiempo de producción. Además, no pueden incorporarse nuevas características a lo largo del tiempo. En escenarios de plantas industriales es habitual que determinadas características no estén disponibles en momentos determinados (sensores estropeados o con mal funcionamiento) para luego estar nuevamente disponibles. En estos mismos escenarios, nuevos sensores pueden desplegarse y empezar a medir y generar nuevas características. Desde un punto de vista tradicional, la única forma de gestionar un número variable de características es reentrenando desde cero.\n",
    "\n",
    "\n",
    "## 2. *Incremental Learning*\n",
    "\n",
    "Como se mencionó en la unidad anterior, el aprendizaje incremental (*incremental learning*) también se conoce como aprendizaje en línea (*online learning*) o aprendizaje de flujo (*stream learning*). Es habitual que se sustituya el término *online* por incremental o *stream* debido a lo confuso del concepto de *online learning*, ya que, generalmente, se refiere a una opción educativa (esto es fácil de comprobar buscando en Google *online learning*). \n",
    "\n",
    "El concepto de *incremental learning* se refiere a ajustar un modelo ML a partir de las observaciones obtenidas a través de un flujo (*stream*) de datos en tiempo real. Expresado de otra manera, los datos para entrenar no están disponibles en su totalidad como en el ML tradicional (dataset de entrenamiento), sino que las observaciones se proporcionan una por una a medida que se leen de un flujo de datos. Por ejemplo, imagina el caso anterior que, en lugar de disponer del conjunto de datos completo para entrenar, tenemos un punto de referencia temporal que proporciona una observación a la vez. En una factoría serían simplemente las medidas que cada segundo se toman de los sensores desplegados. Este flujo se puede simular con un simple bucle: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc1e04-015d-45b7-b6ba-45adfca3c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xi, yi in zip(X, y):#simula obtener una observación por cada vuelta del bucle\n",
    "    pass\n",
    "\n",
    "print(xi)#Imprimimos la última observación para ver su formato\n",
    "print(yi)#Imprimimos la última etiqueta para ver su formato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab7b16",
   "metadata": {},
   "source": [
    "Dado que los datos ya están en memoria, esta no sería la aproximación ideal pero nos ayuda a entender el escenario. Tened en cuenta que en este caso  solo tenemos acceso a una sola muestra cada vez (dentro del bucle y representada por los valores *xi*, *yi*). Este bucle simula el flujo de datos (*data stream*), donde la función zip devuelve un iterador de Python. Esto no es demasiado diferente de la forma en que podríamos iterar a través de un archivo CSV, recibir un stream de datos de Kafka u obtener los resultados de una consulta SQL, fila por fila.\n",
    "\n",
    "Una cosa a tener en cuenta es el hecho de que en este ejemplo `xi` es una instancia de `numpy.array` (los datos se cargaron utilizando la librería de `scikit-learn`) pero, por su diseño, la librería  `River`, utiliza la clase `dict` como base para su trabajo trabajo. Cada observación viene representada por un diccionario y se asume que cada una de ellas equivale a una muestra del *stream* de datos.\n",
    "\n",
    "Parece extraño que `River`considere la clase `dict` apropiada para la gestión eficiente de los datos pero hay algunas consideraciones que vale la pena recordar y tener en cuenta:\n",
    "\n",
    "1. El  uso de `numpy` es interesante en entornos de computación de alto rendimiento. Recuerda que `dict` está implementado en Python, mientras que `numpy.array` está implementado a bajo nivel en C y Fortran. Una de las ventajas de usar `dict` es su facilidad de uso y que facilita la legibilidad del programa.\n",
    "\n",
    "2. El procesamiento en línea es diferente al procesamiento por lotes (*batch learning*), ya que **la vectorización no aporta ninguna aceleración en el proceso** (recordad que solo estamos procesando una observación de cada vez). Las librerías de procesamiento numérico están optimizadas para operaciones vectorizadas e introducen una sobrecarga considerable si solo se procesa una única muestra. En este tipo de escenarios, el uso de  `dict` no penaliza y puede simplificar el desarrollo y facilitar su entendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In numpy.array format:{xi}\\n\")\n",
    "print(f\"In dict format: {dict(zip(dataset.feature_names, xi))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45036aa1",
   "metadata": {},
   "source": [
    "Para facilitar la portabilidad de algoritmos y *pipelines* de trabajo  preexistentes, `River` proporciona una función *wrapper* que transforma los datasets de `scikit-learn` al formato requerido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5eb589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()):\n",
    "    pass\n",
    "\n",
    "print(xi)#Imprimimos la última observación para ver su formato\n",
    "print(yi)#Imprimimos la última etiqueta para ver su formato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08423c3",
   "metadata": {},
   "source": [
    "Si bien la mayoría de las operaciones se traducen con bastante facilidad entre el ML tradicional y el aprendizaje incremental, no todas se pueden hacer de forma directa. Vamos a ejemplificar esto con el problema del escalado de los datos, en concreto con la estandarización (escalar los datos para que tengan media 0 y varianza 1). Esta es una operación trivial sobre lotes (*batches*) de datos pero que en aprendizaje incremental requiere calcular estadísticas dinámicamente. Recordad que no se conocen todos los datos desde el inicio, por lo que desconocemos los valores de la media y la desviación estándar antes de procesarlos.\n",
    "\n",
    "Una primera aproximación podría ser realizar una primera pasada sobre todos los datos para calcular los valores necesarios y luego escalar los valores durante una segunda pasada. Sin embargo, esta solución es inconsistente con el objetivo del aprendizaje incremental: **procesar los datos una sola vez**.\n",
    "\n",
    "La solución  es calcular y usar estadísticas dinámicas donde no se usen  la media y la desviación estándar exactas, sino una estimación que se actualiza con cada nuevo valor que se procesa. Más formalmente, dado la media $\\mu_t$, y el conteo de observacionbes  $n_t$, ambos en el instante temporal $t$, la media móvil puede ser facilmente actualizada para cada muestra aplicando la siguiente función: \n",
    "\n",
    "\n",
    "$$\\large\n",
    "  n_{t+1} = n_t +1 \\\\   $$\n",
    "$$  \n",
    "  \\large\n",
    "  \\mu_{t+1} = \\mu_t +\\frac{x - \\mu_t}{n_{t+1}}    \n",
    "$$\n",
    "\n",
    "De la misma manera, el cálculo de la varianza ($\\sigma_t$) en el instante $t$ podría actualizarse empleando:\n",
    "$$\n",
    "\\large\n",
    "  s_{t+1} = s_t + (x-\\mu_t)\\times(x-\\mu_{t+1})\\\\\n",
    "  \\large\n",
    "  \\sigma_{t+1} = \\frac{s_{t+1}}{n_{t+1}}\n",
    "$$\n",
    "\n",
    " Estas fórmulas pueden ser fácilmente implementadas en Python. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8392c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n, mean, s, variance = 0, 0, 0, 0\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()):\n",
    "    n += 1\n",
    "    mean_t = mean\n",
    "    mean += (xi['mean radius'] - mean_t) / n\n",
    "    s += (xi['mean radius'] - mean_t) * (xi['mean radius'] - mean)\n",
    "    variance = s / n\n",
    "\n",
    "    print(f'Media dinámica: {mean:.3f} - Varianza dinámica: {variance:.3f}')\n",
    "    \n",
    "print(f'Media final: {mean:.3f} - Varianza final: {variance:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d540b4",
   "metadata": {},
   "source": [
    "Comparemos ahora el resultado con la implementación nativa de `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "i = list(dataset.feature_names).index('mean radius')\n",
    "print(f'True mean: {np.mean(X[:, i]):.3f}')\n",
    "print(f'True variance: {np.var(X[:, i]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3006eae",
   "metadata": {},
   "source": [
    "Como era de esperar, los resultados finales  son idénticos, con una diferencia clave: la implementación de `numpy` requiere que todos los datos estén disponibles para el cálculo, mientras que en el caso incremental, se calculan progresivamente. Por lo tanto, debemos ser conscientes de que los resultados con solo unas pocas observaciones no son precisos pero a medida que se procesan observaciones, los resultados se aproximarán a los finales.\n",
    "\n",
    "Aunque la mayoría de las medidas estadísticas  se podrían desarrollar fácilmente en Python de forma dinámica, la realidad es que no es necesario, ya que `River` proporciona la mayoría de ellas en el módulo `stats`. Por ejemplo, para crear tanto la media móvil como la varianza móvil emplearíamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de814e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stats\n",
    "\n",
    "r_mean=stats.Mean()\n",
    "r_variance=stats.Var()\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()): \n",
    "    r_mean.update(xi['mean radius'])\n",
    "    r_variance.update(xi['mean radius'])\n",
    "    print(f'Running mean: {r_mean.get():.3f} - Running variance: {r_variance.get():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c583162-8197-4f8e-8864-cf51dea284a1",
   "metadata": {},
   "source": [
    "#### EJERCICIO 1 PARA ENTREGAR EN EL AULA VIRTUAL\n",
    "\n",
    "Siguiendo el ejemplo anterior, deberéis calcular la mediana, el máximo y el mínimo de la variable *mean_radius*. \n",
    "\n",
    "- Calcula la media, máximo y mínimo empleando todos los datos (forma tradicional).\n",
    "- Calcula la media, máximo y mínimo de forma dinámica con River."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e603d9e-2615-4045-a6a0-a22a42fd351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESPUESTA EJ1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae26e6f",
   "metadata": {},
   "source": [
    "Una vez que sabemos cómo calcular las diferentes estadísticas móviles sobre los datos, el siguiente paso es usarlas para estandarizar las observaciones del flujo de datos. `River` dispone del módulo [preprocessing](https://riverml.xyz/latest/api/preprocessing/AdaptiveStandardScaler/) con varias funciones que permiten aplicar operaciones de preprocesado directamente a las observaciones sin tener que hacer los cálculos *a mano*. Por ejemplo, para estandarizar todas las características del ejemplo anterior, podríamos realizar:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f05374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()):\n",
    "    scaler.learn_one(xi)#actualiza las estadísticas dinámicamente con los valores de la nueva observación\n",
    "    xi_scaled=scaler.transform_one(xi)#estandariza el valor\n",
    "    print(f\"xi:{xi}\")\n",
    "    print(f\"scaled xi {xi_scaled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433c5bf-36f7-4f5e-9f46-b38decc17935",
   "metadata": {},
   "source": [
    "#### EJERCICIO 2 PARA ENTREGAR EN EL AULA VIRTUAL\n",
    "\n",
    "Sabemos que otra de las maneras tradicionales de escalar los datos es con la normalización entre 2 valores. Lamentablemente `River`solo proprorciona la normalización entre 0 y 1.   \n",
    "\n",
    "- Implementa una solución que te permita normalizar dinámicamente los datos de *mean radius* entre dos valores cualquiera. El cálculo dinámico de Max. y Min. lo podéis realizar a través de las funciones propias de River en el módulo `stats`.\n",
    "- Implementa directamente la normalización entre 0 y 1 de la variable *mean radius* con la función de preprocesado de `River`del módulo `preprocessing`.\n",
    "\n",
    "| $\\Huge X^´=a+\\frac{(X-X_{min})(b-a)}{X_{max}-X_{min}}$| \n",
    "|:--:| \n",
    "| Fórmula para normalizar los datos entre los valores [a,b]|\n",
    "\n",
    "\n",
    "**Nota**: Deberéis tener en cuenta la posibilidad de la división por cero debido al cálculo dinámico. En ese caso se devolverá cero.\n",
    "**Nota 2**: Podéis comprobar vuestro algoritmo validando que los datos generados en el intervalo [0-1] son los mismos que con la función de preprocesado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f4e26-2696-4814-8e34-98eeac2fdffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##RESPUESTA EJ2\n",
    "#A\n",
    "\n",
    "#B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ce3c5-3f3f-4e27-8a66-b0115b3ebb2a",
   "metadata": {},
   "source": [
    "### 2.1. Aplicando Aprendizaje Automático Incremental\n",
    "\n",
    "Ahora que ya tenemos los datos escalados, podemos aplicar un algoritmo de aprendizaje automático. En el siguiente ejemplo usamos una regresión logística con un descenso de gradiente estocástico (SGD) como modelo de ML. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4c06d-e17a-4bdf-a8f0-7ef19ecd942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.linear_model import LogisticRegression\n",
    "from river.optim import SGD\n",
    "\n",
    "scaler = StandardScaler()#operación de preprocesado\n",
    "optimizer = SGD(lr=0.01)#optimizador empleado\n",
    "log_reg = LogisticRegression(optimizer)#modelo empleado\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer(), shuffle=True, seed=42):\n",
    "\n",
    "    # Escalado de las característcias\n",
    "    scaler.learn_one(xi)\n",
    "    xi_scaled =scaler.transform_one(xi)\n",
    "    # Testeo del modelo actual empleando una observación nunca vista\n",
    "    yi_pred = log_reg.predict_proba_one(xi_scaled)\n",
    "    # Entrenamiento del modelo con la observación\n",
    "    log_reg.learn_one(xi_scaled, yi)\n",
    "\n",
    "    # Almacenamos el valor real y la predicción\n",
    "    y_true.append(yi)\n",
    "    y_pred.append(yi_pred[True])\n",
    "\n",
    "print(f'ROC AUC: {roc_auc_score(y_true, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c70ac1-ba33-4c1d-8faa-631e49e58523",
   "metadata": {},
   "source": [
    "Los resultados parecen ser ligeramente mejores que los obtenidos con `scikit-learn` pero debemos hacer una comparación más adecuada. Para hacer una comparativa más justa, ambos modelos deberían usar la misma configuración para la validación cruzada (CV). Aunque podríamos definir el mismo proceso con bastante facilidad, los dos procesos podrían hacerse completamente comparables utilizando un módulo integrado de `River` llamado `compat`. Este módulo es un *wrapper* que facilita la compatibilidad con otras librerías. En este caso podemos emplear  la función [convert_river_to_sklearn](https://riverml.xyz/latest/api/compat/convert-river-to-sklearn/) para generar un objeto perfectamente compatible con las funciones de `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad2ab6-b6c3-4c77-91ab-881618885867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.compose import Pipeline\n",
    "from river.compat import convert_river_to_sklearn\n",
    "\n",
    "\n",
    "# Definimos el pipeline de River\n",
    "model = Pipeline(\n",
    "    ('scale', StandardScaler()),\n",
    "    ('ml_model', LogisticRegression())\n",
    ")\n",
    "\n",
    "# Esta función devuelve un objeto del tipo  River2SKLRegressor \n",
    "# que es compatible con la interfaz de  sklearn\n",
    "model = convert_river_to_sklearn(model)\n",
    "\n",
    "# Ahora ya podemos emplear cross_val_score de sklearn con el modelo de River \n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Comparemos los resultados\n",
    "print(f'ROC AUC: {scores.mean():.4f} (± {scores.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc54aad-25de-4f71-ad52-876bb7f4055a",
   "metadata": {},
   "source": [
    "Aunque las métricas son más bajas que en la prueba anterior, los resultados en este caso si son comparables a la aproximación tradicional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b874984b-a119-4f73-ad99-49bf706e0e89",
   "metadata": {},
   "source": [
    "## 3. *Pipelines* en River\n",
    "\n",
    "El \"flujo\" de operaciones en el desarrollo de modelos  a menudo se puede encapsular como una secuencia de pasos o *pipeline*. Para realizarlo, muchas librerías como, por ejemplo, `scikit-learn` o `pandas` proporcionan objetos que permiten implementar este patrón \"declarativo\". En el caso de `River`,  existen un conjunto especial de módulos que permiten implementar estos *pipelines*.\n",
    "\n",
    "En la práctica, muchos  desarrolladores no suelen utilizar *pipelines* para representar los flujos de trabajo debido a que provienen del mundo del ML tradicional (orientado a \"lotes\"), donde lo normal es desarrollar modelos a través de la programación procedimental pero, sin embargo, en el aprendizaje incremental, los *pipelines* representan una forma más natural de trabajar (la evolución del flujo de información).\n",
    "\n",
    "En el siguiente ejemplo, comparamos tanto el método procedimental como el declarativo (a través de *pipelines*) empleando un dataset típico en competiciones de [Kaggle](https://www.kaggle.com/) que está presente en `River`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17438e91-60bf-4691-8deb-2fcfd9c47de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from river.datasets import Restaurants\n",
    "\n",
    "data = Restaurants()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b48bc9-0d4b-4b72-9541-9711fe84ef5f",
   "metadata": {},
   "source": [
    "Veamos la estructura de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6194f2-5f8e-445a-a7fe-5efc18503ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c61275c-2e34-4386-a70c-6fde3fab2a05",
   "metadata": {},
   "source": [
    "### 3.1. Aproximación tradicional procedimental\n",
    "Veamos primero la aproximación clásica:\n",
    "\n",
    "Nota: el paquete `feature_extraction` tiene funciones para extraer características y [TargetAgg](https://riverml.xyz/latest/api/feature-extraction/TargetAgg/) aplica y calcula un agregado sobre la variable objetivo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb5e59-74eb-42b3-acea-ceac29b9d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import feature_extraction, linear_model, metrics, preprocessing, stats, utils\n",
    "\n",
    "# Generación de nuevas variables\n",
    "#Vamos a crear el promedio de visitantes los últimos 7, 14 y 21 días \n",
    "features = (\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\"),\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\"),\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\")\n",
    ")\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "model = linear_model.LinearRegression()\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for x, y in data:\n",
    "\n",
    "    # Ingeniería de variables. Generamos nuevas características desde la fecha\n",
    "    x['weekday'] = x['date'].weekday() #weekday es una función de datetime.date que devuelve el número asociado con el día de la semana (lunes=0,..., domingo=6)\n",
    "    x['is_weekend'] = x['date'].weekday() in (5, 6)\n",
    "    \n",
    "    # Procesa la media móvil del target  \n",
    "    for mean_f in features:\n",
    "    \n",
    "        x = {**x, **mean_f.transform_one(x)}#** unzip un diccionario. De esta forma permite concatenar 2 diccionarios y añadir las nuevas caracteristicas  \n",
    "        mean_f.learn_one(x, y)#Primero transformamos y luego aprendemos porque para la observación actual queremos predecir 'y', por lo que ese valor no puede ser parte de las medias\n",
    "   \n",
    "    # Eliminar las variables que no queremos emplear como características\n",
    "    for key in ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']:\n",
    "        x.pop(key)\n",
    "      \n",
    "    # Escalado de los datos\n",
    "    # Este ejemplo tiene solo un propósito docente\n",
    "    # En un ejemplo real NO necesitamos escalar características como is_holiday o is_weekend\n",
    "    scaler.learn_one(x)\n",
    "    x=scaler.transform_one(x)\n",
    "\n",
    "    # Predicción y Entrenamiento\n",
    "    y_pred = model.predict_one(x)#Importante el orden! Primero se predice y luego se aprende de esa observación.\n",
    "    model.learn_one(x, y)#cambiar el orden lleva a contaminar los resultados\n",
    "\n",
    "    # Actualización de las métricas\n",
    "    #empleando la predicción generada antes de aprender y la etiqueta real\n",
    "    metric.update(y, y_pred) \n",
    "\n",
    "print(metric)\n",
    "\n",
    "#Comprobemos la última observación\n",
    "#recordad que los datos están estandarizados\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e58607-26c5-4e62-a9b8-e6f8bdb3d2e4",
   "metadata": {},
   "source": [
    "### 3.2. Aproximación declarativa empleando *pipelines* de River\n",
    "\n",
    "Reescribamos ahora el código anterior empleando una aproximación declarativa a través de los *pipelines* de `River`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f4963-8fcf-42cd-8723-06e992157c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "\n",
    "# Función para generar nuevas variables\n",
    "def get_date_features(x):\n",
    "    weekday =  x['date'].weekday()\n",
    "    return {'weekday': weekday, 'is_weekend': weekday in (5, 6)}\n",
    "\n",
    "# Construcción del pipeline con los mismos pasos\n",
    "pipeline_model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(),7),target_name=\"last_7_mean\")),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(),14), target_name=\"last_14_mean\")),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(),21), target_name=\"last_21_mean\"))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for x, y in data:\n",
    "\n",
    "    # Genera la predicción\n",
    "    y_pred = pipeline_model.predict_one(x)\n",
    "\n",
    "    # Actualiza el modelo una vez recuperada la salida real\n",
    "    pipeline_model.learn_one(x, y)\n",
    "\n",
    "    # Actualiza las métricas con la predicción y la salida real\n",
    "    metric.update(y, y_pred)\n",
    "\n",
    "print(x)\n",
    "print(metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3dbba-6a6a-48f3-9a9b-c169c52bee38",
   "metadata": {},
   "source": [
    "Cada uno de los pasos del *pipeline* se define en River como una tupla <code>(name, estimator)</code> (similar a `sckit-learn`). En este ejemplo tenemos diferentes pasos para crear variables nuevas, para estandarizarlas y para generar un modelo.\n",
    "\n",
    "Como podrás observar, todos los cálculos y transformaciones para generar nuevas variables se han organizado en el objeto [TransformerUnion](https://riverml.xyz/latest/api/compose/TransformerUnion/), que nos permite agrupar diferentes operaciones de procesamiento en un solo \"transformador\" facilitando el desarrollo de un *pipeline* . El objeto `TransformerUnion` incorpora  una lista de \"transformadores\" y los aplica de forma secuencial a los datos de entrada.\n",
    "\n",
    "La eliminación de variables no necesarias se realiza a través del transformador [Discard](https://riverml.xyz/latest/api/compose/Discard/)\n",
    "La operación de escalado y la integración del modelo ya se había visto anteriormente. Fijaos como el modelo se considera simplemente un paso más en el *pipeline*. \n",
    "\n",
    "\n",
    "\n",
    "El bucle `for`que gestiona la secuencia predecir-aprender-actualizar es tan común que `River` proporciona una función que integra todas las operaciones de forma automática, tal y como se observa en el siguiente código:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ad776-9686-4466-8d8b-db9fe22dd3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import evaluate\n",
    "\n",
    "pipeline_model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\" )),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\")),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\"))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=pipeline_model, metric=metrics.MAE()) #Reemplaza el bucle \"for\". Es equivalente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8a3ae-07a0-4712-a07b-0a68ac7fe6de",
   "metadata": {},
   "source": [
    "A pesar de que el código es correcto, algunas simplificaciones adicionales pueden ser integradas. Por ejemplo, al igual que sucede en `scikit-learn`, los nombres de los *steps* no son obligatorios (se asigna uno por defecto, si no se añade, basándose en el orden de la operación proporcionada). Veamos cómo quedaría esta simplificación:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacb8d8-96f3-4227-b7b1-5507536bef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = compose.Pipeline(\n",
    "    compose.TransformerUnion(\n",
    "        compose.FuncTransformer(get_date_features),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\"),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\"),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\")\n",
    "    ),\n",
    "    compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude'),\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=pipeline_model, metric=metrics.MAE())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746eda68-b6f5-4a9a-8134-8250945a93d9",
   "metadata": {},
   "source": [
    "La siguiente simplificación proviene de la posibilidad de declarar el *pipeline* empleando operaciones matemáticas. Primero, usa \"+\" para crear el objeto `TransformerUnion` y asignarle operaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69a8ce-3842-47f8-90cd-614e935e0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = compose.Pipeline(\n",
    "    compose.FuncTransformer(get_date_features) + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\") + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\") + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\"),\n",
    "\n",
    "    compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude'),\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=pipeline_model, metric=metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7b9e2-fd49-4234-b8a4-87484ef27400",
   "metadata": {},
   "source": [
    "A continuación, emplea el operador \"|\" (tubería) para unir *steps* dentro del *pipeline*. Al igual que en `bash`, este operador permite que la salida de un *step* sea la entrada del siguiente, lo que permite generar un flujo de operaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3dde0-4844-4275-b802-9df1377f3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = (\n",
    "    compose.FuncTransformer(get_date_features) +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\") +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\") +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\")\n",
    ")\n",
    "\n",
    "to_discard = ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']\n",
    "\n",
    "pipeline_model = pipeline_model | compose.Discard(*to_discard)#* unzip el objeto y emplea los elementos individuales como argumentos de la función\n",
    "pipeline_model |= preprocessing.StandardScaler()\n",
    "pipeline_model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=pipeline_model, metric=metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95cfaca-d530-4608-a4df-dfef0f846709",
   "metadata": {},
   "source": [
    "Una simplificación final proviene del hecho de que `River` encapsula automáticamente las funciones en un objeto `FuncTransform` (no es necesario declararlo expresamente!), por lo que el ejemplo final podría ser algo como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d28c07-019a-4a30-aec1-cfa71e2f3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n),target_name=\"last_\"+str(n)+\"_mean\")\n",
    "\n",
    "pipeline_model |= compose.Discard(*to_discard)#* unzip el objeto y emplea los elementos individuales como argumentos de la función\n",
    "pipeline_model |= preprocessing.StandardScaler()\n",
    "pipeline_model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=pipeline_model, metric=metrics.MAE(), print_every=20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787bdd4-41b1-44db-b151-80a0185d93e9",
   "metadata": {},
   "source": [
    "Se ha incluido incluido un argumento adicional en `progressive_val_score` para imprimir cómo cambia la evaluación a lo largo del tiempo (`print_every=20_000`) . El uso de un enfoque procedimental o declarativo no afecta el rendimiento general, sino más bien la forma en que pensamos sobre el modelo y, obviamente, al código final resultante. De hecho, ambos modelos, ya sean procedimentales o declarativos, producirán los mismos resultados y rendimiento. Como punto final sobre los *pipelines*, cabe mencionar que podemos explorar gráficamente el pipeline consultanto el objeto `modelo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e8234-139d-4b81-9670-fa7742dacfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00823a-9151-4040-9917-e95878f7e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a89a70-b0b8-44d4-910a-2cc8848c87da",
   "metadata": {},
   "source": [
    "Por último, para depurar el comportamiento de los diferentes *steps*, se puede usar la función `debug_one`. Imaginad que entrenamos el modelo con los primeros 120,000 ejemplos y queremos saber qué sucede con el siguiente. El siguiente fragmento de código muestra cómo usar la función `debug_one` en este caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fc510-97f8-4f99-a104-99ccf1ad1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "pipeline_model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    pipeline_model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n),target_name=\"last_\"+str(n)+\"_mean\")\n",
    "\n",
    "\n",
    "pipeline_model |= compose.Discard(*to_discard)#* unzip el objeto y emplea los elementos individuales como argumentos de la función\n",
    "pipeline_model |= preprocessing.StandardScaler()\n",
    "pipeline_model |= linear_model.LinearRegression()\n",
    "\n",
    "for x, y in itertools.islice(data, 120_000):\n",
    "    y_pred = pipeline_model.predict_one(x)\n",
    "    pipeline_model.learn_one(x, y)\n",
    "\n",
    "\n",
    "print(pipeline_model.debug_one(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e71a7-d414-4609-a577-71227ff1a5cb",
   "metadata": {},
   "source": [
    "#### EJERCICIO 3 PARA ENTREGAR EN EL AULA VIRTUAL\n",
    "El uso de los *pipelines* requiere práctica e incluso con experiencia en ocasiones puede ser complicado emplearlos.\n",
    "A modo de práctica vamos a emplear el dataset de River `Bikes` con el objetivo de generar un *pipeline* que haga lo siguiente:\n",
    "1. Crea las variables: *dia* y *hora* analizando el atributo *moment*.\n",
    "2. Genera una media móvil sobre la variable objetivo respecto al día, hora y la estación (media móvil de bicis en un día y hora concreta para una estación en particular).\n",
    "3. Descarta las variables: 'station','moment','description', 'dia' y 'hora'.\n",
    "5. Estandariza los datos.\n",
    "6. Emplea un *linear_model*  como modelo de ML.\n",
    "7. Usa el MAE para valorar el modelo.\n",
    "\n",
    "\n",
    "El objetivo para este dataset es predecir el número de bicicletas en cinco estaciones de la ciudad de Toulouse.\n",
    "\n",
    "**Nota**: se puede hacer una media móvil sobre más de una característica empleando una lista. Mirad el API.\n",
    "\n",
    "**Nota 2**: El objetivo no es crear un buen modelo, el objetivo es practicar con los *pipelines*.\n",
    "\n",
    "Tened en cuenta que:\n",
    "* Emplear el símbolo \"+\" genera un `TransformerUnion`. Esto es una lista de transformadores que **trabajan en paralelo** sobre los inputs, es decir, cada transformador tiene de entrada los mismos datos. La salida del `TransformerUnion` es la unión de las salidas de los transformadores\n",
    "* Emplear el símbolo \"|\" modifica el flujo de datos, es decir, si recibimos un flujo y ejecutamos una operación que solo devuelve un valor, todo el flujo se queda reducido a un solo valor\n",
    "* Los símbolos pueden agruparse e intercalarse. Ej. pipeline|=(t1+t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754376db-8d3f-42b9-92f2-87698afaafa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## RESULTADO EJERCICIO 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a15e5-e1b7-4d1a-a635-1a6cd784e6a4",
   "metadata": {},
   "source": [
    "## 4. Aprendizaje incremental con árboles de decisión\n",
    "\n",
    "Hasta ahora nos hemos centrado en técnicas de Aprendizaje Automático que pueden implementarse de manera directa en incremental, como la regresión lineal o la logística pero  existen otras que también pueden adaptarse, aunque requieren de un mayor esfuerzo. \n",
    "\n",
    "Uno de los ejemplos más interesantes son los árboles de decisión y sus variantes. Tradicionalemente, estos métodos han demostrado ser particularmente útiles y eficaces cuando se combinan con técnicas de ensamblado (bagging, boosting, etc.). Las ventajas comúnmente citadas de los árboles de decisión incluyen su flexibilidad, versatilidad con respecto a los datos, simplicidad de implementación e interpretación. \n",
    "No obstante, los algoritmos tradicionales de árboles  requieren varias iteraciones sobre los datos, lo que los hace inadecuados para el aprendizaje incremental (solo podemos ver y procesar las observaciones una vez). Debido a esto, han surgido varios algoritmos que abordan este problema tratando de adaptar los árboles al aprendizaje incremental.\n",
    "Una de las adaptaciones más populares es la de los Árboles de Hoeffding (*Hoeffding Trees*, HT) debido a sus propiedades similares a las de los árboles generados por enfoques tradicionaes (C4.5/J48, CART, M5, etc.). Los HT destacan por:\n",
    "\n",
    "- Solo requieren un único paso sobre los datos (los datos se ven y procesan una sola vez).\n",
    "- Garantizan (teóricamente) la convergencia con su equivalente procesado por lotes, siempre que haya suficientes observaciones y una distribución de datos estacionaria.\n",
    "- Pueden operar con recursos limitados tanto de memoria como de tiempo de cómputo.\n",
    "- Algunas de sus variantes pueden manejar distribuciones no estacionarias (*Adaptive Hoeffding Trees*).\n",
    "\n",
    "De hecho, con el reciente auge de la Inteligencia Artificial Explicable (XAI), estas técnicas de árboles han cobrado una importancia especial, por su mejor interpretación respecto a otros modelos.\n",
    "\n",
    "Para explorar el uso de estos algoritmos de árboles, estudiaremos un ejemplo sencillo donde su aplicación sea representativa. Para ello, utilizaremos el mismo conjunto de datos de restaurantes descargado previamente, pero ahora analizado con un único árbol.\n",
    "\n",
    "**Nota**: fijaos que el preprocesado sigue siendo el mismo en el *pipeline*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184c8fb-3570-442d-a989-1ade3289641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from river import evaluate, metrics, stats, utils, feature_extraction, preprocessing, compose\n",
    "from river.tree import HoeffdingTreeRegressor\n",
    "import itertools\n",
    "\n",
    "data = Restaurants()\n",
    "to_discard = ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']\n",
    "def get_date_features(x):\n",
    "    weekday =  x['date'].weekday()\n",
    "    return {'weekday': weekday, 'is_weekend': weekday in (5, 6)}\n",
    "\n",
    "pipeline_model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    pipeline_model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n),target_name=\"last_\"+str(n)+\"_mean\")\n",
    "\n",
    "pipeline_model |= compose.Discard(*to_discard)\n",
    "pipeline_model |= preprocessing.StandardScaler()\n",
    "pipeline_model |= HoeffdingTreeRegressor(grace_period=250) #grace_period: número de instancias que debe observar antes de intentar dividir una rama\n",
    "\n",
    "for x, y in data:\n",
    "    pipeline_model.learn_one(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff0df5-bc25-4a00-b7cc-965d7c7f921f",
   "metadata": {},
   "source": [
    "\n",
    "Una vez desarrollado el modelo, podemos analizarlo desde diferentes puntos de vista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099bc666-92c1-4613-b400-cfb58050863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muestra la estructura\n",
    "pipeline_model['HoeffdingTreeRegressor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c535d009-7942-42d8-b390-4aa628da67b9",
   "metadata": {},
   "source": [
    "Esto nos da una idea de los parámetros internos del objeto pero también podemos solicitar un resumen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c56da-f58a-4476-b793-d777a0ddeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resumen del árbol\n",
    "pipeline_model['HoeffdingTreeRegressor'].summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26afc729-8140-4ab3-8929-2ad2ecdc4023",
   "metadata": {},
   "source": [
    "O lo podemos transformar en un `DataFrame` de `Pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec17f6a-b8d6-4344-b367-a4cac01a9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas data frame \n",
    "(pipeline_model['HoeffdingTreeRegressor']).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12473c8b-94bb-4c4b-87e3-92abd0b3d318",
   "metadata": {},
   "source": [
    "Este último comando podría ser interesante en un enfoque de clasificación debido a que nos proporciona la capacidad de analizar la decisión interna del modelo y verificar los umbrales y pesos asignados pero, sin embargo, en un modelo de regresión no es tan útil. Veamos un ejemplo en el contexto de la clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b314351-62a4-48e5-836d-599051fd218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import evaluate, metrics, stats, utils, feature_extraction, preprocessing, compose\n",
    "from river.tree import HoeffdingTreeClassifier\n",
    "from river.datasets import Phishing #\n",
    "import itertools\n",
    "\n",
    "data = Phishing()\n",
    "\n",
    "model = HoeffdingTreeClassifier(grace_period=50)\n",
    "\n",
    "for x, y in data:\n",
    "    model.learn_one(x, y)\n",
    "    \n",
    "    \n",
    "#resumen del modelo\n",
    "print(model.summary)\n",
    "\n",
    "#convierte el modelo en un DataFrame\n",
    "model.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9ac40-2f16-4587-8d23-f3ddb503fe75",
   "metadata": {},
   "source": [
    "Otra opción interesante cara a analizar e interpretar el modelo es dibujarlo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48517e-6b46-4dfd-9ad7-9947fc6f1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Es necesario tener la librería graphviz instalada.\n",
    "model.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff9657-f7fa-40f7-a9b4-d4370814c534",
   "metadata": {},
   "source": [
    "`River` tiene muchas implementaciones basadas en árboles y, tal y como se describe en su [web](https://riverml.xyz/latest/recipes/on-hoeffding-trees/#1-trees-trees-everywhere-gardening-101-with-river), estas podían organizarse según la siguiente tabla: \n",
    "\n",
    "| Name | Acronym | Task | Non-stationary data? | Comments | Source |\n",
    "| :- | :-: | :- | :-: | :- | :-: |\n",
    "| Hoeffding Tree Classifier | HTC | Classification | No | Basic HT for classification tasks | [[1]](https://dl.acm.org/doi/pdf/10.1145/347090.347107)|\n",
    "| Hoeffding Adaptive Tree Classifier | HATC | Classification | Yes | Modifies HTC by adding an instance of ADWIN to each node to detect and react to drift detection | [[2]](https://link.springer.com/chapter/10.1007/978-3-642-03915-7_22)|\n",
    "| Extremely Fast Decision Tree Classifier | EFDT | Classification | No | Deploys split decisions as soon as possible and periodically revisit decisions and redo them if necessary. Not as fast in practice as the name implies, but it tends to converge faster than HTC to the model generated by a batch DT | [[3]](https://dl.acm.org/doi/abs/10.1145/3219819.3220005)|\n",
    "| Hoeffding Tree Regressor | HTR | Regression | No | Basic HT for regression tasks. It is an adaptation of the [FIRT/FIMT](https://link.springer.com/article/10.1007/s10618-010-0201-y) algorithm that bears some semblance to HTC | [[4]](https://link.springer.com/article/10.1007/s10618-010-0201-y)|\n",
    "| Hoeffding Adaptive Tree Regressor | HATR | Regression | Yes | Modifies HTR by adding an instance of ADWIN to each node to detect and react to drift detection |-|\n",
    "| incremental Structured-Output Prediction Tree Regressor| iSOUPT | Multi-target regression | No | Multi-target version of HTR | [[5]](https://link.springer.com/article/10.1007/s10844-017-0462-7)|\n",
    "| Label Combination Hoeffding Tree Classifier | LCHTC | Multi-label classification | No | Creates a numerical code for each combination of the binary labels and uses HTC to learn from this encoded representation. At prediction time, decodes the modified representation to obtain the original label set |-| \n",
    "\n",
    "\n",
    "Como podéis ver, cada variante de árbol tiene un objetivo específico, aunque a veces puedan solaparse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
