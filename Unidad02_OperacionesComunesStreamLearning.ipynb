{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde64848-c46c-473a-8d70-8a215f907131",
   "metadata": {},
   "source": [
    "![MIoT_GDPI](img/MIOT_GDPI_header.png)\n",
    "\n",
    "# Unidad 02 - Operaciones comunes en Stream Learning\n",
    "\n",
    "El objetivo principal de esta práctica es que os familiaricéis con las operaciones básicas para trabajar en *online learning* y como integrarlas en un *pipeline* de [River](https://riverml.xyz/). Podréis comprobar que su uso es similar al de `scikit-learn` que ya conocéis.\n",
    "\n",
    "La mayor parte del contenido de este Notebook se dedica a explicar el uso de los comandos y de su integración en un *pipeline* apoyándose en ejemplos concretos que ilustran su aplicación a un problema real. Es crucial que dediquéis tiempo a leer y comprender el material, en lugar de simplemente ejecutar el código. Os invitamos a experimentar modificando y variando el código proporcionado para que podáis explorar las distintas opciones y profundizar en su funcionamiento.\n",
    "\n",
    "\n",
    "\n",
    "**importante**: El Notebook contiene varios ejercicios sencillos. Debéis desarrollarlos durante la clase y enviarlos por el aula virtual del curso, en la tarea correspondiente.\n",
    "\n",
    "\n",
    "\n",
    "## Referencias útiles para la práctica\n",
    "1. API Pandas: [https://pandas.pydata.org/docs/reference/index.html](https://pandas.pydata.org/docs/reference/index.html)\n",
    "2. Api River: [https://riverml.xyz](https://riverml.xyz)\n",
    "3. Api Scikit-Learn: [https://scikit-learn.org/stable/api/index.html](https://scikit-learn.org/stable/api/index.html)\n",
    "4. Bahri, M., Bifet, A., Gama, J., Gomes, H. M., & Maniu, S. (2021). [Data stream analysis: Foundations, major tasks and tools](https://doi.org/10.1002/widm.1405). Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 11(3), e1405.\n",
    "5. Gomes, H. M., Read, J., Bifet, A., Barddal, J. P., & Gama, J. (2019). [Machine learning for streaming data: state of the art, challenges, and opportunities](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://kdd.org/exploration_files/3._CR_7._Machine_learning_for_streaming_data_state_of_the_art-Final.pdf). ACM SIGKDD Explorations Newsletter, 21(2), 6-22.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d32499-9479-4eb3-bfca-eea5f0ee6b7c",
   "metadata": {},
   "source": [
    "## Del Batch ML tradicional al Online ML\n",
    "\n",
    "Este tutorial está inspirado en los [ejemplos](https://riverml.xyz/latest/recipes/reading-data/) proporcionados por River en su [página web](https://riverml.xyz/latest/), por  tanto, os animamos a consultar la documentación original para una exploración más profunda de cada uno de los temas particulares que se muestren a continuación.\n",
    "\n",
    "En general, casi cualquier enfoque asociado al aprendizaje automático contiene los siguientes pasos:\n",
    "\n",
    "1. Entender y definir el problema y su contexto.\n",
    "2. Obtener los datos.\n",
    "3. Explorar, analizar y entender los datos.\n",
    "4. Preprocesar los datos.\n",
    "5. Seleccionar, optimizar y entrenar los modelos ML.\n",
    "6. Evaluar y el modelo seleccionado.\n",
    "7. Desplegar, monitorizar y mantener la solución.\n",
    "\n",
    "Antes de comenzar, vamos a examinar un ejemplo clásico que utiliza todos estos pasos utilizando la librería `scikit-learn:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028cc4e-86ba-41fb-ab0f-20cb17c700d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from rich import print\n",
    "\n",
    "\n",
    "# Carga de datos\n",
    "dataset = load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# Preparación del pipeline con los pasos de preprocesado y el modelo seleccionado\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),                       # Estandarización de los datos avg 0 y std 1\n",
    "    ('extractor', PCA(0.95)),                          # Reducción de la dimensionalidad con PCA (0.95 of the varianza)\n",
    "    ('classifier', LogisticRegression(solver='lbfgs')) # Selección y configuración del modelo\n",
    "])\n",
    "\n",
    "# Definición de una validación cruzada estableciendo una semilla\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluación del modelo para cada fold de la validación cruzada\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "scores = cross_val_score(pipe, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Imprime la media de la métrica y su desciación estándard\n",
    "print(f'ROC AUC: {scores.mean():.4f} (± {scores.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881aa86b-be0c-418b-bb7e-f1b9e0c2ddfc",
   "metadata": {},
   "source": [
    "¿Cuál es el problema con esta aproximación? En realidad, no tiene ningún problema, pero existen algunas desventajas potenciales, particularmente con respecto al tamaño del conjunto de los datos, la aparición de datos nuevos y la heterogeneidad de las características a lo largo del tiempo.\n",
    "\n",
    "* **Tamaño del conjunto de datos**: imagina que los datos para el entrenamiento fueran demasiado grandes para caber en la memoria de tu ordenador. En ese caso, el programa, probablemente,  se habría bloqueado. Aunque se pueden aplicar diferentes técnicas para minimizar este problema,  existe un límite para estas optimizaciones. De hecho, si el dataset constara de millones de observaciones (ej.  cientos de gigabytes), podría haberse requerido hardware especial solo para cargar el dataset.\n",
    "\n",
    "* **Datos nuevos**: Otro problema potencial es la incorporación de  datos  nuevos al modelo. Los enfoques tradicionales requieren comenzar a entrenar desde cero con un nuevo conjunto de datos resultante de la combinación de los datos antiguos con las nuevas observaciones disponibles. Esto es particularmente problemático en aplicaciones en tiempo real donde tienes  datos nuevos disponibles cada poco. En muchas aplicaciones reales, la solución es desarrollar un pipeline de integración continua que pueda generar y desplegar un nuevo modelo cada poco. es importante destacar que por datos nuevos nos referimos a observaciones que son radicalmente diferentes de las empleadas para entrenar, dónde las relaciones entre las entradas (*inputs*) y salidas (*outputs*) ha cambiado de forma que el modelo no puede gestionarlas.\n",
    "\n",
    "* **Heterogeneidad de características**: Finalmente, otra desventaja con el enfoque tradicional es que las características con las que se ha entrenado deberán estar disponibles siempre durante el tiempo de producción. Además, no pueden incorporarse nuevas características a lo largo del tiempo. En escenarios de plantas industriales es habitual que determinadas características no estén disponibles en un momento determinado (sensores en estropeados o con mal funcionamiento) para luego estar nuevamente disponibles. En estos mismos escenarios, nuevos sensores pueden desplegarse y empezar a medir y generar nuevas características. Desde un punto de vista tradicional, la única forma de gestionar un número cambiante de características es reentrenando desde cero.\n",
    "\n",
    "\n",
    "## 2. *Incremental Learning*\n",
    "\n",
    "Como se mencionó en la unidad anterior, el aprendizaje incremental (*incremental learning*) también se conoce como aprendizaje en línea (*online learning*) o aprendizaje de flujo (*stream learning*). Es habitual que se sustituya el término *online* por incremental o *stream* debido a lo confuso del concepto de \"aprendizaje en línea\", que, generalmente,  se refiere a una opción educativa (esto es bastante obvio si buscas en Google \"aprendizaje en línea\"). \n",
    "\n",
    "El concepto de *incremental learning* se refiere a ajustar un modelo ML a partir de las observaciones obtenidas a través de un flujo (*stream*) de datos en tiempo real. Expresado de otra manera, los datos no están disponibles en su totalidad como en el ML tradicional (dataset de entrenamiento), sino que las observaciones se proporcionan una por una a medida que se leen de un flujo de datos. Por ejemplo, imagina el caso anterior que, en lugar de disponer del conjunto de datos completo para entrenar, tenemos un punto de referencia temporal que proporciona una observación a la vez. Esto se puede simular con un simple bucle: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc1e04-015d-45b7-b6ba-45adfca3c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xi, yi in zip(X, y):#simula obtener una observación por cada vuelta del bucle\n",
    "    pass\n",
    "\n",
    "print(xi)#Imprimimos la última observación para ver su formato\n",
    "print(yi)#Imprimimos la última etiqueta para ver su formato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab7b16",
   "metadata": {},
   "source": [
    "Dado que los datos ya están en memoria, este no es el escenario ideal pero, sin embargo, tened en cuenta que en este caso particular solo tenemos acceso a una sola muestra cada vez (dentro del bucle y representada por los valores xi, yi). Este bucle simula el flujo de datos (*data stream*), donde la función zip devuelve un iterador de Python. Esto no es demasiado diferente de la forma en que podríamos iterar a través de un archivo CSV, recibir un stream de datos de Kafka u obtener los resultados de una consulta SQL, fila por fila.\n",
    "\n",
    "Una cosa a tener en cuenta es el hecho de que en este ejemplo `xi` es una instancia de `numpy.array` (los datos se cargaron utilizando la librería de scikit-learn) pero, por su diseño, la librería  `River`, utiliza la clase `dict` como base para su trabajo trabajo. Cada observación viene representada por un diccionario y se asume que cada una de ellas equivale a una muestra del *stream* de datos.\n",
    "\n",
    "Hay algunas consideraciones que vale la pena recordar y tener en cuenta:\n",
    "\n",
    "1. El  uso de `numpy` es interesante en entornos de computación de alto rendimiento. Recuerda que `dict` está implementado en Python, mientras que `numpy.array` está implementado a bajo nivel en C y Fortran. Una de las ventajas de usar `dict` es su facilidad de uso y que facilita la legibilidad del programa.\n",
    "\n",
    "2. El procesamiento en línea es diferente al procesamiento por lotes (*batch learning*), ya que **la vectorización no aporta ninguna aceleración en el proceso** (recordad que solo estamos procesando una observación de cada vez). Las librerías de procesamiento numérico esán optimizadas para operaciones vectorizadas e introducen una sobrecarga considerable si solo se procesa una única muestra. En este tipo de escenarios, el uso de  `dict` no penaliza y puede simplificar el desarrollo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In numpy.array format:{xi}\\n\")\n",
    "print(f\"In dict format: {dict(zip(dataset.feature_names, xi))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45036aa1",
   "metadata": {},
   "source": [
    "Para facilitar la portabilidad de algoritmos y *pipelines* trabajo de ML preexistentes, `River` proporciona una función *wrapper* que transforma los datasets de `scikit-learn` al formato requerido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5eb589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()):\n",
    "    pass\n",
    "\n",
    "print(xi)#Imprimimos la última observación para ver su formato\n",
    "print(yi)#Imprimimos la última etiqueta para ver su formato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08423c3",
   "metadata": {},
   "source": [
    "Si bien la mayoría de las operaciones se traducen con bastante facilidad entre el ML tradicional y el aprendizaje incremental, no todas se pueden hacer de forma directa. Vamos a ejemplificar esto con el problema del escalado de los datos, en concreto con la estandarización (escalar los datos para que tengan media 0 y varianza 1). Esta es una operación trivial sobre lotes de datos pero que en aprendizaje incremental requerirá calcular estadísticas dinámicamente. \n",
    "\n",
    " se debe tener cuidado al convertir entre los bien conocidos batches (lotes) y streams (flujos). Ilustraremos esto considerando el problema del escalado de datos. Recordemos que la estandarización de los datos, es decir, escalar los datos para que tengan media 0 y varianza 1, es una operación trivial sobre lotes, mientras que para los flujos de datos se requerirán estadísticas en ejecución (running) o móviles (moving). Esta sencilla operación se vuelve un poco más complicada en el aprendizaje incremental porque no conocemos los valores de la media y la desviación estándar antes de procesar todos los datos.\n",
    "\n",
    "Una primera aproximación podría ser realizar una primera pasada sobre todos los datos para calcular los valores necesarios y luego escalar los valores durante una segunda pasada. Sin embargo, esta solución es inconsistente con nuestro objetivo: procesar los datos una sola vez.\n",
    "\n",
    "La solución  es usar estadísticas dinámicas donde no se usen  la media y la desviación estándar exactas, sino una estimación que se actualiza con cada nuevo valor. Más formalmente, dado la media $\\mu_t$, y el conteo de observacionbes  $n_t$, ambos en el instante temporal $t$, la media móvil puede ser facilmente actualizada para cada muestra aplicando la siguiente función: \n",
    "\n",
    "\n",
    "$$\\large\n",
    "  n_{t+1} = n_t +1 \\\\   $$\n",
    "$$  \n",
    "  \\large\n",
    "  \\mu_{t+1} = \\mu_t +\\frac{x - \\mu_t}{n_{t+1}}    \n",
    "$$\n",
    "\n",
    "De la misma manera, el cálculo de la varianza ($\\sigma_t$) en el instante $t$ podría actualizarse empleando:\n",
    "$$\n",
    "\\large\n",
    "  s_{t+1} = s_t + (x-\\mu_t)\\times(x-\\mu_{t+1})\\\\\n",
    "  \\large\n",
    "  \\sigma_{t+1} = \\frac{s_{t+1}}{n_{t+1}}\n",
    "$$\n",
    "\n",
    " Estas fórmulas pueden ser fácilmente implementadas en Python. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8392c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n, mean, s, variance = 0, 0, 0, 0\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()):\n",
    "    n += 1\n",
    "    mean_t = mean\n",
    "    mean += (xi['mean radius'] - mean_t) / n\n",
    "    s += (xi['mean radius'] - mean_t) * (xi['mean radius'] - mean)\n",
    "    variance = s / n\n",
    "\n",
    "    print(f'Media dinámica: {mean:.3f} - Varianza dinámica: {variance:.3f}')\n",
    "    \n",
    "print(f'Media final: {mean:.3f} - Varianza final: {variance:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d540b4",
   "metadata": {},
   "source": [
    "Comparemos ahora el resultado con la implementación nativa de `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "i = list(dataset.feature_names).index('mean radius')\n",
    "print(f'True mean: {np.mean(X[:, i]):.3f}')\n",
    "print(f'True variance: {np.var(X[:, i]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3006eae",
   "metadata": {},
   "source": [
    "Como era de esperar, los resultados finales  son idénticos, con una diferencia clave: la implementación de `numpy` requiere que todos los datos estén disponibles para el cálculo, mientras que en el caso *online*, se calculan progresivamente. Por lo tanto, debemos ser conscientes de estos comportamientos y darnos cuenta de que los resultados con solo unas pocas observaciones no son precisos.\n",
    "\n",
    "Aunque la mayoría de las medidas estadísticas  se podrían desarrollar fácilmente en Python de forma dinámica, la realidad es que no es necesario, ya que River proporciona la mayoría de ellas en el módulo `stats`. Por ejemplo, para crear tanto la media móvil como la varianza móvil emplearíamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de814e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stats\n",
    "\n",
    "r_mean=stats.Mean()\n",
    "r_variance=stats.Var()\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()): \n",
    "    r_mean.update(xi['mean radius'])\n",
    "    r_variance.update(xi['mean radius'])\n",
    "    print(f'Running mean: {r_mean.get():.3f} - Running variance: {r_variance.get():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae26e6f",
   "metadata": {},
   "source": [
    "Una vez que sabemos cómo calcular las diferentes estadísticas móviles sobre los datos, el siguiente paso es usarlas para estandarizar las observaciones de manera similar al enfoque tradicional. En `River`, se, dentro del módulo `preprocessin`están disponibles varias funciones  para aplicar este tipo de operaciones. Por ejemplo, para estandarizar todas las características del ejemplo anterior, podríamos realizar:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f05374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()):\n",
    "    scaler.learn_one(xi)\n",
    "    xi_scaled=scaler.transform_one(xi)\n",
    "    print(f\"xi:{xi}\")\n",
    "    print(f\"scaled xi {xi_scaled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ce3c5-3f3f-4e27-8a66-b0115b3ebb2a",
   "metadata": {},
   "source": [
    "### Aplicando Machine Learning\n",
    "\n",
    "Ahora que ya tenemos los datos están escalados, podemos aplicar un algoritmo de aprendizaje automático. En el siguiente ejemplo usamos una regresión logística con un descenso de gradiente estocástico (SGD). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4c06d-e17a-4bdf-a8f0-7ef19ecd942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.linear_model import LogisticRegression\n",
    "from river.optim import SGD\n",
    "\n",
    "scaler = StandardScaler()\n",
    "optimizer = SGD(lr=0.01)\n",
    "log_reg = LogisticRegression(optimizer)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer(), shuffle=True, seed=42):\n",
    "\n",
    "    # Escalado de las característcias\n",
    "    scaler.learn_one(xi)\n",
    "    xi_scaled =scaler.transform_one(xi)\n",
    "    # Testeo del modelo actual empleando una observación nunca vista\n",
    "    yi_pred = log_reg.predict_proba_one(xi_scaled)\n",
    "    # Entrenamiento del modelo con la observación\n",
    "    log_reg.learn_one(xi_scaled, yi)\n",
    "\n",
    "    # Almacenamos el valor real y la predicción\n",
    "    y_true.append(yi)\n",
    "    y_pred.append(yi_pred[True])\n",
    "\n",
    "print(f'ROC AUC: {roc_auc_score(y_true, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c70ac1-ba33-4c1d-8faa-631e49e58523",
   "metadata": {},
   "source": [
    "Los resultados parecen ser ligeramente mejores que los obtenidos con `scikit-learn` pero debemos hacer una comparación más adecuada. Para hacer una comparación más justa, ambos modelos deberían usar la misma configuración para la validación cruzada (CV). Aunque podríamos definir el mismo proceso con bastante facilidad, los dos procesos podrían hacerse completamente comparables utilizando un módulo integrado de `River` llamado `compat`. Este módulo es un *wrapper* que mejora la compatibilidad con otras librerías de Python. Esto se puede realizar llamando a la función `convert_river_to_sklearn`,  que genera un objeto perfectamente compatible con las funciones de `scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad2ab6-b6c3-4c77-91ab-881618885867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.compose import Pipeline\n",
    "from river.compat import convert_river_to_sklearn\n",
    "\n",
    "\n",
    "# Definimos el pipeline de River\n",
    "model = Pipeline(\n",
    "    ('scale', StandardScaler()),\n",
    "    ('ml_model', LogisticRegression())\n",
    ")\n",
    "\n",
    "# Esta función devuelve un objeto del tipo  SKLRegressorWrapper \n",
    "# que es compatible con la intergaz de  sklearn\n",
    "model = convert_river_to_sklearn(model)\n",
    "\n",
    "# Ahora ya podemos emplear cross_val_score de sklearn con el modelo de River \n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Comparemos los resultados\n",
    "print(f'ROC AUC: {scores.mean():.4f} (± {scores.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc54aad-25de-4f71-ad52-876bb7f4055a",
   "metadata": {},
   "source": [
    "Aunque las métricas son más bajas que en la prueba anterior, los resultados en este caso si son comparables a la aproximación tradicional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b874984b-a119-4f73-ad99-49bf706e0e89",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "El \"flujo\" de información en el desarrollo de modelos de  aprendizaje automático  a menudo se puede encapsular como una secuencia de pasos o *pipeline*. Para realizarlo, muchas librerías como, por ejemplo, `scikit-learn` o `pandas` proporcionan objetos que permiten implementar este patrón \"declarativo\". En el caso de `River`,  existen un conjunto especial de módulos que permiten implementar estos *pipelines*.\n",
    "\n",
    "En la práctica, muchos  desarrolladores no suelen utilizar *pipelines* para representar los flujos de trabajo debido a que  provienen del mundo del ML tradicional (orientado a \"lotes\"), donde los normal es desarrollar modelos a través de la programación procedimental pero, sin embargo, en el aprendizaje *online*, los *pipelines* representan una forma más natural de trabajar.\n",
    "\n",
    "En el siguiente ejemplo, comparamos tanto el método procedimental como el declarativo (a través de *pipelines*) empleando un dataset típico en competiciones de Kaggle.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17438e91-60bf-4691-8deb-2fcfd9c47de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from river.datasets import Restaurants\n",
    "\n",
    "data = Restaurants()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acce4b8-c532-4abd-935b-e294dc53322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Veamos la estructura de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6194f2-5f8e-445a-a7fe-5efc18503ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c61275c-2e34-4386-a70c-6fde3fab2a05",
   "metadata": {},
   "source": [
    "### Aproximación procedimental\n",
    "Veamos primero la aproximación clásica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb5e59-74eb-42b3-acea-ceac29b9d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import feature_extraction, linear_model, metrics, preprocessing, stats, utils\n",
    "\n",
    "# Vamos a crear el promedio de los últimos 7, 14 y 21 días\n",
    "features = (\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\"),\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\"),\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\")\n",
    ")\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "model = linear_model.LinearRegression()\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for x, y in data:\n",
    "\n",
    "    # Ingeniería de variables\n",
    "    x['weekday'] = x['date'].weekday() #weekday is a function of datetime.date. It returns a number linked to the week day (monday=0,..., sunday=6)\n",
    "    x['is_weekend'] = x['date'].weekday() in (5, 6)\n",
    "    \n",
    "    # Procesa la media móvil del target  \n",
    "    for mean_f in features:\n",
    "    \n",
    "        x = {**x, **mean_f.transform_one(x)}#** unzip un diccionario. De esta forma permite concatenar 2 diccionarios   \n",
    "        mean_f.learn_one(x, y)\n",
    "   \n",
    "    # Eliminar la variables que no queremos emplear como características\n",
    "    for key in ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']:\n",
    "        x.pop(key)\n",
    "      \n",
    "    # Escalado de los datos\n",
    "    # Este ejemplo tiene solo un propósito docente\n",
    "    # En un ejemplo real no necesitamos escalar características como is_holiday o is_weekend\n",
    "    scaler.learn_one(x)\n",
    "    x=scaler.transform_one(x)\n",
    "\n",
    "    # Predicción y Entrenamiento\n",
    "    y_pred = model.predict_one(x)\n",
    "    model.learn_one(x, y)\n",
    "\n",
    "    # Actualización de las métricas\n",
    "    metric.update(y, y_pred)\n",
    "\n",
    "print(metric)\n",
    "\n",
    "#Comprobemos la última observación\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e58607-26c5-4e62-a9b8-e6f8bdb3d2e4",
   "metadata": {},
   "source": [
    "### Aproximación declarativa empleando *pipelines* de River\n",
    "\n",
    "Reescribamos ahora el código anterior empleando una aproximación declarativa a través de los *pipelines* de `River"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f4963-8fcf-42cd-8723-06e992157c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "\n",
    "# Función para generar nuevas variables\n",
    "def get_date_features(x):\n",
    "    weekday =  x['date'].weekday()\n",
    "    return {'weekday': weekday, 'is_weekend': weekday in (5, 6)}\n",
    "\n",
    "# Consutrcción del pipeline con los mismos pasos\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(),7),target_name=\"last_7_mean\")),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(),14), target_name=\"last_14_mean\")),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(),21), target_name=\"last_21_mean\"))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for x, y in data:\n",
    "\n",
    "    # Genera la predicción\n",
    "    y_pred = model.predict_one(x)\n",
    "\n",
    "    # Actualiza el modelo una vez recuperada la salida real\n",
    "    model.learn_one(x, y)\n",
    "\n",
    "    # Actualiza las métricas con la predicción y la salida real\n",
    "    metric.update(y, y_pred)\n",
    "\n",
    "print(x)\n",
    "print(metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3dbba-6a6a-48f3-9a9b-c169c52bee38",
   "metadata": {},
   "source": [
    "Como podrás observas, todos los cálculos y transformaciones se han organizado en el objeto `TransformerUnion`, que nos permite agrupar diferentes operaciones de procesamiento en un solo \"transformador\" facilitando el desarrollo de un *pipeline* de aprendizaje automático. El objeto `TransformerUnion` incorpora  una lista de \"transformadores\" y los aplica de forma secuencial a los datos de entrada.\n",
    "\n",
    "El bucle `for`que gestiona la secuencia predecir-aprender-actualizar es tna común que `River`proporciona una función que integra todas las operaciones de forma automática, tal y como se observa en el siguiente código:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ad776-9686-4466-8d8b-db9fe22dd3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import evaluate\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\" )),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\")),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\"))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE()) #Reemplaza el bucle \"for\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8a3ae-07a0-4712-a07b-0a68ac7fe6de",
   "metadata": {},
   "source": [
    "A pesar de que el código es correcto, algunas simplificaciones adicionales pueden ser integradas. Por ejemplo, al igual que sucede en `scikit-learn`, los nombres de los *steps* no son obligatorios (se asigna uno por defecto, si no se añade, basándose en el orden de la operación proporcionada). Veamos cómo quedaría esta simplificación:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacb8d8-96f3-4227-b7b1-5507536bef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compose.Pipeline(\n",
    "    compose.TransformerUnion(\n",
    "        compose.FuncTransformer(get_date_features),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\"),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\"),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\")\n",
    "    ),\n",
    "    compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude'),\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746eda68-b6f5-4a9a-8134-8250945a93d9",
   "metadata": {},
   "source": [
    "La siguiente simplificación proviene de la posibilidad de declarar el *pipeline* empleando operaciones matemáticas. Primero, usa \"+\" para crear el objeto `TransformerUnion` y asignarle operaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69a8ce-3842-47f8-90cd-614e935e0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compose.Pipeline(\n",
    "    compose.FuncTransformer(get_date_features) + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\") + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\") + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\"),\n",
    "\n",
    "    compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude'),\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7b9e2-fd49-4234-b8a4-87484ef27400",
   "metadata": {},
   "source": [
    "A continuación, emplea el operador \"|\" (tubería) para unir *steps* dentro del *pipeline*. Al igual que en `bash`, este operador permite que la salida de un *step* sea la entrada del siguiente, lo que permite generar un flujo de operaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3dde0-4844-4275-b802-9df1377f3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = (\n",
    "    compose.FuncTransformer(get_date_features) +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7),target_name=\"last_7_mean\") +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14),target_name=\"last_14_mean\") +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21),target_name=\"last_21_mean\")\n",
    ")\n",
    "\n",
    "to_discard = ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']\n",
    "\n",
    "model = model | compose.Discard(*to_discard)#* unzip el objeto y emplea los elementos individuales como argumentos de la función\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95cfaca-d530-4608-a4df-dfef0f846709",
   "metadata": {},
   "source": [
    "Una simplificación final proviene del hecho de que `River` encapsula automáticamente las funciones en un objeto `FuncTransform` (no es necesario declararlo expresamente!), por lo que el ejemplo final podría ser algo como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d28c07-019a-4a30-aec1-cfa71e2f3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n),target_name=\"last_\"+str(n)+\"_mean\")\n",
    "\n",
    "model |= compose.Discard(*to_discard)#* unzip el objeto y emplea los elementos individuales como argumentos de la función\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE(), print_every=20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787bdd4-41b1-44db-b151-80a0185d93e9",
   "metadata": {},
   "source": [
    "Se ha incluido incluido un argumento adicional en `progressive_val_score` para imprimir cómo cambia la evaluación a lo largo del tiempo (`print_every=20_000`) . El uso de un enfoque procedimental o declarativo no afecta el rendimiento general, sino más bien la forma en que pensamos sobre el modelo y, obviamente, al código final resultante. De hecho, ambos modelos, ya sean procedimentales o declarativos, producirán los mismos resultados y rendimiento. Como punto final sobre los *pipelines*, cabe mencionar que podemos explorar gráficamente el pipeline consultanto el objeto `modelo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e8234-139d-4b81-9670-fa7742dacfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a89a70-b0b8-44d4-910a-2cc8848c87da",
   "metadata": {},
   "source": [
    "Por último, para depurar el comportamiento de los diferentes *steps*, se puede usar la función `debug_one`. Imagina que entrenamos el modelo con los primeros 120,000 ejemplos y queremos saber qué sucede con el siguiente. El siguiente fragmento de código muestra cómo usar la función `debug_one` en este caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fc510-97f8-4f99-a104-99ccf1ad1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n),target_name=\"last_\"+str(n)+\"_mean\")\n",
    "\n",
    "\n",
    "model |= compose.Discard(*to_discard)#* unzip el objeto y emplea los elementos individuales como argumentos de la función\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "for x, y in itertools.islice(data, 120_000):\n",
    "    y_pred = model.predict_one(x)\n",
    "    model.learn_one(x, y)\n",
    "\n",
    "x, y = next(iter(data))\n",
    "print(model.debug_one(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a15e5-e1b7-4d1a-a635-1a6cd784e6a4",
   "metadata": {},
   "source": [
    "## Aprendizaje incremental con árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184c8fb-3570-442d-a989-1ade3289641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from river import evaluate, metrics, stats, utils, feature_extraction, preprocessing, compose\n",
    "from river.tree import HoeffdingTreeRegressor\n",
    "import itertools\n",
    "\n",
    "data = Restaurants()\n",
    "to_discard = ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']\n",
    "def get_date_features(x):\n",
    "    weekday =  x['date'].weekday()\n",
    "    return {'weekday': weekday, 'is_weekend': weekday in (5, 6)}\n",
    "\n",
    "model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n),target_name=\"last_\"+str(n)+\"_mean\")\n",
    "\n",
    "model |= compose.Discard(*to_discard)\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= HoeffdingTreeRegressor(grace_period=250) #Number of instances a leaf should observe between split attempts\n",
    "\n",
    "for x, y in data:\n",
    "    model.learn_one(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8758ec1-59ea-4516-9615-a11c5fb86541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Una vez desarrollado el modelo, podemos analizarlo desde diferentes puntos de vista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099bc666-92c1-4613-b400-cfb58050863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muestra la estructura\n",
    "model['HoeffdingTreeRegressor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c535d009-7942-42d8-b390-4aa628da67b9",
   "metadata": {},
   "source": [
    "Esto nos da una idea de los parámetros internos del objeto pero también podemos solicitar un resumen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c56da-f58a-4476-b793-d777a0ddeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resumen del árbol\n",
    "model['HoeffdingTreeRegressor'].summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26afc729-8140-4ab3-8929-2ad2ecdc4023",
   "metadata": {},
   "source": [
    "O lo podemos transformar en un `DataFrame` de `Pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec17f6a-b8d6-4344-b367-a4cac01a9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas data frame \n",
    "(model['HoeffdingTreeRegressor']).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12473c8b-94bb-4c4b-87e3-92abd0b3d318",
   "metadata": {},
   "source": [
    "Este último comando podría ser interesante en un enfoque de clasificación debido a que nos proporciona la capacidad de analizar la decisión interna del modelo y verificar los umbrales y pesos asignados pero, sin embargo, en un modelo de regresión no es tan útil. Veamos un ejemplo en el contexto de la clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b314351-62a4-48e5-836d-599051fd218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import evaluate, metrics, stats, utils, feature_extraction, preprocessing, compose\n",
    "from river.tree import HoeffdingTreeClassifier\n",
    "from river.datasets import Phishing #\n",
    "import itertools\n",
    "\n",
    "data = Phishing()\n",
    "\n",
    "model = HoeffdingTreeClassifier(grace_period=50)\n",
    "\n",
    "for x, y in data:\n",
    "    model.learn_one(x, y)\n",
    "    \n",
    "    \n",
    "#resumen del modelo\n",
    "print(model.summary)\n",
    "\n",
    "#convierte el modelo en un DataFrame\n",
    "model.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5febea4-88b3-4dff-a75d-b787bba3bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Otra opción interesante cara a analizar e interpretar el modelo es dibujarlo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48517e-6b46-4dfd-9ad7-9947fc6f1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Es necesario tener la librería graphviz instalada.\n",
    "model.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff9657-f7fa-40f7-a9b4-d4370814c534",
   "metadata": {},
   "source": [
    "`River` tiene muchas implementaciones basadas en árboles y, lal y como se describe en su [web](https://riverml.xyz/latest/recipes/on-hoeffding-trees/#1-trees-trees-everywhere-gardening-101-with-river), estas podían organizarse según la siguiente tabla: \n",
    "\n",
    "| Name | Acronym | Task | Non-stationary data? | Comments | Source |\n",
    "| :- | :-: | :- | :-: | :- | :-: |\n",
    "| Hoeffding Tree Classifier | HTC | Classification | No | Basic HT for classification tasks | [[1]](https://dl.acm.org/doi/pdf/10.1145/347090.347107)|\n",
    "| Hoeffding Adaptive Tree Classifier | HATC | Classification | Yes | Modifies HTC by adding an instance of ADWIN to each node to detect and react to drift detection | [[2]](https://link.springer.com/chapter/10.1007/978-3-642-03915-7_22)|\n",
    "| Extremely Fast Decision Tree Classifier | EFDT | Classification | No | Deploys split decisions as soon as possible and periodically revisit decisions and redo them if necessary. Not as fast in practice as the name implies, but it tends to converge faster than HTC to the model generated by a batch DT | [[3]](https://dl.acm.org/doi/abs/10.1145/3219819.3220005)|\n",
    "| Hoeffding Tree Regressor | HTR | Regression | No | Basic HT for regression tasks. It is an adaptation of the [FIRT/FIMT](https://link.springer.com/article/10.1007/s10618-010-0201-y) algorithm that bears some semblance to HTC | [[4]](https://link.springer.com/article/10.1007/s10618-010-0201-y)|\n",
    "| Hoeffding Adaptive Tree Regressor | HATR | Regression | Yes | Modifies HTR by adding an instance of ADWIN to each node to detect and react to drift detection |-|\n",
    "| incremental Structured-Output Prediction Tree Regressor| iSOUPT | Multi-target regression | No | Multi-target version of HTR | [[5]](https://link.springer.com/article/10.1007/s10844-017-0462-7)|\n",
    "| Label Combination Hoeffding Tree Classifier | LCHTC | Multi-label classification | No | Creates a numerical code for each combination of the binary labels and uses HTC to learn from this encoded representation. At prediction time, decodes the modified representation to obtain the original label set |-| \n",
    "\n",
    "\n",
    "Como podéis ver, cada variante de árbol tiene un objetivo específico, aunque a veces puedan solaparse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
